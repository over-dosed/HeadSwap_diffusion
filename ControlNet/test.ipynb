{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Feature_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "test_branch.cuda()\n",
    "\n",
    "data_for_id = torch.randn(4, 1024).cuda()\n",
    "data_for_global = torch.randn(4, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_branch(data_for_id, data_for_global)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Condition_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "\n",
    "test_branch = Condition_Branch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def get_code_dict(code_dict, batch_size = 4, pose_threshold = 0.02):\n",
    "    # this method get original a clip code_dict as input\n",
    "    # return the indexs selected randomly and the corresponding combined code_dict\n",
    "\n",
    "    tforms = code_dict['tforms']\n",
    "    shape_code = code_dict['shape']\n",
    "    tex_code = code_dict['tex']\n",
    "    exp_code = code_dict['exp']\n",
    "    pose_code = code_dict['pose']\n",
    "    cam_code = code_dict['cam']\n",
    "    light_code = code_dict['light']\n",
    "\n",
    "    tforms_new = torch.zeros(batch_size, tforms.shape[1], tforms.shape[2])\n",
    "    shape_code_new = torch.zeros(batch_size, shape_code.shape[1])\n",
    "    tex_code_new = torch.zeros(batch_size, tex_code.shape[1])\n",
    "    exp_code_new = torch.zeros(batch_size, exp_code.shape[1])\n",
    "    pose_code_new = torch.zeros(batch_size, pose_code.shape[1])\n",
    "    cam_code_new = torch.zeros(batch_size, cam_code.shape[1])\n",
    "    light_code_new = torch.zeros(batch_size, light_code.shape[1], light_code.shape[2])\n",
    "\n",
    "    total_num = pose_code.shape[0]\n",
    "    count = 0\n",
    "    index = []\n",
    "\n",
    "    while True:\n",
    "        a = random.randint(0, total_num-1)       # a for source\n",
    "        b = random.randint(0, total_num-1)       # b for target\n",
    "        if abs(torch.mean(pose_code[a] - pose_code[b])) >= pose_threshold:\n",
    "\n",
    "            # get combined code\n",
    "            tforms_new[count, :] = tforms[b]\n",
    "            shape_code_new[count, :] = shape_code[a]\n",
    "            tex_code_new[count, :] = tex_code[a]\n",
    "            exp_code_new[count, :] = exp_code[b]\n",
    "            pose_code_new[count, :] = pose_code[b]\n",
    "            cam_code_new[count, :] = cam_code[b]\n",
    "            light_code_new[count, :] = light_code[b]\n",
    "\n",
    "            # get index\n",
    "            index.append((a, b))\n",
    "\n",
    "            count +=1\n",
    "\n",
    "            if count == batch_size:\n",
    "                new_code_dict = {\n",
    "                    'tforms':tforms_new.cuda(),\n",
    "                    'shape':shape_code_new.cuda(),\n",
    "                    'tex':tex_code_new.cuda(),\n",
    "                    'exp':exp_code_new.cuda(),\n",
    "                    'pose':pose_code_new.cuda(),\n",
    "                    'cam':cam_code_new.cuda(),\n",
    "                    'light':light_code_new.cuda()\n",
    "                }\n",
    "                return new_code_dict, index\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "clip_path = '/data0/wc_data/VFHQ/train/Clip+xz26EN_LRa8+P0+C0+F4517-4639'\n",
    "\n",
    "with open(osp.join(clip_path, '3DMM_condition.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "codedict, index = get_code_dict(data)\n",
    "\n",
    "source_image_list = []\n",
    "target_image_list = []\n",
    "mask_image_list = []\n",
    "bg_image_list = []\n",
    "\n",
    "\n",
    "# get images\n",
    "for i in range(len(index)):\n",
    "    source_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][0]).zfill(8)))\n",
    "    target_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][1]).zfill(8)))\n",
    "    mask_image_path = osp.join(clip_path, 'mask_{}.jpg'.format(str(index[i][1]).zfill(8)))\n",
    "    source_image_list.append(np.asarray(Image.open(source_image_path).convert(\"RGB\")))\n",
    "    target_image_list.append(np.asarray(Image.open(target_image_path).convert(\"RGB\")))\n",
    "    mask_image_list.append(np.asarray(Image.open(mask_image_path)))\n",
    "\n",
    "# get masked images (background)\n",
    "for i in range(len(index)):\n",
    "    mask = mask_image_list[i]\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 11)\n",
    "    mask = np.where( (mask <= 0), 0, 255).astype('uint8')\n",
    "    bg_image_list.append(cv2.bitwise_and(target_image_list[i], target_image_list[i], mask = 255 - mask))\n",
    "\n",
    "source_images = np.asarray(source_image_list)\n",
    "target_images = np.asarray(target_image_list) # np.array, uint8, \n",
    "mask_images = np.asarray(mask_image_list)\n",
    "bg_images = np.asarray(bg_image_list)\n",
    "\n",
    "bg_images = torch.from_numpy((bg_images / 255.0).transpose(0, 3, 1, 2))\n",
    "bg_images = bg_images.cuda()\n",
    "\n",
    "# for key in codedict:\n",
    "#     print('key: {} has shape : {} '.format(key, str(codedict[key].shape)))\n",
    "\n",
    "out = test_branch(codedict, bg_images)\n",
    "\n",
    "Image_source = source_image_list[0]\n",
    "Image_target = target_image_list[0]\n",
    "Image_mask = np.tile(mask_image_list[0] , (3, 1, 1)).transpose(1, 2, 0)\n",
    "Image_bg = bg_image_list[0]\n",
    "Image_out = (out[0].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "Image_concat = np.concatenate((Image_source, Image_target, Image_mask, Image_bg, Image_out), axis= 1)\n",
    "a = Image.fromarray(Image_concat)\n",
    "a.save('/home/wenchi/zxy/HSD/test_condition.jpg')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test pose distance and test for pose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with open('/data0/wc_data/VFHQ/train/Clip+_aZphIp0KQE+P0+C1+F2675-2891/3DMM_condition.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "pose_code = data['pose']\n",
    "\n",
    "total_num = pose_code.shape[0]\n",
    " \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "row_list = []\n",
    "col_list = []\n",
    "# for i in range(1, 100):\n",
    "#     sum = 0\n",
    "#     for j in range(total_num - i):\n",
    "#         pose_1 = pose_code[j]\n",
    "#         pose_2 = pose_code[j+i]\n",
    "#         sum += torch.mean(pose_1 - pose_2)\n",
    "#     sum /= (total_num - i)\n",
    "#     print('间隔{}帧的图像pose 平均差值为{}'.format(i, sum))\n",
    "#     i_list.append(i)\n",
    "#     sum_list.append(sum)\n",
    "result = [0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(100000):\n",
    "    a = random.randint(0, total_num-1)\n",
    "    b = random.randint(0, total_num-1)\n",
    "\n",
    "    pose_1 = pose_code[a]\n",
    "    pose_2 = pose_code[b]\n",
    "    temp = torch.mean(pose_1 - pose_2)\n",
    "    index = min(abs(int(temp / 0.01)), 6)\n",
    "    result[index] +=1\n",
    "\n",
    "\n",
    "plt.scatter(['0~0.01', '0.01~0.02', '0.02~0.03', '0.03~0.04', '0.04~0.05', '0.05~0.06', '>=0.06'], result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.randn(6)\n",
    "print(a-b)\n",
    "print(torch.mean(a - b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test combine ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.system('export PYTHONPATH=/home/wenchi/zxy/HSD/ControlNet/')\n",
    "sys.path.append('/home/wenchi/zxy/HSD/ControlNet/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_path = \"/data1/wc_log/zxy/ckpt/v3.2-epoch=805-global_step=781819.0.ckpt\"\n",
    "# b_path = '/data1/wc_log/zxy/ckpt/v3.6-epoch=99-global_step=64599.0.ckpt'\n",
    "\n",
    "\n",
    "import torch\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "\n",
    "pretrained_weights = torch.load(a_path)['state_dict']\n",
    "# contrast_weights = torch.load(b_path)['state_dict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pretrained_weights['control_model.input_blocks.8.0.out_layers.3.weight'].cuda()\n",
    "b = contrast_weights['control_model.input_blocks.8.0.out_layers.3.weight']\n",
    "distance = a - b\n",
    "print(distance.data)\n",
    "print(distance.min())\n",
    "print(distance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pretrained_weights:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.5.yaml]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.5.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key cond_stage_model.id_residual_ST1.norm.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.norm.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.proj_in.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.proj_in.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn1.to_q.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn1.to_k.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn1.to_v.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn1.to_out.0.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn1.to_out.0.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.ff.net.0.proj.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.ff.net.0.proj.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.ff.net.2.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.ff.net.2.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn2.to_q.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn2.to_k.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn2.to_v.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn2.to_out.0.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.attn2.to_out.0.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm1.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm1.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm2.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm2.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm3.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.transformer_blocks.0.norm3.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.proj_out.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST1.proj_out.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.norm.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.norm.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.proj_in.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.proj_in.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn1.to_q.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn1.to_k.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn1.to_v.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn1.to_out.0.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn1.to_out.0.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.ff.net.0.proj.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.ff.net.0.proj.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.ff.net.2.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.ff.net.2.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn2.to_q.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn2.to_k.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn2.to_v.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn2.to_out.0.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.attn2.to_out.0.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm1.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm1.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm2.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm2.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm3.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.transformer_blocks.0.norm3.bias not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.proj_out.weight not in target_dict\n",
      "key cond_stage_model.id_residual_ST2.proj_out.bias not in target_dict\n",
      "key cond_stage_model.id_residual_conv.weight not in target_dict\n",
      "key cond_stage_model.id_residual_conv.bias not in target_dict\n"
     ]
    }
   ],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = pretrained_weights\n",
    "\n",
    "# for key in target_dict:\n",
    "#     if key in scratch_dict:\n",
    "#         scratch_dict[key] = target_dict[key]\n",
    "#     else:\n",
    "#         print('key {} not in scratch_dict'.format(key))\n",
    "\n",
    "\n",
    "for key in scratch_dict:\n",
    "    if key in target_dict:\n",
    "        scratch_dict[key] = target_dict[key]\n",
    "    else:\n",
    "        print('key {} not in target_dict'.format(key))\n",
    "\n",
    "# 保存 scratch_dict\n",
    "torch.save(scratch_dict, '/data1/wc_log/zxy/ckpt/v3.5.1-begin.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_keys = list(pretrained_weights.keys())\n",
    "for key in pretrained_weights_keys:\n",
    "    print(key)\n",
    "    # prefix = key.split('.', 1)[0]\n",
    "    # if prefix == 'cond_stage_model':\n",
    "    #     del pretrained_weights[key]\n",
    "\n",
    "# for key in condition_weight:\n",
    "#     add_key = 'cond_stage_model.' + key\n",
    "#     pretrained_weights[add_key] = condition_weight[key].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v2.yaml')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "# print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = {}\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  CLIPVisionModel\n",
    "from ControlNet.ldm.modules.encoders.xf import LayerNorm, Transformer\n",
    "\n",
    "version=\"openai/clip-vit-large-patch14\"\n",
    "transformer = CLIPVisionModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "mapper = Transformer(\n",
    "                n_ctx = 257,\n",
    "                width = 1024,\n",
    "                layers = 5,\n",
    "                heads = 8,\n",
    "            )\n",
    "final_ln = LayerNorm(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = transformer(pixel_values=image)\n",
    "    print('last_hidden_state : ', outputs.last_hidden_state.shape)\n",
    "    print('pooler_output : ', outputs.pooler_output.shape)\n",
    "\n",
    "    z = outputs.last_hidden_state\n",
    "    print('z : ', z.shape)\n",
    "    z = mapper(z)\n",
    "    print('z mapper: ', z.shape)\n",
    "    z = final_ln(z)\n",
    "    print('z final: ', z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox\n",
    "\n",
    "def smooth_mask(mask_image):\n",
    "    mask_image = cv2.GaussianBlur(mask_image, (11, 11), 11)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def get_align_image(bbox, img, reshape_size = 224):\n",
    "    h, w, _ = img.shape\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size))\n",
    "    aligned_img = np.asarray(aligned_img)\n",
    "    return aligned_img\n",
    "\n",
    "source_mask_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/mask_00000067.jpg'\n",
    "source_mask = np.asarray(Image.open(source_mask_path)) # (H, W)\n",
    "source_mask = smooth_mask(source_mask)\n",
    "\n",
    "plt.imshow(source_mask, cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/00000067.png'\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\"))\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = mask_find_bbox(source_mask)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test random mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def smooth_mask(mask_image, ksize=(11, 11), sigmaX= 11, sigmaY= 11):\n",
    "    # need to be applied in data preprocess, and drop this\n",
    "    # GaussianBlur again to reduce mask edge serrate\n",
    "    mask_image = cv2.GaussianBlur(mask_image, ksize, sigmaX=sigmaX, sigmaY = sigmaY)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000021.png'\n",
    "target_mask_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/mask_00000021.jpg'\n",
    "\n",
    "# target_image_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "# target_mask_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/mask_00000053.jpg'\n",
    "\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\"))\n",
    "target_mask_image = np.asarray(Image.open(target_mask_path))\n",
    "\n",
    "plt.imshow(target_mask_image, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(55, 55), sigmaX= 33, sigmaY= 33)\n",
    "import random\n",
    "random_int = random.sample(range(-15, 20), 4)\n",
    "print(random_int)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(33, 33), sigmaX= 33, sigmaY= 33)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(11, 11), sigmaX= 33, sigmaY= 33)\n",
    "target_mask_image_big = smooth_mask(target_mask_image,ksize=(33 + random_int[0]*2, 33 + random_int[1]*2), sigmaX= 33 + random_int[2]*2, sigmaY= 43 + random_int[3]*2)\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlarged_box = mask_find_bbox(target_mask_image_big)\n",
    "random_point_nums = 50\n",
    "\n",
    "x_coords = np.random.randint(enlarged_box[0], enlarged_box[2], (random_point_nums, 1))\n",
    "y_coords = np.random.randint(enlarged_box[1], enlarged_box[3], (random_point_nums, 1))\n",
    "points = np.concatenate([x_coords, y_coords], axis= 1)\n",
    "\n",
    "pixel_values = target_mask_image_big[x_coords, y_coords]\n",
    "mask = np.concatenate([pixel_values == 0, pixel_values == 0], axis= 1)\n",
    "black_points = points[mask]\n",
    "black_points = np.reshape(black_points, (-1, 2))\n",
    "\n",
    "hull = cv2.convexHull(black_points)\n",
    "\n",
    "\n",
    "# blackbg = np.zeros((512, 512), dtype=np.uint8)\n",
    "# for hull_one in hull:\n",
    "#     blackbg = cv2.circle(blackbg, hull_one[0], radius=3, color=(255, 255, 255), thickness=-1)\n",
    "# plt.imshow(blackbg, cmap= 'bone')\n",
    "\n",
    "# # 将这些点作为轮廓生成一个面，面里面填充纯白色\n",
    "cv2.fillPoly(target_mask_image_big, [black_points], 255)\n",
    "\n",
    "# # 显示图像\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_image_distance = target_mask_image_big - target_mask_image\n",
    "plt.imshow(target_mask_image_distance, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_masked = cv2.bitwise_and(target_image, target_image, mask = target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_background = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_background)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test id loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ControlNet.utils.arcface_pytorch.models.resnet import resnet_face18\n",
    "from face_alignment.detection.sfd.sfd_detector import SFDDetector\n",
    "\n",
    "arcface_model_path = '/home/wenchi/zxy/HSD/ControlNet/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth'\n",
    "\n",
    "arcface_model = resnet_face18(False)   # arcface get id information\n",
    "state_dict = torch.load(arcface_model_path)\n",
    "arcface_model.load_state_dict(state_dict)\n",
    "# arcface_model.cuda()\n",
    "arcface_model.eval()\n",
    "\n",
    "face_detector = SFDDetector(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cosine_distance(u: torch.Tensor, v: torch.Tensor, dim) -> torch.Tensor:\n",
    "    # 计算两个张量之间的余弦距离\n",
    "    return 1.0 - F.cosine_similarity(u, v, dim=dim)\n",
    "\n",
    "def process_a_image(bbox, img, reshape_size = 128):\n",
    "    ##  to process a img for arcface\n",
    "    ##  img : numpy, uint8, 0~255, (3, h, w), RGB\n",
    "    img = img.transpose(1, 2, 0) # 3, h, w -> h, w, 3\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size), Image.LANCZOS).convert('L')\n",
    "    img = np.asarray(aligned_img)\n",
    "    img = np.dstack((img, np.fliplr(img)))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = img[:, np.newaxis, :, :]\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    img -= 127.5\n",
    "    img /= 127.5\n",
    "\n",
    "    # 2 * 1 * 128 * 128\n",
    "\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch_id(img_batch, face_detector, arcface_model):\n",
    "    ## this method get id feature of a image batch\n",
    "    ## img_batch: tensor,  (B, 3, size, size), RGB, -1~1\n",
    "\n",
    "    img_batch = (img_batch + 1.0) * 127.5\n",
    "\n",
    "    output_batch = face_detector.detect_from_batch(img_batch)\n",
    "\n",
    "    preprocessed_batch = []\n",
    "    for i in range(len(output_batch)):\n",
    "        BBox = output_batch[i][0].astype('int32')\n",
    "        img = img_batch[i].numpy().astype('uint8')\n",
    "\n",
    "        img = process_a_image(BBox, img)\n",
    "        preprocessed_batch.append(img)\n",
    "    preprocessed_batch = np.concatenate(preprocessed_batch, axis=0) # preprocessed_batch : (B*2, 1, 128, 128)\n",
    "    preprocessed_batch = torch.from_numpy(preprocessed_batch)\n",
    "\n",
    "    output_batch = arcface_model(preprocessed_batch)\n",
    "    fe_1 = output_batch[::2]\n",
    "    fe_2 = output_batch[1::2]\n",
    "    feature = torch.cat((fe_1, fe_2), dim=1)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_a_1 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000034.png'\n",
    "image_a_2 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000005.png'\n",
    "\n",
    "image_b_1 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "image_b_2 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000023.png'\n",
    "# image_b_1 = image_a_2\n",
    "\n",
    "\n",
    "image_a_1 = np.asarray(Image.open(image_a_1).convert(\"RGB\"))\n",
    "image_a_2 = np.asarray(Image.open(image_a_2).convert(\"RGB\"))\n",
    "image_b_1 = np.asarray(Image.open(image_b_1).convert(\"RGB\"))\n",
    "image_b_2 = np.asarray(Image.open(image_b_2).convert(\"RGB\"))\n",
    "\n",
    "image_a_1 = torch.from_numpy((image_a_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_a_2 = torch.from_numpy((image_a_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_1 = torch.from_numpy((image_b_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_2 = torch.from_numpy((image_b_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "\n",
    "image_batch_a = image_a_1.unsqueeze(0)\n",
    "image_batch_b = image_a_2.unsqueeze(0)\n",
    "\n",
    "image_batch_a = torch.concatenate([image_a_1.unsqueeze(0), image_a_2.unsqueeze(0)])\n",
    "image_batch_b = torch.concatenate([image_b_1.unsqueeze(0), image_b_2.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_a = get_batch_id(image_batch_a, face_detector, arcface_model) # B, 1024\n",
    "feature_b = get_batch_id(image_batch_b, face_detector, arcface_model)\n",
    "\n",
    "loss = cosine_distance(feature_a, feature_b, dim=1)\n",
    "print(loss)\n",
    "print(str(np.around(loss[0].numpy(), decimals=3)))\n",
    "print(loss.mean())\n",
    "print(loss.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "b = torch.zeros(1, requires_grad= True)\n",
    "b = b + torch.from_numpy(np.asarray(Image.open(\"/data0/wc_data/VFHQ/train/Clip+y9Tnjy1D-RA+P0+C0+F24114-24280/00000002.png\").convert('RGB')).copy()).float()\n",
    "\n",
    "b = (b - 127.5) / 127.5\n",
    "print(b.is_leaf)\n",
    "b = b.transpose(0, 2)\n",
    "\n",
    "b = transforms.functional.crop(b, top = -10, left = 8, height = 300, width = 406)\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "                transforms.Resize(size=(128, 128), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "            )\n",
    "transformer = torch.jit.script(transforms)\n",
    "print(b.grad_fn)\n",
    "\n",
    "b = transformer(b)\n",
    "print(b.grad_fn)\n",
    "\n",
    "c = b.transpose(0, 2).detach().numpy()\n",
    "print(c.min())\n",
    "plt.imshow(c, cmap='gray')\n",
    "\n",
    "print(b.grad_fn)\n",
    "\n",
    "b = b.transpose(0, 2)\n",
    "\n",
    "b = torch.dstack((b, torch.fliplr(b)))\n",
    "b = b.transpose(0, 2)\n",
    "b = b.unsqueeze(1)\n",
    "print(b.shape)\n",
    "\n",
    "\n",
    "d = torch.concatenate([b, b, b], dim=0)\n",
    "print(d.shape)\n",
    "\n",
    "# out = (b - c).max()\n",
    "# print(out)\n",
    "# print(out.grad_fn)\n",
    "\n",
    "# print(a.is_leaf, b.is_leaf, out.is_leaf)\n",
    "\n",
    "# out.backward()\n",
    "# print(a.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.id_loss import ID_loss\n",
    "\n",
    "a = ID_loss('cuda', '/home/wenchi/zxy/HSD/ControlNet/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13148735e92407d4e5779ae154c1cf483ec3f984d296a4cf4fc2e020ad66c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Feature_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "test_branch.cuda()\n",
    "\n",
    "data_for_id = torch.randn(4, 1024).cuda()\n",
    "data_for_global = torch.randn(4, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_branch(data_for_id, data_for_global)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Condition_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "\n",
    "test_branch = Condition_Branch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def get_code_dict(code_dict, batch_size = 4, pose_threshold = 0.02):\n",
    "    # this method get original a clip code_dict as input\n",
    "    # return the indexs selected randomly and the corresponding combined code_dict\n",
    "\n",
    "    tforms = code_dict['tforms']\n",
    "    shape_code = code_dict['shape']\n",
    "    tex_code = code_dict['tex']\n",
    "    exp_code = code_dict['exp']\n",
    "    pose_code = code_dict['pose']\n",
    "    cam_code = code_dict['cam']\n",
    "    light_code = code_dict['light']\n",
    "\n",
    "    tforms_new = torch.zeros(batch_size, tforms.shape[1], tforms.shape[2])\n",
    "    shape_code_new = torch.zeros(batch_size, shape_code.shape[1])\n",
    "    tex_code_new = torch.zeros(batch_size, tex_code.shape[1])\n",
    "    exp_code_new = torch.zeros(batch_size, exp_code.shape[1])\n",
    "    pose_code_new = torch.zeros(batch_size, pose_code.shape[1])\n",
    "    cam_code_new = torch.zeros(batch_size, cam_code.shape[1])\n",
    "    light_code_new = torch.zeros(batch_size, light_code.shape[1], light_code.shape[2])\n",
    "\n",
    "    total_num = pose_code.shape[0]\n",
    "    count = 0\n",
    "    index = []\n",
    "\n",
    "    while True:\n",
    "        a = random.randint(0, total_num-1)       # a for source\n",
    "        b = random.randint(0, total_num-1)       # b for target\n",
    "        if abs(torch.mean(pose_code[a] - pose_code[b])) >= pose_threshold:\n",
    "\n",
    "            # get combined code\n",
    "            tforms_new[count, :] = tforms[b]\n",
    "            shape_code_new[count, :] = shape_code[a]\n",
    "            tex_code_new[count, :] = tex_code[a]\n",
    "            exp_code_new[count, :] = exp_code[b]\n",
    "            pose_code_new[count, :] = pose_code[b]\n",
    "            cam_code_new[count, :] = cam_code[b]\n",
    "            light_code_new[count, :] = light_code[b]\n",
    "\n",
    "            # get index\n",
    "            index.append((a, b))\n",
    "\n",
    "            count +=1\n",
    "\n",
    "            if count == batch_size:\n",
    "                new_code_dict = {\n",
    "                    'tforms':tforms_new.cuda(),\n",
    "                    'shape':shape_code_new.cuda(),\n",
    "                    'tex':tex_code_new.cuda(),\n",
    "                    'exp':exp_code_new.cuda(),\n",
    "                    'pose':pose_code_new.cuda(),\n",
    "                    'cam':cam_code_new.cuda(),\n",
    "                    'light':light_code_new.cuda()\n",
    "                }\n",
    "                return new_code_dict, index\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "clip_path = '/data0/wc_data/VFHQ/train/Clip+xz26EN_LRa8+P0+C0+F4517-4639'\n",
    "\n",
    "with open(osp.join(clip_path, '3DMM_condition.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "codedict, index = get_code_dict(data)\n",
    "\n",
    "source_image_list = []\n",
    "target_image_list = []\n",
    "mask_image_list = []\n",
    "bg_image_list = []\n",
    "\n",
    "\n",
    "# get images\n",
    "for i in range(len(index)):\n",
    "    source_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][0]).zfill(8)))\n",
    "    target_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][1]).zfill(8)))\n",
    "    mask_image_path = osp.join(clip_path, 'mask_{}.jpg'.format(str(index[i][1]).zfill(8)))\n",
    "    source_image_list.append(np.asarray(Image.open(source_image_path).convert(\"RGB\")))\n",
    "    target_image_list.append(np.asarray(Image.open(target_image_path).convert(\"RGB\")))\n",
    "    mask_image_list.append(np.asarray(Image.open(mask_image_path)))\n",
    "\n",
    "# get masked images (background)\n",
    "for i in range(len(index)):\n",
    "    mask = mask_image_list[i]\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 11)\n",
    "    mask = np.where( (mask <= 0), 0, 255).astype('uint8')\n",
    "    bg_image_list.append(cv2.bitwise_and(target_image_list[i], target_image_list[i], mask = 255 - mask))\n",
    "\n",
    "source_images = np.asarray(source_image_list)\n",
    "target_images = np.asarray(target_image_list) # np.array, uint8, \n",
    "mask_images = np.asarray(mask_image_list)\n",
    "bg_images = np.asarray(bg_image_list)\n",
    "\n",
    "bg_images = torch.from_numpy((bg_images / 255.0).transpose(0, 3, 1, 2))\n",
    "bg_images = bg_images.cuda()\n",
    "\n",
    "# for key in codedict:\n",
    "#     print('key: {} has shape : {} '.format(key, str(codedict[key].shape)))\n",
    "\n",
    "out = test_branch(codedict, bg_images)\n",
    "\n",
    "Image_source = source_image_list[0]\n",
    "Image_target = target_image_list[0]\n",
    "Image_mask = np.tile(mask_image_list[0] , (3, 1, 1)).transpose(1, 2, 0)\n",
    "Image_bg = bg_image_list[0]\n",
    "Image_out = (out[0].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "Image_concat = np.concatenate((Image_source, Image_target, Image_mask, Image_bg, Image_out), axis= 1)\n",
    "a = Image.fromarray(Image_concat)\n",
    "a.save('/home/wenchi/zxy/HSD/test_condition.jpg')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test pose distance and test for pose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with open('/data0/wc_data/VFHQ/train/Clip+_aZphIp0KQE+P0+C1+F2675-2891/3DMM_condition.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "pose_code = data['pose']\n",
    "\n",
    "total_num = pose_code.shape[0]\n",
    " \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "row_list = []\n",
    "col_list = []\n",
    "# for i in range(1, 100):\n",
    "#     sum = 0\n",
    "#     for j in range(total_num - i):\n",
    "#         pose_1 = pose_code[j]\n",
    "#         pose_2 = pose_code[j+i]\n",
    "#         sum += torch.mean(pose_1 - pose_2)\n",
    "#     sum /= (total_num - i)\n",
    "#     print('间隔{}帧的图像pose 平均差值为{}'.format(i, sum))\n",
    "#     i_list.append(i)\n",
    "#     sum_list.append(sum)\n",
    "result = [0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(100000):\n",
    "    a = random.randint(0, total_num-1)\n",
    "    b = random.randint(0, total_num-1)\n",
    "\n",
    "    pose_1 = pose_code[a]\n",
    "    pose_2 = pose_code[b]\n",
    "    temp = torch.mean(pose_1 - pose_2)\n",
    "    index = min(abs(int(temp / 0.01)), 6)\n",
    "    result[index] +=1\n",
    "\n",
    "\n",
    "plt.scatter(['0~0.01', '0.01~0.02', '0.02~0.03', '0.03~0.04', '0.04~0.05', '0.05~0.06', '>=0.06'], result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.randn(6)\n",
    "print(a-b)\n",
    "print(torch.mean(a - b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test combine ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.system('export PYTHONPATH=/home/wenchi/zxy/HSD/ControlNet/')\n",
    "sys.path.append('/home/wenchi/zxy/HSD/ControlNet/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n",
      "Enabled sliced_attention.\n"
     ]
    }
   ],
   "source": [
    "control_path = \"/data1/wc_log/zxy/ckpt/v3.5.2-epoch=183-global_step=14903.0.ckpt\"\n",
    "pretrain_path = '/data1/wc_log/zxy/ckpt/v3.6-epoch=205-global_step=6965.0.ckpt'\n",
    "# b_path = '/data1/wc_log/zxy/ckpt/v3.6-epoch=99-global_step=64599.0.ckpt'\n",
    "\n",
    "\n",
    "import torch\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "\n",
    "pretrained_weights = torch.load(pretrain_path, map_location='cpu')['state_dict']\n",
    "# mapper_weights = torch.load(mapper_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_weights = torch.load(control_path, map_location='cpu')['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_model.time_embed.0.weight\n",
      "control_model.time_embed.0.bias\n",
      "control_model.time_embed.2.weight\n",
      "control_model.time_embed.2.bias\n",
      "control_model.input_blocks.0.0.weight\n",
      "control_model.input_blocks.0.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.0.weight\n",
      "control_model.input_blocks.1.0.in_layers.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.2.weight\n",
      "control_model.input_blocks.1.0.in_layers.2.bias\n",
      "control_model.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model.input_blocks.1.0.out_layers.0.weight\n",
      "control_model.input_blocks.1.0.out_layers.0.bias\n",
      "control_model.input_blocks.1.0.out_layers.3.weight\n",
      "control_model.input_blocks.1.0.out_layers.3.bias\n",
      "control_model.input_blocks.1.1.norm.weight\n",
      "control_model.input_blocks.1.1.norm.bias\n",
      "control_model.input_blocks.1.1.proj_in.weight\n",
      "control_model.input_blocks.1.1.proj_in.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.1.1.proj_out.weight\n",
      "control_model.input_blocks.1.1.proj_out.bias\n",
      "control_model.input_blocks.2.0.in_layers.0.weight\n",
      "control_model.input_blocks.2.0.in_layers.0.bias\n",
      "control_model.input_blocks.2.0.in_layers.2.weight\n",
      "control_model.input_blocks.2.0.in_layers.2.bias\n",
      "control_model.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model.input_blocks.2.0.out_layers.0.weight\n",
      "control_model.input_blocks.2.0.out_layers.0.bias\n",
      "control_model.input_blocks.2.0.out_layers.3.weight\n",
      "control_model.input_blocks.2.0.out_layers.3.bias\n",
      "control_model.input_blocks.2.1.norm.weight\n",
      "control_model.input_blocks.2.1.norm.bias\n",
      "control_model.input_blocks.2.1.proj_in.weight\n",
      "control_model.input_blocks.2.1.proj_in.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.2.1.proj_out.weight\n",
      "control_model.input_blocks.2.1.proj_out.bias\n",
      "control_model.input_blocks.3.0.op.weight\n",
      "control_model.input_blocks.3.0.op.bias\n",
      "control_model.input_blocks.4.0.in_layers.0.weight\n",
      "control_model.input_blocks.4.0.in_layers.0.bias\n",
      "control_model.input_blocks.4.0.in_layers.2.weight\n",
      "control_model.input_blocks.4.0.in_layers.2.bias\n",
      "control_model.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model.input_blocks.4.0.out_layers.0.weight\n",
      "control_model.input_blocks.4.0.out_layers.0.bias\n",
      "control_model.input_blocks.4.0.out_layers.3.weight\n",
      "control_model.input_blocks.4.0.out_layers.3.bias\n",
      "control_model.input_blocks.4.0.skip_connection.weight\n",
      "control_model.input_blocks.4.0.skip_connection.bias\n",
      "control_model.input_blocks.4.1.norm.weight\n",
      "control_model.input_blocks.4.1.norm.bias\n",
      "control_model.input_blocks.4.1.proj_in.weight\n",
      "control_model.input_blocks.4.1.proj_in.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.4.1.proj_out.weight\n",
      "control_model.input_blocks.4.1.proj_out.bias\n",
      "control_model.input_blocks.5.0.in_layers.0.weight\n",
      "control_model.input_blocks.5.0.in_layers.0.bias\n",
      "control_model.input_blocks.5.0.in_layers.2.weight\n",
      "control_model.input_blocks.5.0.in_layers.2.bias\n",
      "control_model.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model.input_blocks.5.0.out_layers.0.weight\n",
      "control_model.input_blocks.5.0.out_layers.0.bias\n",
      "control_model.input_blocks.5.0.out_layers.3.weight\n",
      "control_model.input_blocks.5.0.out_layers.3.bias\n",
      "control_model.input_blocks.5.1.norm.weight\n",
      "control_model.input_blocks.5.1.norm.bias\n",
      "control_model.input_blocks.5.1.proj_in.weight\n",
      "control_model.input_blocks.5.1.proj_in.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.5.1.proj_out.weight\n",
      "control_model.input_blocks.5.1.proj_out.bias\n",
      "control_model.input_blocks.6.0.op.weight\n",
      "control_model.input_blocks.6.0.op.bias\n",
      "control_model.input_blocks.7.0.in_layers.0.weight\n",
      "control_model.input_blocks.7.0.in_layers.0.bias\n",
      "control_model.input_blocks.7.0.in_layers.2.weight\n",
      "control_model.input_blocks.7.0.in_layers.2.bias\n",
      "control_model.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model.input_blocks.7.0.out_layers.0.weight\n",
      "control_model.input_blocks.7.0.out_layers.0.bias\n",
      "control_model.input_blocks.7.0.out_layers.3.weight\n",
      "control_model.input_blocks.7.0.out_layers.3.bias\n",
      "control_model.input_blocks.7.0.skip_connection.weight\n",
      "control_model.input_blocks.7.0.skip_connection.bias\n",
      "control_model.input_blocks.7.1.norm.weight\n",
      "control_model.input_blocks.7.1.norm.bias\n",
      "control_model.input_blocks.7.1.proj_in.weight\n",
      "control_model.input_blocks.7.1.proj_in.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.7.1.proj_out.weight\n",
      "control_model.input_blocks.7.1.proj_out.bias\n",
      "control_model.input_blocks.8.0.in_layers.0.weight\n",
      "control_model.input_blocks.8.0.in_layers.0.bias\n",
      "control_model.input_blocks.8.0.in_layers.2.weight\n",
      "control_model.input_blocks.8.0.in_layers.2.bias\n",
      "control_model.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model.input_blocks.8.0.out_layers.0.weight\n",
      "control_model.input_blocks.8.0.out_layers.0.bias\n",
      "control_model.input_blocks.8.0.out_layers.3.weight\n",
      "control_model.input_blocks.8.0.out_layers.3.bias\n",
      "control_model.input_blocks.8.1.norm.weight\n",
      "control_model.input_blocks.8.1.norm.bias\n",
      "control_model.input_blocks.8.1.proj_in.weight\n",
      "control_model.input_blocks.8.1.proj_in.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.8.1.proj_out.weight\n",
      "control_model.input_blocks.8.1.proj_out.bias\n",
      "control_model.input_blocks.9.0.op.weight\n",
      "control_model.input_blocks.9.0.op.bias\n",
      "control_model.input_blocks.10.0.in_layers.0.weight\n",
      "control_model.input_blocks.10.0.in_layers.0.bias\n",
      "control_model.input_blocks.10.0.in_layers.2.weight\n",
      "control_model.input_blocks.10.0.in_layers.2.bias\n",
      "control_model.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model.input_blocks.10.0.out_layers.0.weight\n",
      "control_model.input_blocks.10.0.out_layers.0.bias\n",
      "control_model.input_blocks.10.0.out_layers.3.weight\n",
      "control_model.input_blocks.10.0.out_layers.3.bias\n",
      "control_model.input_blocks.11.0.in_layers.0.weight\n",
      "control_model.input_blocks.11.0.in_layers.0.bias\n",
      "control_model.input_blocks.11.0.in_layers.2.weight\n",
      "control_model.input_blocks.11.0.in_layers.2.bias\n",
      "control_model.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model.input_blocks.11.0.out_layers.0.weight\n",
      "control_model.input_blocks.11.0.out_layers.0.bias\n",
      "control_model.input_blocks.11.0.out_layers.3.weight\n",
      "control_model.input_blocks.11.0.out_layers.3.bias\n",
      "control_model.zero_convs.0.0.weight\n",
      "control_model.zero_convs.0.0.bias\n",
      "control_model.zero_convs.1.0.weight\n",
      "control_model.zero_convs.1.0.bias\n",
      "control_model.zero_convs.2.0.weight\n",
      "control_model.zero_convs.2.0.bias\n",
      "control_model.zero_convs.3.0.weight\n",
      "control_model.zero_convs.3.0.bias\n",
      "control_model.zero_convs.4.0.weight\n",
      "control_model.zero_convs.4.0.bias\n",
      "control_model.zero_convs.5.0.weight\n",
      "control_model.zero_convs.5.0.bias\n",
      "control_model.zero_convs.6.0.weight\n",
      "control_model.zero_convs.6.0.bias\n",
      "control_model.zero_convs.7.0.weight\n",
      "control_model.zero_convs.7.0.bias\n",
      "control_model.zero_convs.8.0.weight\n",
      "control_model.zero_convs.8.0.bias\n",
      "control_model.zero_convs.9.0.weight\n",
      "control_model.zero_convs.9.0.bias\n",
      "control_model.zero_convs.10.0.weight\n",
      "control_model.zero_convs.10.0.bias\n",
      "control_model.zero_convs.11.0.weight\n",
      "control_model.zero_convs.11.0.bias\n",
      "control_model.input_hint_block.0.weight\n",
      "control_model.input_hint_block.0.bias\n",
      "control_model.input_hint_block.2.weight\n",
      "control_model.input_hint_block.2.bias\n",
      "control_model.input_hint_block.4.weight\n",
      "control_model.input_hint_block.4.bias\n",
      "control_model.input_hint_block.6.weight\n",
      "control_model.input_hint_block.6.bias\n",
      "control_model.input_hint_block.8.weight\n",
      "control_model.input_hint_block.8.bias\n",
      "control_model.input_hint_block.10.weight\n",
      "control_model.input_hint_block.10.bias\n",
      "control_model.input_hint_block.12.weight\n",
      "control_model.input_hint_block.12.bias\n",
      "control_model.input_hint_block.14.weight\n",
      "control_model.input_hint_block.14.bias\n",
      "control_model.middle_block.0.in_layers.0.weight\n",
      "control_model.middle_block.0.in_layers.0.bias\n",
      "control_model.middle_block.0.in_layers.2.weight\n",
      "control_model.middle_block.0.in_layers.2.bias\n",
      "control_model.middle_block.0.emb_layers.1.weight\n",
      "control_model.middle_block.0.emb_layers.1.bias\n",
      "control_model.middle_block.0.out_layers.0.weight\n",
      "control_model.middle_block.0.out_layers.0.bias\n",
      "control_model.middle_block.0.out_layers.3.weight\n",
      "control_model.middle_block.0.out_layers.3.bias\n",
      "control_model.middle_block.1.norm.weight\n",
      "control_model.middle_block.1.norm.bias\n",
      "control_model.middle_block.1.proj_in.weight\n",
      "control_model.middle_block.1.proj_in.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model.middle_block.1.proj_out.weight\n",
      "control_model.middle_block.1.proj_out.bias\n",
      "control_model.middle_block.2.in_layers.0.weight\n",
      "control_model.middle_block.2.in_layers.0.bias\n",
      "control_model.middle_block.2.in_layers.2.weight\n",
      "control_model.middle_block.2.in_layers.2.bias\n",
      "control_model.middle_block.2.emb_layers.1.weight\n",
      "control_model.middle_block.2.emb_layers.1.bias\n",
      "control_model.middle_block.2.out_layers.0.weight\n",
      "control_model.middle_block.2.out_layers.0.bias\n",
      "control_model.middle_block.2.out_layers.3.weight\n",
      "control_model.middle_block.2.out_layers.3.bias\n",
      "control_model.middle_block_out.0.weight\n",
      "control_model.middle_block_out.0.bias\n"
     ]
    }
   ],
   "source": [
    "new_weights = {}\n",
    "for key in pretrained_weights:\n",
    "    if 'control_model.' in key:\n",
    "        print(key)\n",
    "        new_weights[key] = control_weights[key]\n",
    "    else:\n",
    "        new_weights[key] = pretrained_weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.6.yaml]\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "import torch\n",
    "from cldm.model import create_model\n",
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.6.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas\n",
      "alphas_cumprod\n",
      "alphas_cumprod_prev\n",
      "sqrt_alphas_cumprod\n",
      "sqrt_one_minus_alphas_cumprod\n",
      "log_one_minus_alphas_cumprod\n",
      "sqrt_recip_alphas_cumprod\n",
      "sqrt_recipm1_alphas_cumprod\n",
      "posterior_variance\n",
      "posterior_log_variance_clipped\n",
      "posterior_mean_coef1\n",
      "posterior_mean_coef2\n",
      "logvar\n",
      "model.diffusion_model.time_embed.0.weight\n",
      "model.diffusion_model.time_embed.0.bias\n",
      "model.diffusion_model.time_embed.2.weight\n",
      "model.diffusion_model.time_embed.2.bias\n",
      "model.diffusion_model.input_blocks.0.0.weight\n",
      "model.diffusion_model.input_blocks.0.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.1.1.norm.weight\n",
      "model.diffusion_model.input_blocks.1.1.norm.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.2.1.norm.weight\n",
      "model.diffusion_model.input_blocks.2.1.norm.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.3.0.op.weight\n",
      "model.diffusion_model.input_blocks.3.0.op.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.4.1.norm.weight\n",
      "model.diffusion_model.input_blocks.4.1.norm.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.5.1.norm.weight\n",
      "model.diffusion_model.input_blocks.5.1.norm.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.6.0.op.weight\n",
      "model.diffusion_model.input_blocks.6.0.op.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.7.1.norm.weight\n",
      "model.diffusion_model.input_blocks.7.1.norm.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.8.1.norm.weight\n",
      "model.diffusion_model.input_blocks.8.1.norm.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.9.0.op.weight\n",
      "model.diffusion_model.input_blocks.9.0.op.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.1.norm.weight\n",
      "model.diffusion_model.middle_block.1.norm.bias\n",
      "model.diffusion_model.middle_block.1.proj_in.weight\n",
      "model.diffusion_model.middle_block.1.proj_in.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.middle_block.1.proj_out.weight\n",
      "model.diffusion_model.middle_block.1.proj_out.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.1.conv.weight\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.3.1.norm.weight\n",
      "model.diffusion_model.output_blocks.3.1.norm.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.4.1.norm.weight\n",
      "model.diffusion_model.output_blocks.4.1.norm.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.5.1.norm.weight\n",
      "model.diffusion_model.output_blocks.5.1.norm.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.2.conv.weight\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.6.1.norm.weight\n",
      "model.diffusion_model.output_blocks.6.1.norm.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.7.1.norm.weight\n",
      "model.diffusion_model.output_blocks.7.1.norm.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.8.1.norm.weight\n",
      "model.diffusion_model.output_blocks.8.1.norm.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.2.conv.weight\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.9.1.norm.weight\n",
      "model.diffusion_model.output_blocks.9.1.norm.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.10.1.norm.weight\n",
      "model.diffusion_model.output_blocks.10.1.norm.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.11.1.norm.weight\n",
      "model.diffusion_model.output_blocks.11.1.norm.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.bias\n",
      "model.diffusion_model.out.0.weight\n",
      "model.diffusion_model.out.0.bias\n",
      "model.diffusion_model.out.2.weight\n",
      "model.diffusion_model.out.2.bias\n",
      "proj_out.weight\n",
      "proj_out.bias\n",
      "first_stage_model.encoder.conv_in.weight\n",
      "first_stage_model.encoder.conv_in.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.0.downsample.conv.weight\n",
      "first_stage_model.encoder.down.0.downsample.conv.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.1.downsample.conv.weight\n",
      "first_stage_model.encoder.down.1.downsample.conv.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.2.downsample.conv.weight\n",
      "first_stage_model.encoder.down.2.downsample.conv.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv2.bias\n",
      "first_stage_model.encoder.mid.block_1.norm1.weight\n",
      "first_stage_model.encoder.mid.block_1.norm1.bias\n",
      "first_stage_model.encoder.mid.block_1.conv1.weight\n",
      "first_stage_model.encoder.mid.block_1.conv1.bias\n",
      "first_stage_model.encoder.mid.block_1.norm2.weight\n",
      "first_stage_model.encoder.mid.block_1.norm2.bias\n",
      "first_stage_model.encoder.mid.block_1.conv2.weight\n",
      "first_stage_model.encoder.mid.block_1.conv2.bias\n",
      "first_stage_model.encoder.mid.attn_1.norm.weight\n",
      "first_stage_model.encoder.mid.attn_1.norm.bias\n",
      "first_stage_model.encoder.mid.attn_1.q.weight\n",
      "first_stage_model.encoder.mid.attn_1.q.bias\n",
      "first_stage_model.encoder.mid.attn_1.k.weight\n",
      "first_stage_model.encoder.mid.attn_1.k.bias\n",
      "first_stage_model.encoder.mid.attn_1.v.weight\n",
      "first_stage_model.encoder.mid.attn_1.v.bias\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.encoder.mid.block_2.norm1.weight\n",
      "first_stage_model.encoder.mid.block_2.norm1.bias\n",
      "first_stage_model.encoder.mid.block_2.conv1.weight\n",
      "first_stage_model.encoder.mid.block_2.conv1.bias\n",
      "first_stage_model.encoder.mid.block_2.norm2.weight\n",
      "first_stage_model.encoder.mid.block_2.norm2.bias\n",
      "first_stage_model.encoder.mid.block_2.conv2.weight\n",
      "first_stage_model.encoder.mid.block_2.conv2.bias\n",
      "first_stage_model.encoder.norm_out.weight\n",
      "first_stage_model.encoder.norm_out.bias\n",
      "first_stage_model.encoder.conv_out.weight\n",
      "first_stage_model.encoder.conv_out.bias\n",
      "first_stage_model.decoder.conv_in.weight\n",
      "first_stage_model.decoder.conv_in.bias\n",
      "first_stage_model.decoder.mid.block_1.norm1.weight\n",
      "first_stage_model.decoder.mid.block_1.norm1.bias\n",
      "first_stage_model.decoder.mid.block_1.conv1.weight\n",
      "first_stage_model.decoder.mid.block_1.conv1.bias\n",
      "first_stage_model.decoder.mid.block_1.norm2.weight\n",
      "first_stage_model.decoder.mid.block_1.norm2.bias\n",
      "first_stage_model.decoder.mid.block_1.conv2.weight\n",
      "first_stage_model.decoder.mid.block_1.conv2.bias\n",
      "first_stage_model.decoder.mid.attn_1.norm.weight\n",
      "first_stage_model.decoder.mid.attn_1.norm.bias\n",
      "first_stage_model.decoder.mid.attn_1.q.weight\n",
      "first_stage_model.decoder.mid.attn_1.q.bias\n",
      "first_stage_model.decoder.mid.attn_1.k.weight\n",
      "first_stage_model.decoder.mid.attn_1.k.bias\n",
      "first_stage_model.decoder.mid.attn_1.v.weight\n",
      "first_stage_model.decoder.mid.attn_1.v.bias\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.decoder.mid.block_2.norm1.weight\n",
      "first_stage_model.decoder.mid.block_2.norm1.bias\n",
      "first_stage_model.decoder.mid.block_2.conv1.weight\n",
      "first_stage_model.decoder.mid.block_2.conv1.bias\n",
      "first_stage_model.decoder.mid.block_2.norm2.weight\n",
      "first_stage_model.decoder.mid.block_2.norm2.bias\n",
      "first_stage_model.decoder.mid.block_2.conv2.weight\n",
      "first_stage_model.decoder.mid.block_2.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.upsample.conv.weight\n",
      "first_stage_model.decoder.up.1.upsample.conv.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.2.upsample.conv.weight\n",
      "first_stage_model.decoder.up.2.upsample.conv.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.3.upsample.conv.weight\n",
      "first_stage_model.decoder.up.3.upsample.conv.bias\n",
      "first_stage_model.decoder.norm_out.weight\n",
      "first_stage_model.decoder.norm_out.bias\n",
      "first_stage_model.decoder.conv_out.weight\n",
      "first_stage_model.decoder.conv_out.bias\n",
      "first_stage_model.quant_conv.weight\n",
      "first_stage_model.quant_conv.bias\n",
      "first_stage_model.post_quant_conv.weight\n",
      "first_stage_model.post_quant_conv.bias\n",
      "cond_stage_model.transformer.vision_model.embeddings.class_embedding\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_ids\n",
      "cond_stage_model.transformer.vision_model.embeddings.patch_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.weight\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.bias\n",
      "cond_stage_model.final_ln.weight\n",
      "cond_stage_model.final_ln.bias\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.0.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.0.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.0.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.0.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.1.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.1.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.1.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.1.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.2.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.2.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.2.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.2.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.3.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.3.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.3.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.3.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.4.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.4.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.4.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.4.ln_2.bias\n",
      "cond_stage_model.proj_in.weight\n",
      "cond_stage_model.proj_in.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.norm.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.norm.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_in.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_in.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.0.proj.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.0.proj.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm1.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm1.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm3.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm3.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.0.proj.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.0.proj.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm1.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm1.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm3.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm3.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_out.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_out.bias\n",
      "cond_stage_model.id_residual_block.id_residual_linear.weight\n",
      "cond_stage_model.id_residual_block.id_residual_linear.bias\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.norm.weight\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.norm.bias\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.qkv.weight\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.qkv.bias\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.proj_out.weight\n",
      "cond_stage_model.id_residual_block.id_residual_mapper.proj_out.bias\n",
      "control_model.time_embed.0.weight\n",
      "control_model.time_embed.0.bias\n",
      "control_model.time_embed.2.weight\n",
      "control_model.time_embed.2.bias\n",
      "control_model.input_blocks.0.0.weight\n",
      "control_model.input_blocks.0.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.0.weight\n",
      "control_model.input_blocks.1.0.in_layers.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.2.weight\n",
      "control_model.input_blocks.1.0.in_layers.2.bias\n",
      "control_model.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model.input_blocks.1.0.out_layers.0.weight\n",
      "control_model.input_blocks.1.0.out_layers.0.bias\n",
      "control_model.input_blocks.1.0.out_layers.3.weight\n",
      "control_model.input_blocks.1.0.out_layers.3.bias\n",
      "control_model.input_blocks.1.1.norm.weight\n",
      "control_model.input_blocks.1.1.norm.bias\n",
      "control_model.input_blocks.1.1.proj_in.weight\n",
      "control_model.input_blocks.1.1.proj_in.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.1.1.proj_out.weight\n",
      "control_model.input_blocks.1.1.proj_out.bias\n",
      "control_model.input_blocks.2.0.in_layers.0.weight\n",
      "control_model.input_blocks.2.0.in_layers.0.bias\n",
      "control_model.input_blocks.2.0.in_layers.2.weight\n",
      "control_model.input_blocks.2.0.in_layers.2.bias\n",
      "control_model.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model.input_blocks.2.0.out_layers.0.weight\n",
      "control_model.input_blocks.2.0.out_layers.0.bias\n",
      "control_model.input_blocks.2.0.out_layers.3.weight\n",
      "control_model.input_blocks.2.0.out_layers.3.bias\n",
      "control_model.input_blocks.2.1.norm.weight\n",
      "control_model.input_blocks.2.1.norm.bias\n",
      "control_model.input_blocks.2.1.proj_in.weight\n",
      "control_model.input_blocks.2.1.proj_in.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.2.1.proj_out.weight\n",
      "control_model.input_blocks.2.1.proj_out.bias\n",
      "control_model.input_blocks.3.0.op.weight\n",
      "control_model.input_blocks.3.0.op.bias\n",
      "control_model.input_blocks.4.0.in_layers.0.weight\n",
      "control_model.input_blocks.4.0.in_layers.0.bias\n",
      "control_model.input_blocks.4.0.in_layers.2.weight\n",
      "control_model.input_blocks.4.0.in_layers.2.bias\n",
      "control_model.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model.input_blocks.4.0.out_layers.0.weight\n",
      "control_model.input_blocks.4.0.out_layers.0.bias\n",
      "control_model.input_blocks.4.0.out_layers.3.weight\n",
      "control_model.input_blocks.4.0.out_layers.3.bias\n",
      "control_model.input_blocks.4.0.skip_connection.weight\n",
      "control_model.input_blocks.4.0.skip_connection.bias\n",
      "control_model.input_blocks.4.1.norm.weight\n",
      "control_model.input_blocks.4.1.norm.bias\n",
      "control_model.input_blocks.4.1.proj_in.weight\n",
      "control_model.input_blocks.4.1.proj_in.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.4.1.proj_out.weight\n",
      "control_model.input_blocks.4.1.proj_out.bias\n",
      "control_model.input_blocks.5.0.in_layers.0.weight\n",
      "control_model.input_blocks.5.0.in_layers.0.bias\n",
      "control_model.input_blocks.5.0.in_layers.2.weight\n",
      "control_model.input_blocks.5.0.in_layers.2.bias\n",
      "control_model.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model.input_blocks.5.0.out_layers.0.weight\n",
      "control_model.input_blocks.5.0.out_layers.0.bias\n",
      "control_model.input_blocks.5.0.out_layers.3.weight\n",
      "control_model.input_blocks.5.0.out_layers.3.bias\n",
      "control_model.input_blocks.5.1.norm.weight\n",
      "control_model.input_blocks.5.1.norm.bias\n",
      "control_model.input_blocks.5.1.proj_in.weight\n",
      "control_model.input_blocks.5.1.proj_in.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.5.1.proj_out.weight\n",
      "control_model.input_blocks.5.1.proj_out.bias\n",
      "control_model.input_blocks.6.0.op.weight\n",
      "control_model.input_blocks.6.0.op.bias\n",
      "control_model.input_blocks.7.0.in_layers.0.weight\n",
      "control_model.input_blocks.7.0.in_layers.0.bias\n",
      "control_model.input_blocks.7.0.in_layers.2.weight\n",
      "control_model.input_blocks.7.0.in_layers.2.bias\n",
      "control_model.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model.input_blocks.7.0.out_layers.0.weight\n",
      "control_model.input_blocks.7.0.out_layers.0.bias\n",
      "control_model.input_blocks.7.0.out_layers.3.weight\n",
      "control_model.input_blocks.7.0.out_layers.3.bias\n",
      "control_model.input_blocks.7.0.skip_connection.weight\n",
      "control_model.input_blocks.7.0.skip_connection.bias\n",
      "control_model.input_blocks.7.1.norm.weight\n",
      "control_model.input_blocks.7.1.norm.bias\n",
      "control_model.input_blocks.7.1.proj_in.weight\n",
      "control_model.input_blocks.7.1.proj_in.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.7.1.proj_out.weight\n",
      "control_model.input_blocks.7.1.proj_out.bias\n",
      "control_model.input_blocks.8.0.in_layers.0.weight\n",
      "control_model.input_blocks.8.0.in_layers.0.bias\n",
      "control_model.input_blocks.8.0.in_layers.2.weight\n",
      "control_model.input_blocks.8.0.in_layers.2.bias\n",
      "control_model.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model.input_blocks.8.0.out_layers.0.weight\n",
      "control_model.input_blocks.8.0.out_layers.0.bias\n",
      "control_model.input_blocks.8.0.out_layers.3.weight\n",
      "control_model.input_blocks.8.0.out_layers.3.bias\n",
      "control_model.input_blocks.8.1.norm.weight\n",
      "control_model.input_blocks.8.1.norm.bias\n",
      "control_model.input_blocks.8.1.proj_in.weight\n",
      "control_model.input_blocks.8.1.proj_in.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.8.1.proj_out.weight\n",
      "control_model.input_blocks.8.1.proj_out.bias\n",
      "control_model.input_blocks.9.0.op.weight\n",
      "control_model.input_blocks.9.0.op.bias\n",
      "control_model.input_blocks.10.0.in_layers.0.weight\n",
      "control_model.input_blocks.10.0.in_layers.0.bias\n",
      "control_model.input_blocks.10.0.in_layers.2.weight\n",
      "control_model.input_blocks.10.0.in_layers.2.bias\n",
      "control_model.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model.input_blocks.10.0.out_layers.0.weight\n",
      "control_model.input_blocks.10.0.out_layers.0.bias\n",
      "control_model.input_blocks.10.0.out_layers.3.weight\n",
      "control_model.input_blocks.10.0.out_layers.3.bias\n",
      "control_model.input_blocks.11.0.in_layers.0.weight\n",
      "control_model.input_blocks.11.0.in_layers.0.bias\n",
      "control_model.input_blocks.11.0.in_layers.2.weight\n",
      "control_model.input_blocks.11.0.in_layers.2.bias\n",
      "control_model.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model.input_blocks.11.0.out_layers.0.weight\n",
      "control_model.input_blocks.11.0.out_layers.0.bias\n",
      "control_model.input_blocks.11.0.out_layers.3.weight\n",
      "control_model.input_blocks.11.0.out_layers.3.bias\n",
      "control_model.zero_convs.0.0.weight\n",
      "control_model.zero_convs.0.0.bias\n",
      "control_model.zero_convs.1.0.weight\n",
      "control_model.zero_convs.1.0.bias\n",
      "control_model.zero_convs.2.0.weight\n",
      "control_model.zero_convs.2.0.bias\n",
      "control_model.zero_convs.3.0.weight\n",
      "control_model.zero_convs.3.0.bias\n",
      "control_model.zero_convs.4.0.weight\n",
      "control_model.zero_convs.4.0.bias\n",
      "control_model.zero_convs.5.0.weight\n",
      "control_model.zero_convs.5.0.bias\n",
      "control_model.zero_convs.6.0.weight\n",
      "control_model.zero_convs.6.0.bias\n",
      "control_model.zero_convs.7.0.weight\n",
      "control_model.zero_convs.7.0.bias\n",
      "control_model.zero_convs.8.0.weight\n",
      "control_model.zero_convs.8.0.bias\n",
      "control_model.zero_convs.9.0.weight\n",
      "control_model.zero_convs.9.0.bias\n",
      "control_model.zero_convs.10.0.weight\n",
      "control_model.zero_convs.10.0.bias\n",
      "control_model.zero_convs.11.0.weight\n",
      "control_model.zero_convs.11.0.bias\n",
      "control_model.input_hint_block.0.weight\n",
      "control_model.input_hint_block.0.bias\n",
      "control_model.input_hint_block.2.weight\n",
      "control_model.input_hint_block.2.bias\n",
      "control_model.input_hint_block.4.weight\n",
      "control_model.input_hint_block.4.bias\n",
      "control_model.input_hint_block.6.weight\n",
      "control_model.input_hint_block.6.bias\n",
      "control_model.input_hint_block.8.weight\n",
      "control_model.input_hint_block.8.bias\n",
      "control_model.input_hint_block.10.weight\n",
      "control_model.input_hint_block.10.bias\n",
      "control_model.input_hint_block.12.weight\n",
      "control_model.input_hint_block.12.bias\n",
      "control_model.input_hint_block.14.weight\n",
      "control_model.input_hint_block.14.bias\n",
      "control_model.middle_block.0.in_layers.0.weight\n",
      "control_model.middle_block.0.in_layers.0.bias\n",
      "control_model.middle_block.0.in_layers.2.weight\n",
      "control_model.middle_block.0.in_layers.2.bias\n",
      "control_model.middle_block.0.emb_layers.1.weight\n",
      "control_model.middle_block.0.emb_layers.1.bias\n",
      "control_model.middle_block.0.out_layers.0.weight\n",
      "control_model.middle_block.0.out_layers.0.bias\n",
      "control_model.middle_block.0.out_layers.3.weight\n",
      "control_model.middle_block.0.out_layers.3.bias\n",
      "control_model.middle_block.1.norm.weight\n",
      "control_model.middle_block.1.norm.bias\n",
      "control_model.middle_block.1.proj_in.weight\n",
      "control_model.middle_block.1.proj_in.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model.middle_block.1.proj_out.weight\n",
      "control_model.middle_block.1.proj_out.bias\n",
      "control_model.middle_block.2.in_layers.0.weight\n",
      "control_model.middle_block.2.in_layers.0.bias\n",
      "control_model.middle_block.2.in_layers.2.weight\n",
      "control_model.middle_block.2.in_layers.2.bias\n",
      "control_model.middle_block.2.emb_layers.1.weight\n",
      "control_model.middle_block.2.emb_layers.1.bias\n",
      "control_model.middle_block.2.out_layers.0.weight\n",
      "control_model.middle_block.2.out_layers.0.bias\n",
      "control_model.middle_block.2.out_layers.3.weight\n",
      "control_model.middle_block.2.out_layers.3.bias\n",
      "control_model.middle_block_out.0.weight\n",
      "control_model.middle_block_out.0.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.2.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.2.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.3.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.3.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.4.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.4.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.5.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.5.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.0.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.2.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.2.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.3.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.3.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.4.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.4.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.5.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.5.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.0.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.sep.weight\n",
      "face_feature_extractor.extractor.arcface.sep_bn.weight\n",
      "face_feature_extractor.extractor.arcface.sep_bn.bias\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_mean\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_var\n",
      "face_feature_extractor.extractor.arcface.sep_bn.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.prelu.weight\n",
      "face_feature_extractor.extractor.arcface.bn2.weight\n",
      "face_feature_extractor.extractor.arcface.bn2.bias\n",
      "face_feature_extractor.extractor.arcface.bn2.running_mean\n",
      "face_feature_extractor.extractor.arcface.bn2.running_var\n",
      "face_feature_extractor.extractor.arcface.bn2.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.linear.weight\n",
      "face_feature_extractor.extractor.arcface.linear.bias\n",
      "face_feature_extractor.extractor.arcface.features.weight\n",
      "face_feature_extractor.extractor.arcface.features.bias\n",
      "face_feature_extractor.extractor.arcface.features.running_mean\n",
      "face_feature_extractor.extractor.arcface.features.running_var\n",
      "face_feature_extractor.extractor.arcface.features.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "for key in scratch_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_stage_model.proj_in.weight train from scratch\n",
      "cond_stage_model.proj_in.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.0.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.3.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep_bn.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep_bn.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.sep_bn.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.prelu.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.bn2.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.bn2.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.bn2.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.bn2.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.bn2.num_batches_tracked train from scratch\n",
      "face_feature_extractor.extractor.arcface.linear.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.linear.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.features.weight train from scratch\n",
      "face_feature_extractor.extractor.arcface.features.bias train from scratch\n",
      "face_feature_extractor.extractor.arcface.features.running_mean train from scratch\n",
      "face_feature_extractor.extractor.arcface.features.running_var train from scratch\n",
      "face_feature_extractor.extractor.arcface.features.num_batches_tracked train from scratch\n"
     ]
    }
   ],
   "source": [
    "new_final_weights = {}\n",
    "for key in scratch_dict:\n",
    "    if key in new_weights :\n",
    "        new_final_weights[key] = new_weights[key]\n",
    "    else:\n",
    "        print(f'{key} train from scratch')\n",
    "        new_final_weights[key] = scratch_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_final_weights, '/data1/wc_log/zxy/ckpt/v3.6.1-begin.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pretrained_weights['control_model.input_blocks.8.0.out_layers.3.weight'].cuda()\n",
    "b = contrast_weights['control_model.input_blocks.8.0.out_layers.3.weight']\n",
    "distance = a - b\n",
    "print(distance.data)\n",
    "print(distance.min())\n",
    "print(distance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pretrained_weights:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = pretrained_weights\n",
    "mapper_dict = mapper_weights\n",
    "\n",
    "# for key in target_dict:\n",
    "#     if key in scratch_dict:\n",
    "#         scratch_dict[key] = target_dict[key]\n",
    "#     else:\n",
    "#         print('key {} not in scratch_dict'.format(key))\n",
    "\n",
    "\n",
    "for key in pretrained_weights:\n",
    "    if 'cond_stage_model.mapper' in key:\n",
    "        pretrained_weights[key] = mapper_dict[key]\n",
    "        print('key {} in mapper_dict'.format(key))\n",
    "\n",
    "# 保存 scratch_dict\n",
    "torch.save(pretrained_weights, '/data1/wc_log/zxy/ckpt/v3.5.2-begin.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_keys = list(pretrained_weights.keys())\n",
    "for key in pretrained_weights_keys:\n",
    "    print(key)\n",
    "    # prefix = key.split('.', 1)[0]\n",
    "    # if prefix == 'cond_stage_model':\n",
    "    #     del pretrained_weights[key]\n",
    "\n",
    "# for key in condition_weight:\n",
    "#     add_key = 'cond_stage_model.' + key\n",
    "#     pretrained_weights[add_key] = condition_weight[key].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v2.yaml')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "# print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = {}\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  CLIPVisionModel\n",
    "from ControlNet.ldm.modules.encoders.xf import LayerNorm, Transformer\n",
    "\n",
    "version=\"openai/clip-vit-large-patch14\"\n",
    "transformer = CLIPVisionModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "mapper = Transformer(\n",
    "                n_ctx = 257,\n",
    "                width = 1024,\n",
    "                layers = 5,\n",
    "                heads = 8,\n",
    "            )\n",
    "final_ln = LayerNorm(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = transformer(pixel_values=image)\n",
    "    print('last_hidden_state : ', outputs.last_hidden_state.shape)\n",
    "    print('pooler_output : ', outputs.pooler_output.shape)\n",
    "\n",
    "    z = outputs.last_hidden_state\n",
    "    print('z : ', z.shape)\n",
    "    z = mapper(z)\n",
    "    print('z mapper: ', z.shape)\n",
    "    z = final_ln(z)\n",
    "    print('z final: ', z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox\n",
    "\n",
    "def smooth_mask(mask_image):\n",
    "    mask_image = cv2.GaussianBlur(mask_image, (11, 11), 11)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def get_align_image(bbox, img, reshape_size = 224):\n",
    "    h, w, _ = img.shape\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size))\n",
    "    aligned_img = np.asarray(aligned_img)\n",
    "    return aligned_img\n",
    "\n",
    "source_mask_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/mask_00000067.jpg'\n",
    "source_mask = np.asarray(Image.open(source_mask_path)) # (H, W)\n",
    "source_mask = smooth_mask(source_mask)\n",
    "\n",
    "plt.imshow(source_mask, cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/00000067.png'\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\"))\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = mask_find_bbox(source_mask)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test random mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def smooth_mask(mask_image, ksize=(11, 11), sigmaX= 11, sigmaY= 11):\n",
    "    # need to be applied in data preprocess, and drop this\n",
    "    # GaussianBlur again to reduce mask edge serrate\n",
    "    mask_image = cv2.GaussianBlur(mask_image, ksize, sigmaX=sigmaX, sigmaY = sigmaY)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000021.png'\n",
    "target_mask_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/mask_00000021.jpg'\n",
    "\n",
    "# target_image_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "# target_mask_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/mask_00000053.jpg'\n",
    "\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\"))\n",
    "target_mask_image = np.asarray(Image.open(target_mask_path))\n",
    "\n",
    "plt.imshow(target_mask_image, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(55, 55), sigmaX= 33, sigmaY= 33)\n",
    "import random\n",
    "random_int = random.sample(range(-15, 20), 4)\n",
    "print(random_int)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(33, 33), sigmaX= 33, sigmaY= 33)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(11, 11), sigmaX= 33, sigmaY= 33)\n",
    "target_mask_image_big = smooth_mask(target_mask_image,ksize=(33 + random_int[0]*2, 33 + random_int[1]*2), sigmaX= 33 + random_int[2]*2, sigmaY= 43 + random_int[3]*2)\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlarged_box = mask_find_bbox(target_mask_image_big)\n",
    "random_point_nums = 50\n",
    "\n",
    "x_coords = np.random.randint(enlarged_box[0], enlarged_box[2], (random_point_nums, 1))\n",
    "y_coords = np.random.randint(enlarged_box[1], enlarged_box[3], (random_point_nums, 1))\n",
    "points = np.concatenate([x_coords, y_coords], axis= 1)\n",
    "\n",
    "pixel_values = target_mask_image_big[x_coords, y_coords]\n",
    "mask = np.concatenate([pixel_values == 0, pixel_values == 0], axis= 1)\n",
    "black_points = points[mask]\n",
    "black_points = np.reshape(black_points, (-1, 2))\n",
    "\n",
    "hull = cv2.convexHull(black_points)\n",
    "\n",
    "\n",
    "# blackbg = np.zeros((512, 512), dtype=np.uint8)\n",
    "# for hull_one in hull:\n",
    "#     blackbg = cv2.circle(blackbg, hull_one[0], radius=3, color=(255, 255, 255), thickness=-1)\n",
    "# plt.imshow(blackbg, cmap= 'bone')\n",
    "\n",
    "# # 将这些点作为轮廓生成一个面，面里面填充纯白色\n",
    "cv2.fillPoly(target_mask_image_big, [black_points], 255)\n",
    "\n",
    "# # 显示图像\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_image_distance = target_mask_image_big - target_mask_image\n",
    "plt.imshow(target_mask_image_distance, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_masked = cv2.bitwise_and(target_image, target_image, mask = target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_background = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_background)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test id loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f12b0131d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from utils.Face_Alignment.align_faces import warp_and_crop_face_tensor, get_reference_facial_points\n",
    "from utils.Face_Alignment.retinaface.detector import RetinafaceDetector\n",
    "\n",
    "image_a_1 = '/data1/wc_log/zxy/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000034.png'\n",
    "image_a_2 = '/data1/wc_log/zxy/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000025.png'\n",
    "\n",
    "image_b_1 = '/home/wenchi/zxy/HSD/compare_1.jpg'\n",
    "image_b_2 = '/home/wenchi/zxy/HSD/compare_2.jpg'\n",
    "\n",
    "detector = RetinafaceDetector(pretrained_path='/home/wenchi/zxy/HSD/ControlNet/utils/Face_Alignment/retinaface/weights/mobilenet0.25_Final.pth', \n",
    "                              type='cuda')\n",
    "\n",
    "\n",
    "def crop_align_image(img: torch.Tensor, detector: RetinafaceDetector, output_size = (112, 112)) -> torch.Tensor:\n",
    "    '''\n",
    "    Crop and align a face image.\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): The input image tensor, with shape (3, 512, 512) and values in the range of 0~255.\n",
    "        detector (RetinafaceDetector): The face detector used to detect the facial landmarks.\n",
    "        output_size (Tuple[int, int]): The output size of the aligned face image. Default is (112, 112).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The aligned face image tensor, with shape (1, 3, 112, 112) and values in the range of 0~255.\n",
    "    '''\n",
    "    \n",
    "    _, facial5points = detector.detect_faces(img)\n",
    "\n",
    "    # detect no face\n",
    "    if len(facial5points) == 0:\n",
    "        return torch.zeros((1, 3, 112, 112))\n",
    "\n",
    "    facial5points = np.reshape(facial5points[0], (2, 5))\n",
    "\n",
    "    default_square = True\n",
    "    inner_padding_factor = 0.25\n",
    "    outer_padding = (0, 0)\n",
    "\n",
    "    # get the reference 5 landmarks position in the crop settings\n",
    "    reference_5pts = get_reference_facial_points(\n",
    "        output_size, inner_padding_factor, outer_padding, default_square)\n",
    "\n",
    "    # dst_img = warp_and_crop_face(raw, facial5points, reference_5pts, crop_size)\n",
    "    dst_img = warp_and_crop_face_tensor(img, facial5points, reference_pts=reference_5pts, crop_size=output_size) # tensor, (1, 3, 112, 112), 0~1, RGB\n",
    "    return dst_img\n",
    "\n",
    "dst_img_a1 = np.asarray(Image.open(image_b_1).convert(\"RGB\"))\n",
    "dst_img_a2 = np.asarray(Image.open(image_b_2).convert(\"RGB\"))\n",
    "\n",
    "dst_img_a1 = torch.from_numpy(dst_img_a1.transpose(2, 0, 1).copy()).float().unsqueeze(0) # tensor (1, 3, 512, 512), 0~255, RGB\n",
    "dst_img_a2 = torch.from_numpy(dst_img_a2.transpose(2, 0, 1).copy()).float().unsqueeze(0) # tensor (1, 3, 512, 512), 0~255, RGB\n",
    "\n",
    "dst_img_a1 = crop_align_image(dst_img_a1, detector) # tensor (1, 3, 112, 112), 0~255, RGB\n",
    "dst_img_a2 = crop_align_image(dst_img_a2, detector) # tensor (1, 3, 112, 112), 0~255, RGB\n",
    "\n",
    "plt.imshow(dst_img_a1.squeeze(0).permute(1, 2, 0) / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.arcface.nets.arcface import Arcface as arcface\n",
    "\n",
    "model_path='/home/wenchi/zxy/HSD/ControlNet/utils/arcface/model_data/arcface_mobilenet_v1.pth'\n",
    "\n",
    "model = arcface(backbone='mobilenetv1', mode=\"predict\").eval()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2281], device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "dst_img_a1 = ((dst_img_a1 - 127.5) / 127.5).cuda() # (1, 3, 112, 112), -1~1\n",
    "output1 = model(dst_img_a1)\n",
    "\n",
    "dst_img_a2 = ((dst_img_a2 - 127.5) / 127.5).cuda() # (1, 3, 112, 112), -1~1\n",
    "output2 = model(dst_img_a2)\n",
    "\n",
    "# calculate cosine similarity\n",
    "import torch.nn.functional as F\n",
    "distance = 1.0 - F.cosine_similarity(output1, output2, dim=1)\n",
    "distance.backward()\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_a_1 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000034.png'\n",
    "image_a_2 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000005.png'\n",
    "\n",
    "image_b_1 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "image_b_2 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000023.png'\n",
    "# image_b_1 = image_a_2\n",
    "\n",
    "\n",
    "image_a_1 = np.asarray(Image.open(image_a_1).convert(\"RGB\"))\n",
    "image_a_2 = np.asarray(Image.open(image_a_2).convert(\"RGB\"))\n",
    "image_b_1 = np.asarray(Image.open(image_b_1).convert(\"RGB\"))\n",
    "image_b_2 = np.asarray(Image.open(image_b_2).convert(\"RGB\"))\n",
    "\n",
    "image_a_1 = torch.from_numpy((image_a_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_a_2 = torch.from_numpy((image_a_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_1 = torch.from_numpy((image_b_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_2 = torch.from_numpy((image_b_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "\n",
    "image_batch_a = image_a_1.unsqueeze(0)\n",
    "image_batch_b = image_a_2.unsqueeze(0)\n",
    "\n",
    "image_batch_a = torch.concatenate([image_a_1.unsqueeze(0), image_a_2.unsqueeze(0)])\n",
    "image_batch_b = torch.concatenate([image_b_1.unsqueeze(0), image_b_2.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_a = get_batch_id(image_batch_a, face_detector, arcface_model) # B, 1024\n",
    "feature_b = get_batch_id(image_batch_b, face_detector, arcface_model)\n",
    "\n",
    "loss = cosine_distance(feature_a, feature_b, dim=1)\n",
    "print(loss)\n",
    "print(str(np.around(loss[0].numpy(), decimals=3)))\n",
    "print(loss.mean())\n",
    "print(loss.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((224, 224, 3))\n",
    "\n",
    "a = a - (104, 117, 123)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13148735e92407d4e5779ae154c1cf483ec3f984d296a4cf4fc2e020ad66c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Feature_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "test_branch.cuda()\n",
    "\n",
    "data_for_id = torch.randn(4, 1024).cuda()\n",
    "data_for_global = torch.randn(4, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_branch(data_for_id, data_for_global)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Condition_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "\n",
    "test_branch = Condition_Branch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def get_code_dict(code_dict, batch_size = 4, pose_threshold = 0.02):\n",
    "    # this method get original a clip code_dict as input\n",
    "    # return the indexs selected randomly and the corresponding combined code_dict\n",
    "\n",
    "    tforms = code_dict['tforms']\n",
    "    shape_code = code_dict['shape']\n",
    "    tex_code = code_dict['tex']\n",
    "    exp_code = code_dict['exp']\n",
    "    pose_code = code_dict['pose']\n",
    "    cam_code = code_dict['cam']\n",
    "    light_code = code_dict['light']\n",
    "\n",
    "    tforms_new = torch.zeros(batch_size, tforms.shape[1], tforms.shape[2])\n",
    "    shape_code_new = torch.zeros(batch_size, shape_code.shape[1])\n",
    "    tex_code_new = torch.zeros(batch_size, tex_code.shape[1])\n",
    "    exp_code_new = torch.zeros(batch_size, exp_code.shape[1])\n",
    "    pose_code_new = torch.zeros(batch_size, pose_code.shape[1])\n",
    "    cam_code_new = torch.zeros(batch_size, cam_code.shape[1])\n",
    "    light_code_new = torch.zeros(batch_size, light_code.shape[1], light_code.shape[2])\n",
    "\n",
    "    total_num = pose_code.shape[0]\n",
    "    count = 0\n",
    "    index = []\n",
    "\n",
    "    while True:\n",
    "        a = random.randint(0, total_num-1)       # a for source\n",
    "        b = random.randint(0, total_num-1)       # b for target\n",
    "        if abs(torch.mean(pose_code[a] - pose_code[b])) >= pose_threshold:\n",
    "\n",
    "            # get combined code\n",
    "            tforms_new[count, :] = tforms[b]\n",
    "            shape_code_new[count, :] = shape_code[a]\n",
    "            tex_code_new[count, :] = tex_code[a]\n",
    "            exp_code_new[count, :] = exp_code[b]\n",
    "            pose_code_new[count, :] = pose_code[b]\n",
    "            cam_code_new[count, :] = cam_code[b]\n",
    "            light_code_new[count, :] = light_code[b]\n",
    "\n",
    "            # get index\n",
    "            index.append((a, b))\n",
    "\n",
    "            count +=1\n",
    "\n",
    "            if count == batch_size:\n",
    "                new_code_dict = {\n",
    "                    'tforms':tforms_new.cuda(),\n",
    "                    'shape':shape_code_new.cuda(),\n",
    "                    'tex':tex_code_new.cuda(),\n",
    "                    'exp':exp_code_new.cuda(),\n",
    "                    'pose':pose_code_new.cuda(),\n",
    "                    'cam':cam_code_new.cuda(),\n",
    "                    'light':light_code_new.cuda()\n",
    "                }\n",
    "                return new_code_dict, index\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "clip_path = '/data0/wc_data/VFHQ/train/Clip+xz26EN_LRa8+P0+C0+F4517-4639'\n",
    "\n",
    "with open(osp.join(clip_path, '3DMM_condition.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "codedict, index = get_code_dict(data)\n",
    "\n",
    "source_image_list = []\n",
    "target_image_list = []\n",
    "mask_image_list = []\n",
    "bg_image_list = []\n",
    "\n",
    "\n",
    "# get images\n",
    "for i in range(len(index)):\n",
    "    source_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][0]).zfill(8)))\n",
    "    target_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][1]).zfill(8)))\n",
    "    mask_image_path = osp.join(clip_path, 'mask_{}.jpg'.format(str(index[i][1]).zfill(8)))\n",
    "    source_image_list.append(np.asarray(Image.open(source_image_path).convert(\"RGB\")))\n",
    "    target_image_list.append(np.asarray(Image.open(target_image_path).convert(\"RGB\")))\n",
    "    mask_image_list.append(np.asarray(Image.open(mask_image_path)))\n",
    "\n",
    "# get masked images (background)\n",
    "for i in range(len(index)):\n",
    "    mask = mask_image_list[i]\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 11)\n",
    "    mask = np.where( (mask <= 0), 0, 255).astype('uint8')\n",
    "    bg_image_list.append(cv2.bitwise_and(target_image_list[i], target_image_list[i], mask = 255 - mask))\n",
    "\n",
    "source_images = np.asarray(source_image_list)\n",
    "target_images = np.asarray(target_image_list) # np.array, uint8, \n",
    "mask_images = np.asarray(mask_image_list)\n",
    "bg_images = np.asarray(bg_image_list)\n",
    "\n",
    "bg_images = torch.from_numpy((bg_images / 255.0).transpose(0, 3, 1, 2))\n",
    "bg_images = bg_images.cuda()\n",
    "\n",
    "# for key in codedict:\n",
    "#     print('key: {} has shape : {} '.format(key, str(codedict[key].shape)))\n",
    "\n",
    "out = test_branch(codedict, bg_images)\n",
    "\n",
    "Image_source = source_image_list[0]\n",
    "Image_target = target_image_list[0]\n",
    "Image_mask = np.tile(mask_image_list[0] , (3, 1, 1)).transpose(1, 2, 0)\n",
    "Image_bg = bg_image_list[0]\n",
    "Image_out = (out[0].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "Image_concat = np.concatenate((Image_source, Image_target, Image_mask, Image_bg, Image_out), axis= 1)\n",
    "a = Image.fromarray(Image_concat)\n",
    "a.save('/home/wenchi/zxy/HSD/test_condition.jpg')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test pose distance and test for pose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with open('/data0/wc_data/VFHQ/train/Clip+_aZphIp0KQE+P0+C1+F2675-2891/3DMM_condition.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "pose_code = data['pose']\n",
    "\n",
    "total_num = pose_code.shape[0]\n",
    " \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "row_list = []\n",
    "col_list = []\n",
    "# for i in range(1, 100):\n",
    "#     sum = 0\n",
    "#     for j in range(total_num - i):\n",
    "#         pose_1 = pose_code[j]\n",
    "#         pose_2 = pose_code[j+i]\n",
    "#         sum += torch.mean(pose_1 - pose_2)\n",
    "#     sum /= (total_num - i)\n",
    "#     print('间隔{}帧的图像pose 平均差值为{}'.format(i, sum))\n",
    "#     i_list.append(i)\n",
    "#     sum_list.append(sum)\n",
    "result = [0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(100000):\n",
    "    a = random.randint(0, total_num-1)\n",
    "    b = random.randint(0, total_num-1)\n",
    "\n",
    "    pose_1 = pose_code[a]\n",
    "    pose_2 = pose_code[b]\n",
    "    temp = torch.mean(pose_1 - pose_2)\n",
    "    index = min(abs(int(temp / 0.01)), 6)\n",
    "    result[index] +=1\n",
    "\n",
    "\n",
    "plt.scatter(['0~0.01', '0.01~0.02', '0.02~0.03', '0.03~0.04', '0.04~0.05', '0.05~0.06', '>=0.06'], result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.randn(6)\n",
    "print(a-b)\n",
    "print(torch.mean(a - b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test combine ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.system('export PYTHONPATH=/home/wenchi/zxy/HSD/ControlNet/')\n",
    "sys.path.append('/home/wenchi/zxy/HSD/ControlNet/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_path = \"/data1/wc_log/zxy/ckpt/v3.5.2-epoch=183-global_step=14903.0.ckpt\"\n",
    "pretrain_path = '/data1/wc_log/zxy/ckpt/v3.7_adapter-begin.ckpt'\n",
    "# b_path = '/data1/wc_log/zxy/ckpt/v3.6-epoch=99-global_step=64599.0.ckpt'\n",
    "\n",
    "\n",
    "import torch\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "\n",
    "# pretrained_weights = torch.load(pretrain_path, map_location='cpu')['state_dict']\n",
    "pretrained_weights = torch.load(pretrain_path, map_location='cpu')\n",
    "# mapper_weights = torch.load(mapper_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_weights = torch.load(control_path, map_location='cpu')['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_model.time_embed.0.weight\n",
      "control_model.time_embed.0.bias\n",
      "control_model.time_embed.2.weight\n",
      "control_model.time_embed.2.bias\n",
      "control_model.input_blocks.0.0.weight\n",
      "control_model.input_blocks.0.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.0.weight\n",
      "control_model.input_blocks.1.0.in_layers.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.2.weight\n",
      "control_model.input_blocks.1.0.in_layers.2.bias\n",
      "control_model.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model.input_blocks.1.0.out_layers.0.weight\n",
      "control_model.input_blocks.1.0.out_layers.0.bias\n",
      "control_model.input_blocks.1.0.out_layers.3.weight\n",
      "control_model.input_blocks.1.0.out_layers.3.bias\n",
      "control_model.input_blocks.1.1.norm.weight\n",
      "control_model.input_blocks.1.1.norm.bias\n",
      "control_model.input_blocks.1.1.proj_in.weight\n",
      "control_model.input_blocks.1.1.proj_in.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.1.1.proj_out.weight\n",
      "control_model.input_blocks.1.1.proj_out.bias\n",
      "control_model.input_blocks.2.0.in_layers.0.weight\n",
      "control_model.input_blocks.2.0.in_layers.0.bias\n",
      "control_model.input_blocks.2.0.in_layers.2.weight\n",
      "control_model.input_blocks.2.0.in_layers.2.bias\n",
      "control_model.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model.input_blocks.2.0.out_layers.0.weight\n",
      "control_model.input_blocks.2.0.out_layers.0.bias\n",
      "control_model.input_blocks.2.0.out_layers.3.weight\n",
      "control_model.input_blocks.2.0.out_layers.3.bias\n",
      "control_model.input_blocks.2.1.norm.weight\n",
      "control_model.input_blocks.2.1.norm.bias\n",
      "control_model.input_blocks.2.1.proj_in.weight\n",
      "control_model.input_blocks.2.1.proj_in.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.2.1.proj_out.weight\n",
      "control_model.input_blocks.2.1.proj_out.bias\n",
      "control_model.input_blocks.3.0.op.weight\n",
      "control_model.input_blocks.3.0.op.bias\n",
      "control_model.input_blocks.4.0.in_layers.0.weight\n",
      "control_model.input_blocks.4.0.in_layers.0.bias\n",
      "control_model.input_blocks.4.0.in_layers.2.weight\n",
      "control_model.input_blocks.4.0.in_layers.2.bias\n",
      "control_model.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model.input_blocks.4.0.out_layers.0.weight\n",
      "control_model.input_blocks.4.0.out_layers.0.bias\n",
      "control_model.input_blocks.4.0.out_layers.3.weight\n",
      "control_model.input_blocks.4.0.out_layers.3.bias\n",
      "control_model.input_blocks.4.0.skip_connection.weight\n",
      "control_model.input_blocks.4.0.skip_connection.bias\n",
      "control_model.input_blocks.4.1.norm.weight\n",
      "control_model.input_blocks.4.1.norm.bias\n",
      "control_model.input_blocks.4.1.proj_in.weight\n",
      "control_model.input_blocks.4.1.proj_in.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.4.1.proj_out.weight\n",
      "control_model.input_blocks.4.1.proj_out.bias\n",
      "control_model.input_blocks.5.0.in_layers.0.weight\n",
      "control_model.input_blocks.5.0.in_layers.0.bias\n",
      "control_model.input_blocks.5.0.in_layers.2.weight\n",
      "control_model.input_blocks.5.0.in_layers.2.bias\n",
      "control_model.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model.input_blocks.5.0.out_layers.0.weight\n",
      "control_model.input_blocks.5.0.out_layers.0.bias\n",
      "control_model.input_blocks.5.0.out_layers.3.weight\n",
      "control_model.input_blocks.5.0.out_layers.3.bias\n",
      "control_model.input_blocks.5.1.norm.weight\n",
      "control_model.input_blocks.5.1.norm.bias\n",
      "control_model.input_blocks.5.1.proj_in.weight\n",
      "control_model.input_blocks.5.1.proj_in.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.5.1.proj_out.weight\n",
      "control_model.input_blocks.5.1.proj_out.bias\n",
      "control_model.input_blocks.6.0.op.weight\n",
      "control_model.input_blocks.6.0.op.bias\n",
      "control_model.input_blocks.7.0.in_layers.0.weight\n",
      "control_model.input_blocks.7.0.in_layers.0.bias\n",
      "control_model.input_blocks.7.0.in_layers.2.weight\n",
      "control_model.input_blocks.7.0.in_layers.2.bias\n",
      "control_model.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model.input_blocks.7.0.out_layers.0.weight\n",
      "control_model.input_blocks.7.0.out_layers.0.bias\n",
      "control_model.input_blocks.7.0.out_layers.3.weight\n",
      "control_model.input_blocks.7.0.out_layers.3.bias\n",
      "control_model.input_blocks.7.0.skip_connection.weight\n",
      "control_model.input_blocks.7.0.skip_connection.bias\n",
      "control_model.input_blocks.7.1.norm.weight\n",
      "control_model.input_blocks.7.1.norm.bias\n",
      "control_model.input_blocks.7.1.proj_in.weight\n",
      "control_model.input_blocks.7.1.proj_in.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.7.1.proj_out.weight\n",
      "control_model.input_blocks.7.1.proj_out.bias\n",
      "control_model.input_blocks.8.0.in_layers.0.weight\n",
      "control_model.input_blocks.8.0.in_layers.0.bias\n",
      "control_model.input_blocks.8.0.in_layers.2.weight\n",
      "control_model.input_blocks.8.0.in_layers.2.bias\n",
      "control_model.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model.input_blocks.8.0.out_layers.0.weight\n",
      "control_model.input_blocks.8.0.out_layers.0.bias\n",
      "control_model.input_blocks.8.0.out_layers.3.weight\n",
      "control_model.input_blocks.8.0.out_layers.3.bias\n",
      "control_model.input_blocks.8.1.norm.weight\n",
      "control_model.input_blocks.8.1.norm.bias\n",
      "control_model.input_blocks.8.1.proj_in.weight\n",
      "control_model.input_blocks.8.1.proj_in.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.8.1.proj_out.weight\n",
      "control_model.input_blocks.8.1.proj_out.bias\n",
      "control_model.input_blocks.9.0.op.weight\n",
      "control_model.input_blocks.9.0.op.bias\n",
      "control_model.input_blocks.10.0.in_layers.0.weight\n",
      "control_model.input_blocks.10.0.in_layers.0.bias\n",
      "control_model.input_blocks.10.0.in_layers.2.weight\n",
      "control_model.input_blocks.10.0.in_layers.2.bias\n",
      "control_model.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model.input_blocks.10.0.out_layers.0.weight\n",
      "control_model.input_blocks.10.0.out_layers.0.bias\n",
      "control_model.input_blocks.10.0.out_layers.3.weight\n",
      "control_model.input_blocks.10.0.out_layers.3.bias\n",
      "control_model.input_blocks.11.0.in_layers.0.weight\n",
      "control_model.input_blocks.11.0.in_layers.0.bias\n",
      "control_model.input_blocks.11.0.in_layers.2.weight\n",
      "control_model.input_blocks.11.0.in_layers.2.bias\n",
      "control_model.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model.input_blocks.11.0.out_layers.0.weight\n",
      "control_model.input_blocks.11.0.out_layers.0.bias\n",
      "control_model.input_blocks.11.0.out_layers.3.weight\n",
      "control_model.input_blocks.11.0.out_layers.3.bias\n",
      "control_model.zero_convs.0.0.weight\n",
      "control_model.zero_convs.0.0.bias\n",
      "control_model.zero_convs.1.0.weight\n",
      "control_model.zero_convs.1.0.bias\n",
      "control_model.zero_convs.2.0.weight\n",
      "control_model.zero_convs.2.0.bias\n",
      "control_model.zero_convs.3.0.weight\n",
      "control_model.zero_convs.3.0.bias\n",
      "control_model.zero_convs.4.0.weight\n",
      "control_model.zero_convs.4.0.bias\n",
      "control_model.zero_convs.5.0.weight\n",
      "control_model.zero_convs.5.0.bias\n",
      "control_model.zero_convs.6.0.weight\n",
      "control_model.zero_convs.6.0.bias\n",
      "control_model.zero_convs.7.0.weight\n",
      "control_model.zero_convs.7.0.bias\n",
      "control_model.zero_convs.8.0.weight\n",
      "control_model.zero_convs.8.0.bias\n",
      "control_model.zero_convs.9.0.weight\n",
      "control_model.zero_convs.9.0.bias\n",
      "control_model.zero_convs.10.0.weight\n",
      "control_model.zero_convs.10.0.bias\n",
      "control_model.zero_convs.11.0.weight\n",
      "control_model.zero_convs.11.0.bias\n",
      "control_model.input_hint_block.0.weight\n",
      "control_model.input_hint_block.0.bias\n",
      "control_model.input_hint_block.2.weight\n",
      "control_model.input_hint_block.2.bias\n",
      "control_model.input_hint_block.4.weight\n",
      "control_model.input_hint_block.4.bias\n",
      "control_model.input_hint_block.6.weight\n",
      "control_model.input_hint_block.6.bias\n",
      "control_model.input_hint_block.8.weight\n",
      "control_model.input_hint_block.8.bias\n",
      "control_model.input_hint_block.10.weight\n",
      "control_model.input_hint_block.10.bias\n",
      "control_model.input_hint_block.12.weight\n",
      "control_model.input_hint_block.12.bias\n",
      "control_model.input_hint_block.14.weight\n",
      "control_model.input_hint_block.14.bias\n",
      "control_model.middle_block.0.in_layers.0.weight\n",
      "control_model.middle_block.0.in_layers.0.bias\n",
      "control_model.middle_block.0.in_layers.2.weight\n",
      "control_model.middle_block.0.in_layers.2.bias\n",
      "control_model.middle_block.0.emb_layers.1.weight\n",
      "control_model.middle_block.0.emb_layers.1.bias\n",
      "control_model.middle_block.0.out_layers.0.weight\n",
      "control_model.middle_block.0.out_layers.0.bias\n",
      "control_model.middle_block.0.out_layers.3.weight\n",
      "control_model.middle_block.0.out_layers.3.bias\n",
      "control_model.middle_block.1.norm.weight\n",
      "control_model.middle_block.1.norm.bias\n",
      "control_model.middle_block.1.proj_in.weight\n",
      "control_model.middle_block.1.proj_in.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model.middle_block.1.proj_out.weight\n",
      "control_model.middle_block.1.proj_out.bias\n",
      "control_model.middle_block.2.in_layers.0.weight\n",
      "control_model.middle_block.2.in_layers.0.bias\n",
      "control_model.middle_block.2.in_layers.2.weight\n",
      "control_model.middle_block.2.in_layers.2.bias\n",
      "control_model.middle_block.2.emb_layers.1.weight\n",
      "control_model.middle_block.2.emb_layers.1.bias\n",
      "control_model.middle_block.2.out_layers.0.weight\n",
      "control_model.middle_block.2.out_layers.0.bias\n",
      "control_model.middle_block.2.out_layers.3.weight\n",
      "control_model.middle_block.2.out_layers.3.bias\n",
      "control_model.middle_block_out.0.weight\n",
      "control_model.middle_block_out.0.bias\n"
     ]
    }
   ],
   "source": [
    "new_weights = {}\n",
    "for key in pretrained_weights:\n",
    "    if 'control_model.' in key:\n",
    "        print(key)\n",
    "        new_weights[key] = control_weights[key]\n",
    "    else:\n",
    "        new_weights[key] = pretrained_weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 903.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.7.yaml]\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "import torch\n",
    "from cldm.model import create_model\n",
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.7.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas\n",
      "alphas_cumprod\n",
      "alphas_cumprod_prev\n",
      "sqrt_alphas_cumprod\n",
      "sqrt_one_minus_alphas_cumprod\n",
      "log_one_minus_alphas_cumprod\n",
      "sqrt_recip_alphas_cumprod\n",
      "sqrt_recipm1_alphas_cumprod\n",
      "posterior_variance\n",
      "posterior_log_variance_clipped\n",
      "posterior_mean_coef1\n",
      "posterior_mean_coef2\n",
      "logvar\n",
      "model.diffusion_model.time_embed.0.weight\n",
      "model.diffusion_model.time_embed.0.bias\n",
      "model.diffusion_model.time_embed.2.weight\n",
      "model.diffusion_model.time_embed.2.bias\n",
      "model.diffusion_model.input_blocks.0.0.weight\n",
      "model.diffusion_model.input_blocks.0.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.1.1.norm.weight\n",
      "model.diffusion_model.input_blocks.1.1.norm.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.2.1.norm.weight\n",
      "model.diffusion_model.input_blocks.2.1.norm.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.3.0.op.weight\n",
      "model.diffusion_model.input_blocks.3.0.op.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.4.1.norm.weight\n",
      "model.diffusion_model.input_blocks.4.1.norm.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.5.1.norm.weight\n",
      "model.diffusion_model.input_blocks.5.1.norm.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.6.0.op.weight\n",
      "model.diffusion_model.input_blocks.6.0.op.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.7.1.norm.weight\n",
      "model.diffusion_model.input_blocks.7.1.norm.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.8.1.norm.weight\n",
      "model.diffusion_model.input_blocks.8.1.norm.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.9.0.op.weight\n",
      "model.diffusion_model.input_blocks.9.0.op.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.1.norm.weight\n",
      "model.diffusion_model.middle_block.1.norm.bias\n",
      "model.diffusion_model.middle_block.1.proj_in.weight\n",
      "model.diffusion_model.middle_block.1.proj_in.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.proj_out.weight\n",
      "model.diffusion_model.middle_block.1.proj_out.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.1.conv.weight\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.3.1.norm.weight\n",
      "model.diffusion_model.output_blocks.3.1.norm.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.4.1.norm.weight\n",
      "model.diffusion_model.output_blocks.4.1.norm.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.5.1.norm.weight\n",
      "model.diffusion_model.output_blocks.5.1.norm.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.2.conv.weight\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.6.1.norm.weight\n",
      "model.diffusion_model.output_blocks.6.1.norm.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.7.1.norm.weight\n",
      "model.diffusion_model.output_blocks.7.1.norm.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.8.1.norm.weight\n",
      "model.diffusion_model.output_blocks.8.1.norm.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.2.conv.weight\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.9.1.norm.weight\n",
      "model.diffusion_model.output_blocks.9.1.norm.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.10.1.norm.weight\n",
      "model.diffusion_model.output_blocks.10.1.norm.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.11.1.norm.weight\n",
      "model.diffusion_model.output_blocks.11.1.norm.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.gamma\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.norm.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.norm.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.bias\n",
      "model.diffusion_model.out.0.weight\n",
      "model.diffusion_model.out.0.bias\n",
      "model.diffusion_model.out.2.weight\n",
      "model.diffusion_model.out.2.bias\n",
      "first_stage_model.encoder.conv_in.weight\n",
      "first_stage_model.encoder.conv_in.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.0.downsample.conv.weight\n",
      "first_stage_model.encoder.down.0.downsample.conv.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.1.downsample.conv.weight\n",
      "first_stage_model.encoder.down.1.downsample.conv.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.2.downsample.conv.weight\n",
      "first_stage_model.encoder.down.2.downsample.conv.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv2.bias\n",
      "first_stage_model.encoder.mid.block_1.norm1.weight\n",
      "first_stage_model.encoder.mid.block_1.norm1.bias\n",
      "first_stage_model.encoder.mid.block_1.conv1.weight\n",
      "first_stage_model.encoder.mid.block_1.conv1.bias\n",
      "first_stage_model.encoder.mid.block_1.norm2.weight\n",
      "first_stage_model.encoder.mid.block_1.norm2.bias\n",
      "first_stage_model.encoder.mid.block_1.conv2.weight\n",
      "first_stage_model.encoder.mid.block_1.conv2.bias\n",
      "first_stage_model.encoder.mid.attn_1.norm.weight\n",
      "first_stage_model.encoder.mid.attn_1.norm.bias\n",
      "first_stage_model.encoder.mid.attn_1.q.weight\n",
      "first_stage_model.encoder.mid.attn_1.q.bias\n",
      "first_stage_model.encoder.mid.attn_1.k.weight\n",
      "first_stage_model.encoder.mid.attn_1.k.bias\n",
      "first_stage_model.encoder.mid.attn_1.v.weight\n",
      "first_stage_model.encoder.mid.attn_1.v.bias\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.encoder.mid.block_2.norm1.weight\n",
      "first_stage_model.encoder.mid.block_2.norm1.bias\n",
      "first_stage_model.encoder.mid.block_2.conv1.weight\n",
      "first_stage_model.encoder.mid.block_2.conv1.bias\n",
      "first_stage_model.encoder.mid.block_2.norm2.weight\n",
      "first_stage_model.encoder.mid.block_2.norm2.bias\n",
      "first_stage_model.encoder.mid.block_2.conv2.weight\n",
      "first_stage_model.encoder.mid.block_2.conv2.bias\n",
      "first_stage_model.encoder.norm_out.weight\n",
      "first_stage_model.encoder.norm_out.bias\n",
      "first_stage_model.encoder.conv_out.weight\n",
      "first_stage_model.encoder.conv_out.bias\n",
      "first_stage_model.decoder.conv_in.weight\n",
      "first_stage_model.decoder.conv_in.bias\n",
      "first_stage_model.decoder.mid.block_1.norm1.weight\n",
      "first_stage_model.decoder.mid.block_1.norm1.bias\n",
      "first_stage_model.decoder.mid.block_1.conv1.weight\n",
      "first_stage_model.decoder.mid.block_1.conv1.bias\n",
      "first_stage_model.decoder.mid.block_1.norm2.weight\n",
      "first_stage_model.decoder.mid.block_1.norm2.bias\n",
      "first_stage_model.decoder.mid.block_1.conv2.weight\n",
      "first_stage_model.decoder.mid.block_1.conv2.bias\n",
      "first_stage_model.decoder.mid.attn_1.norm.weight\n",
      "first_stage_model.decoder.mid.attn_1.norm.bias\n",
      "first_stage_model.decoder.mid.attn_1.q.weight\n",
      "first_stage_model.decoder.mid.attn_1.q.bias\n",
      "first_stage_model.decoder.mid.attn_1.k.weight\n",
      "first_stage_model.decoder.mid.attn_1.k.bias\n",
      "first_stage_model.decoder.mid.attn_1.v.weight\n",
      "first_stage_model.decoder.mid.attn_1.v.bias\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.decoder.mid.block_2.norm1.weight\n",
      "first_stage_model.decoder.mid.block_2.norm1.bias\n",
      "first_stage_model.decoder.mid.block_2.conv1.weight\n",
      "first_stage_model.decoder.mid.block_2.conv1.bias\n",
      "first_stage_model.decoder.mid.block_2.norm2.weight\n",
      "first_stage_model.decoder.mid.block_2.norm2.bias\n",
      "first_stage_model.decoder.mid.block_2.conv2.weight\n",
      "first_stage_model.decoder.mid.block_2.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.upsample.conv.weight\n",
      "first_stage_model.decoder.up.1.upsample.conv.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.2.upsample.conv.weight\n",
      "first_stage_model.decoder.up.2.upsample.conv.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.3.upsample.conv.weight\n",
      "first_stage_model.decoder.up.3.upsample.conv.bias\n",
      "first_stage_model.decoder.norm_out.weight\n",
      "first_stage_model.decoder.norm_out.bias\n",
      "first_stage_model.decoder.conv_out.weight\n",
      "first_stage_model.decoder.conv_out.bias\n",
      "first_stage_model.quant_conv.weight\n",
      "first_stage_model.quant_conv.bias\n",
      "first_stage_model.post_quant_conv.weight\n",
      "first_stage_model.post_quant_conv.bias\n",
      "cond_stage_model.transformer.vision_model.embeddings.class_embedding\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_ids\n",
      "cond_stage_model.transformer.vision_model.embeddings.patch_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.weight\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.bias\n",
      "cond_stage_model.id_proj_in.weight\n",
      "cond_stage_model.id_proj_in.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.attn.c_qkv.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.attn.c_qkv.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.attn.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.attn.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.ln_1.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.ln_1.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.mlp.c_fc.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.mlp.c_fc.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.mlp.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.mlp.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.0.ln_2.weight\n",
      "cond_stage_model.id_mapper.resblocks.0.ln_2.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.attn.c_qkv.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.attn.c_qkv.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.attn.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.attn.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.ln_1.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.ln_1.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.mlp.c_fc.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.mlp.c_fc.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.mlp.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.mlp.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.1.ln_2.weight\n",
      "cond_stage_model.id_mapper.resblocks.1.ln_2.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.attn.c_qkv.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.attn.c_qkv.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.attn.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.attn.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.ln_1.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.ln_1.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.mlp.c_fc.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.mlp.c_fc.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.mlp.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.mlp.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.2.ln_2.weight\n",
      "cond_stage_model.id_mapper.resblocks.2.ln_2.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.attn.c_qkv.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.attn.c_qkv.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.attn.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.attn.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.ln_1.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.ln_1.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.mlp.c_fc.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.mlp.c_fc.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.mlp.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.mlp.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.3.ln_2.weight\n",
      "cond_stage_model.id_mapper.resblocks.3.ln_2.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.attn.c_qkv.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.attn.c_qkv.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.attn.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.attn.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.ln_1.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.ln_1.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.mlp.c_fc.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.mlp.c_fc.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.mlp.c_proj.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.mlp.c_proj.bias\n",
      "cond_stage_model.id_mapper.resblocks.4.ln_2.weight\n",
      "cond_stage_model.id_mapper.resblocks.4.ln_2.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.attn.c_qkv.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.attn.c_qkv.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.attn.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.attn.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.ln_1.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.ln_1.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.mlp.c_fc.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.mlp.c_fc.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.mlp.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.mlp.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.0.ln_2.weight\n",
      "cond_stage_model.global_mapper.resblocks.0.ln_2.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.attn.c_qkv.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.attn.c_qkv.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.attn.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.attn.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.ln_1.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.ln_1.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.mlp.c_fc.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.mlp.c_fc.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.mlp.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.mlp.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.1.ln_2.weight\n",
      "cond_stage_model.global_mapper.resblocks.1.ln_2.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.attn.c_qkv.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.attn.c_qkv.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.attn.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.attn.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.ln_1.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.ln_1.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.mlp.c_fc.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.mlp.c_fc.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.mlp.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.mlp.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.2.ln_2.weight\n",
      "cond_stage_model.global_mapper.resblocks.2.ln_2.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.attn.c_qkv.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.attn.c_qkv.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.attn.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.attn.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.ln_1.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.ln_1.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.mlp.c_fc.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.mlp.c_fc.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.mlp.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.mlp.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.3.ln_2.weight\n",
      "cond_stage_model.global_mapper.resblocks.3.ln_2.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.attn.c_qkv.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.attn.c_qkv.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.attn.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.attn.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.ln_1.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.ln_1.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.mlp.c_fc.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.mlp.c_fc.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.mlp.c_proj.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.mlp.c_proj.bias\n",
      "cond_stage_model.global_mapper.resblocks.4.ln_2.weight\n",
      "cond_stage_model.global_mapper.resblocks.4.ln_2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.norm.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.norm.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_in.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_in.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn1.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.0.proj.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.0.proj.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.ff.net.2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.attn2.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm1.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm1.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm3.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.0.norm3.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn1.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.0.proj.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.0.proj.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.ff.net.2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_q.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_k.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_v.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_out.0.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.attn2.to_out.0.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm1.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm1.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm2.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm2.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm3.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.transformer_blocks.1.norm3.bias\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_out.weight\n",
      "cond_stage_model.id_residual_block.id_residual_ST.proj_out.bias\n",
      "cond_stage_model.id_norm.weight\n",
      "cond_stage_model.id_norm.bias\n",
      "cond_stage_model.global_norm.weight\n",
      "cond_stage_model.global_norm.bias\n",
      "cond_stage_model.id_proj_out.weight\n",
      "cond_stage_model.id_proj_out.bias\n",
      "cond_stage_model.global_proj_out.weight\n",
      "cond_stage_model.global_proj_out.bias\n",
      "control_model.time_embed.0.weight\n",
      "control_model.time_embed.0.bias\n",
      "control_model.time_embed.2.weight\n",
      "control_model.time_embed.2.bias\n",
      "control_model.input_blocks.0.0.weight\n",
      "control_model.input_blocks.0.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.0.weight\n",
      "control_model.input_blocks.1.0.in_layers.0.bias\n",
      "control_model.input_blocks.1.0.in_layers.2.weight\n",
      "control_model.input_blocks.1.0.in_layers.2.bias\n",
      "control_model.input_blocks.1.0.emb_layers.1.weight\n",
      "control_model.input_blocks.1.0.emb_layers.1.bias\n",
      "control_model.input_blocks.1.0.out_layers.0.weight\n",
      "control_model.input_blocks.1.0.out_layers.0.bias\n",
      "control_model.input_blocks.1.0.out_layers.3.weight\n",
      "control_model.input_blocks.1.0.out_layers.3.bias\n",
      "control_model.input_blocks.1.1.norm.weight\n",
      "control_model.input_blocks.1.1.norm.bias\n",
      "control_model.input_blocks.1.1.proj_in.weight\n",
      "control_model.input_blocks.1.1.proj_in.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.1.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.1.1.proj_out.weight\n",
      "control_model.input_blocks.1.1.proj_out.bias\n",
      "control_model.input_blocks.2.0.in_layers.0.weight\n",
      "control_model.input_blocks.2.0.in_layers.0.bias\n",
      "control_model.input_blocks.2.0.in_layers.2.weight\n",
      "control_model.input_blocks.2.0.in_layers.2.bias\n",
      "control_model.input_blocks.2.0.emb_layers.1.weight\n",
      "control_model.input_blocks.2.0.emb_layers.1.bias\n",
      "control_model.input_blocks.2.0.out_layers.0.weight\n",
      "control_model.input_blocks.2.0.out_layers.0.bias\n",
      "control_model.input_blocks.2.0.out_layers.3.weight\n",
      "control_model.input_blocks.2.0.out_layers.3.bias\n",
      "control_model.input_blocks.2.1.norm.weight\n",
      "control_model.input_blocks.2.1.norm.bias\n",
      "control_model.input_blocks.2.1.proj_in.weight\n",
      "control_model.input_blocks.2.1.proj_in.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.2.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.2.1.proj_out.weight\n",
      "control_model.input_blocks.2.1.proj_out.bias\n",
      "control_model.input_blocks.3.0.op.weight\n",
      "control_model.input_blocks.3.0.op.bias\n",
      "control_model.input_blocks.4.0.in_layers.0.weight\n",
      "control_model.input_blocks.4.0.in_layers.0.bias\n",
      "control_model.input_blocks.4.0.in_layers.2.weight\n",
      "control_model.input_blocks.4.0.in_layers.2.bias\n",
      "control_model.input_blocks.4.0.emb_layers.1.weight\n",
      "control_model.input_blocks.4.0.emb_layers.1.bias\n",
      "control_model.input_blocks.4.0.out_layers.0.weight\n",
      "control_model.input_blocks.4.0.out_layers.0.bias\n",
      "control_model.input_blocks.4.0.out_layers.3.weight\n",
      "control_model.input_blocks.4.0.out_layers.3.bias\n",
      "control_model.input_blocks.4.0.skip_connection.weight\n",
      "control_model.input_blocks.4.0.skip_connection.bias\n",
      "control_model.input_blocks.4.1.norm.weight\n",
      "control_model.input_blocks.4.1.norm.bias\n",
      "control_model.input_blocks.4.1.proj_in.weight\n",
      "control_model.input_blocks.4.1.proj_in.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.4.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.4.1.proj_out.weight\n",
      "control_model.input_blocks.4.1.proj_out.bias\n",
      "control_model.input_blocks.5.0.in_layers.0.weight\n",
      "control_model.input_blocks.5.0.in_layers.0.bias\n",
      "control_model.input_blocks.5.0.in_layers.2.weight\n",
      "control_model.input_blocks.5.0.in_layers.2.bias\n",
      "control_model.input_blocks.5.0.emb_layers.1.weight\n",
      "control_model.input_blocks.5.0.emb_layers.1.bias\n",
      "control_model.input_blocks.5.0.out_layers.0.weight\n",
      "control_model.input_blocks.5.0.out_layers.0.bias\n",
      "control_model.input_blocks.5.0.out_layers.3.weight\n",
      "control_model.input_blocks.5.0.out_layers.3.bias\n",
      "control_model.input_blocks.5.1.norm.weight\n",
      "control_model.input_blocks.5.1.norm.bias\n",
      "control_model.input_blocks.5.1.proj_in.weight\n",
      "control_model.input_blocks.5.1.proj_in.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.5.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.5.1.proj_out.weight\n",
      "control_model.input_blocks.5.1.proj_out.bias\n",
      "control_model.input_blocks.6.0.op.weight\n",
      "control_model.input_blocks.6.0.op.bias\n",
      "control_model.input_blocks.7.0.in_layers.0.weight\n",
      "control_model.input_blocks.7.0.in_layers.0.bias\n",
      "control_model.input_blocks.7.0.in_layers.2.weight\n",
      "control_model.input_blocks.7.0.in_layers.2.bias\n",
      "control_model.input_blocks.7.0.emb_layers.1.weight\n",
      "control_model.input_blocks.7.0.emb_layers.1.bias\n",
      "control_model.input_blocks.7.0.out_layers.0.weight\n",
      "control_model.input_blocks.7.0.out_layers.0.bias\n",
      "control_model.input_blocks.7.0.out_layers.3.weight\n",
      "control_model.input_blocks.7.0.out_layers.3.bias\n",
      "control_model.input_blocks.7.0.skip_connection.weight\n",
      "control_model.input_blocks.7.0.skip_connection.bias\n",
      "control_model.input_blocks.7.1.norm.weight\n",
      "control_model.input_blocks.7.1.norm.bias\n",
      "control_model.input_blocks.7.1.proj_in.weight\n",
      "control_model.input_blocks.7.1.proj_in.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.7.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.7.1.proj_out.weight\n",
      "control_model.input_blocks.7.1.proj_out.bias\n",
      "control_model.input_blocks.8.0.in_layers.0.weight\n",
      "control_model.input_blocks.8.0.in_layers.0.bias\n",
      "control_model.input_blocks.8.0.in_layers.2.weight\n",
      "control_model.input_blocks.8.0.in_layers.2.bias\n",
      "control_model.input_blocks.8.0.emb_layers.1.weight\n",
      "control_model.input_blocks.8.0.emb_layers.1.bias\n",
      "control_model.input_blocks.8.0.out_layers.0.weight\n",
      "control_model.input_blocks.8.0.out_layers.0.bias\n",
      "control_model.input_blocks.8.0.out_layers.3.weight\n",
      "control_model.input_blocks.8.0.out_layers.3.bias\n",
      "control_model.input_blocks.8.1.norm.weight\n",
      "control_model.input_blocks.8.1.norm.bias\n",
      "control_model.input_blocks.8.1.proj_in.weight\n",
      "control_model.input_blocks.8.1.proj_in.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.input_blocks.8.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.input_blocks.8.1.proj_out.weight\n",
      "control_model.input_blocks.8.1.proj_out.bias\n",
      "control_model.input_blocks.9.0.op.weight\n",
      "control_model.input_blocks.9.0.op.bias\n",
      "control_model.input_blocks.10.0.in_layers.0.weight\n",
      "control_model.input_blocks.10.0.in_layers.0.bias\n",
      "control_model.input_blocks.10.0.in_layers.2.weight\n",
      "control_model.input_blocks.10.0.in_layers.2.bias\n",
      "control_model.input_blocks.10.0.emb_layers.1.weight\n",
      "control_model.input_blocks.10.0.emb_layers.1.bias\n",
      "control_model.input_blocks.10.0.out_layers.0.weight\n",
      "control_model.input_blocks.10.0.out_layers.0.bias\n",
      "control_model.input_blocks.10.0.out_layers.3.weight\n",
      "control_model.input_blocks.10.0.out_layers.3.bias\n",
      "control_model.input_blocks.11.0.in_layers.0.weight\n",
      "control_model.input_blocks.11.0.in_layers.0.bias\n",
      "control_model.input_blocks.11.0.in_layers.2.weight\n",
      "control_model.input_blocks.11.0.in_layers.2.bias\n",
      "control_model.input_blocks.11.0.emb_layers.1.weight\n",
      "control_model.input_blocks.11.0.emb_layers.1.bias\n",
      "control_model.input_blocks.11.0.out_layers.0.weight\n",
      "control_model.input_blocks.11.0.out_layers.0.bias\n",
      "control_model.input_blocks.11.0.out_layers.3.weight\n",
      "control_model.input_blocks.11.0.out_layers.3.bias\n",
      "control_model.zero_convs.0.0.weight\n",
      "control_model.zero_convs.0.0.bias\n",
      "control_model.zero_convs.1.0.weight\n",
      "control_model.zero_convs.1.0.bias\n",
      "control_model.zero_convs.2.0.weight\n",
      "control_model.zero_convs.2.0.bias\n",
      "control_model.zero_convs.3.0.weight\n",
      "control_model.zero_convs.3.0.bias\n",
      "control_model.zero_convs.4.0.weight\n",
      "control_model.zero_convs.4.0.bias\n",
      "control_model.zero_convs.5.0.weight\n",
      "control_model.zero_convs.5.0.bias\n",
      "control_model.zero_convs.6.0.weight\n",
      "control_model.zero_convs.6.0.bias\n",
      "control_model.zero_convs.7.0.weight\n",
      "control_model.zero_convs.7.0.bias\n",
      "control_model.zero_convs.8.0.weight\n",
      "control_model.zero_convs.8.0.bias\n",
      "control_model.zero_convs.9.0.weight\n",
      "control_model.zero_convs.9.0.bias\n",
      "control_model.zero_convs.10.0.weight\n",
      "control_model.zero_convs.10.0.bias\n",
      "control_model.zero_convs.11.0.weight\n",
      "control_model.zero_convs.11.0.bias\n",
      "control_model.input_hint_block.0.weight\n",
      "control_model.input_hint_block.0.bias\n",
      "control_model.input_hint_block.2.weight\n",
      "control_model.input_hint_block.2.bias\n",
      "control_model.input_hint_block.4.weight\n",
      "control_model.input_hint_block.4.bias\n",
      "control_model.input_hint_block.6.weight\n",
      "control_model.input_hint_block.6.bias\n",
      "control_model.input_hint_block.8.weight\n",
      "control_model.input_hint_block.8.bias\n",
      "control_model.input_hint_block.10.weight\n",
      "control_model.input_hint_block.10.bias\n",
      "control_model.input_hint_block.12.weight\n",
      "control_model.input_hint_block.12.bias\n",
      "control_model.input_hint_block.14.weight\n",
      "control_model.input_hint_block.14.bias\n",
      "control_model.middle_block.0.in_layers.0.weight\n",
      "control_model.middle_block.0.in_layers.0.bias\n",
      "control_model.middle_block.0.in_layers.2.weight\n",
      "control_model.middle_block.0.in_layers.2.bias\n",
      "control_model.middle_block.0.emb_layers.1.weight\n",
      "control_model.middle_block.0.emb_layers.1.bias\n",
      "control_model.middle_block.0.out_layers.0.weight\n",
      "control_model.middle_block.0.out_layers.0.bias\n",
      "control_model.middle_block.0.out_layers.3.weight\n",
      "control_model.middle_block.0.out_layers.3.bias\n",
      "control_model.middle_block.1.norm.weight\n",
      "control_model.middle_block.1.norm.bias\n",
      "control_model.middle_block.1.proj_in.weight\n",
      "control_model.middle_block.1.proj_in.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.gamma\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.norm.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.norm.bias\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.attn.to_q.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.attn.to_k.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.attn.to_v.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.attn.to_out.0.weight\n",
      "control_model.middle_block.1.transformer_blocks.0.adapter.attn.to_out.0.bias\n",
      "control_model.middle_block.1.proj_out.weight\n",
      "control_model.middle_block.1.proj_out.bias\n",
      "control_model.middle_block.2.in_layers.0.weight\n",
      "control_model.middle_block.2.in_layers.0.bias\n",
      "control_model.middle_block.2.in_layers.2.weight\n",
      "control_model.middle_block.2.in_layers.2.bias\n",
      "control_model.middle_block.2.emb_layers.1.weight\n",
      "control_model.middle_block.2.emb_layers.1.bias\n",
      "control_model.middle_block.2.out_layers.0.weight\n",
      "control_model.middle_block.2.out_layers.0.bias\n",
      "control_model.middle_block.2.out_layers.3.weight\n",
      "control_model.middle_block.2.out_layers.3.bias\n",
      "control_model.middle_block_out.0.weight\n",
      "control_model.middle_block_out.0.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.2.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.2.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.2.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.2.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.3.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.3.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.3.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.3.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.4.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.4.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.4.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.4.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.5.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.5.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage1.5.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage1.5.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.0.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.0.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.2.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.2.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.2.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.2.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.3.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.3.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.3.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.3.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.4.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.4.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.4.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.4.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.5.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.5.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage2.5.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage2.5.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.0.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.0.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.0.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.0.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.1.0.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.1.1.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.stage3.1.3.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.weight\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.bias\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_mean\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.running_var\n",
      "face_feature_extractor.extractor.arcface.stage3.1.4.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.sep.weight\n",
      "face_feature_extractor.extractor.arcface.sep_bn.weight\n",
      "face_feature_extractor.extractor.arcface.sep_bn.bias\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_mean\n",
      "face_feature_extractor.extractor.arcface.sep_bn.running_var\n",
      "face_feature_extractor.extractor.arcface.sep_bn.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.prelu.weight\n",
      "face_feature_extractor.extractor.arcface.bn2.weight\n",
      "face_feature_extractor.extractor.arcface.bn2.bias\n",
      "face_feature_extractor.extractor.arcface.bn2.running_mean\n",
      "face_feature_extractor.extractor.arcface.bn2.running_var\n",
      "face_feature_extractor.extractor.arcface.bn2.num_batches_tracked\n",
      "face_feature_extractor.extractor.arcface.linear.weight\n",
      "face_feature_extractor.extractor.arcface.linear.bias\n",
      "face_feature_extractor.extractor.arcface.features.weight\n",
      "face_feature_extractor.extractor.arcface.features.bias\n",
      "face_feature_extractor.extractor.arcface.features.running_mean\n",
      "face_feature_extractor.extractor.arcface.features.running_var\n",
      "face_feature_extractor.extractor.arcface.features.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "for key in scratch_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key control_model.zero_convs.0.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.0.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.1.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.1.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.2.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.2.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.3.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.3.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.4.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.4.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.5.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.5.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.6.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.6.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.7.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.7.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.8.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.8.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.9.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.9.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.10.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.10.0.bias not in pretrained weights\n",
      "key control_model.zero_convs.11.0.weight not in pretrained weights\n",
      "key control_model.zero_convs.11.0.bias not in pretrained weights\n",
      "key control_model.input_hint_block.0.weight not in pretrained weights\n",
      "key control_model.input_hint_block.0.bias not in pretrained weights\n",
      "key control_model.input_hint_block.2.weight not in pretrained weights\n",
      "key control_model.input_hint_block.2.bias not in pretrained weights\n",
      "key control_model.input_hint_block.4.weight not in pretrained weights\n",
      "key control_model.input_hint_block.4.bias not in pretrained weights\n",
      "key control_model.input_hint_block.6.weight not in pretrained weights\n",
      "key control_model.input_hint_block.6.bias not in pretrained weights\n",
      "key control_model.input_hint_block.8.weight not in pretrained weights\n",
      "key control_model.input_hint_block.8.bias not in pretrained weights\n",
      "key control_model.input_hint_block.10.weight not in pretrained weights\n",
      "key control_model.input_hint_block.10.bias not in pretrained weights\n",
      "key control_model.input_hint_block.12.weight not in pretrained weights\n",
      "key control_model.input_hint_block.12.bias not in pretrained weights\n",
      "key control_model.input_hint_block.14.weight not in pretrained weights\n",
      "key control_model.input_hint_block.14.bias not in pretrained weights\n",
      "key control_model.middle_block_out.0.weight not in pretrained weights\n",
      "key control_model.middle_block_out.0.bias not in pretrained weights\n"
     ]
    }
   ],
   "source": [
    "new_final_weights = {}\n",
    "for key in scratch_dict:\n",
    "    if key in pretrained_weights and 'control_model.' not in key:\n",
    "        new_final_weights[key] = pretrained_weights[key]\n",
    "    else:\n",
    "        new_key = key.replace('control_model.', 'model.diffusion_model.')\n",
    "        if new_key in pretrained_weights:\n",
    "            new_final_weights[key] = pretrained_weights[new_key]\n",
    "        else:\n",
    "            new_final_weights[key] = scratch_dict[key]\n",
    "            print('key {} not in pretrained weights'.format(key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_final_weights, '/data1/wc_log/zxy/ckpt/v3.7_adapter-begin.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pretrained_weights['control_model.input_blocks.8.0.out_layers.3.weight'].cuda()\n",
    "b = contrast_weights['control_model.input_blocks.8.0.out_layers.3.weight']\n",
    "distance = a - b\n",
    "print(distance.data)\n",
    "print(distance.min())\n",
    "print(distance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pretrained_weights:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = pretrained_weights\n",
    "mapper_dict = mapper_weights\n",
    "\n",
    "# for key in target_dict:\n",
    "#     if key in scratch_dict:\n",
    "#         scratch_dict[key] = target_dict[key]\n",
    "#     else:\n",
    "#         print('key {} not in scratch_dict'.format(key))\n",
    "\n",
    "\n",
    "for key in pretrained_weights:\n",
    "    if 'cond_stage_model.mapper' in key:\n",
    "        pretrained_weights[key] = mapper_dict[key]\n",
    "        print('key {} in mapper_dict'.format(key))\n",
    "\n",
    "# 保存 scratch_dict\n",
    "torch.save(pretrained_weights, '/data1/wc_log/zxy/ckpt/v3.5.2-begin.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_keys = list(pretrained_weights.keys())\n",
    "for key in pretrained_weights_keys:\n",
    "    print(key)\n",
    "    # prefix = key.split('.', 1)[0]\n",
    "    # if prefix == 'cond_stage_model':\n",
    "    #     del pretrained_weights[key]\n",
    "\n",
    "# for key in condition_weight:\n",
    "#     add_key = 'cond_stage_model.' + key\n",
    "#     pretrained_weights[add_key] = condition_weight[key].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v2.yaml')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "# print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = {}\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  CLIPVisionModel\n",
    "from ControlNet.ldm.modules.encoders.xf import LayerNorm, Transformer\n",
    "\n",
    "version=\"openai/clip-vit-large-patch14\"\n",
    "transformer = CLIPVisionModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "mapper = Transformer(\n",
    "                n_ctx = 257,\n",
    "                width = 1024,\n",
    "                layers = 5,\n",
    "                heads = 8,\n",
    "            )\n",
    "final_ln = LayerNorm(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = transformer(pixel_values=image)\n",
    "    print('last_hidden_state : ', outputs.last_hidden_state.shape)\n",
    "    print('pooler_output : ', outputs.pooler_output.shape)\n",
    "\n",
    "    z = outputs.last_hidden_state\n",
    "    print('z : ', z.shape)\n",
    "    z = mapper(z)\n",
    "    print('z mapper: ', z.shape)\n",
    "    z = final_ln(z)\n",
    "    print('z final: ', z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox\n",
    "\n",
    "def smooth_mask(mask_image):\n",
    "    mask_image = cv2.GaussianBlur(mask_image, (11, 11), 11)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def get_align_image(bbox, img, reshape_size = 224):\n",
    "    h, w, _ = img.shape\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size))\n",
    "    aligned_img = np.asarray(aligned_img)\n",
    "    return aligned_img\n",
    "\n",
    "source_mask_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/mask_00000067.jpg'\n",
    "source_mask = np.asarray(Image.open(source_mask_path)) # (H, W)\n",
    "source_mask = smooth_mask(source_mask)\n",
    "\n",
    "plt.imshow(source_mask, cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = '/data0/wc_data/VFHQ/train/Clip+Y8k-XLGO2SY+P0+C0+F950-1055/00000067.png'\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\"))\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = mask_find_bbox(source_mask)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image)\n",
    "plt.imshow(source_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test random mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def smooth_mask(mask_image, ksize=(11, 11), sigmaX= 11, sigmaY= 11):\n",
    "    # need to be applied in data preprocess, and drop this\n",
    "    # GaussianBlur again to reduce mask edge serrate\n",
    "    mask_image = cv2.GaussianBlur(mask_image, ksize, sigmaX=sigmaX, sigmaY = sigmaY)\n",
    "    mask_image = np.where( (mask_image <= 0), 0, 255).astype('uint8')\n",
    "    return mask_image\n",
    "\n",
    "def mask_find_bbox(mask):\n",
    "    mask_col = np.sum(mask, axis= 0)\n",
    "    mask_row = np.sum(mask, axis= 1)\n",
    "\n",
    "    left = np.where(mask_col >= 255)[0][0]\n",
    "    right = np.where(mask_col >= 255)[0][-1]\n",
    "    up = np.where(mask_row >= 255)[0][0]\n",
    "    down = np.where(mask_row >= 255)[0][-1]\n",
    "\n",
    "    bbox = [left, up, right, down]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000021.png'\n",
    "target_mask_path = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/mask_00000021.jpg'\n",
    "\n",
    "# target_image_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "# target_mask_path = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/mask_00000053.jpg'\n",
    "\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\"))\n",
    "target_mask_image = np.asarray(Image.open(target_mask_path))\n",
    "\n",
    "plt.imshow(target_mask_image, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(55, 55), sigmaX= 33, sigmaY= 33)\n",
    "import random\n",
    "random_int = random.sample(range(-15, 20), 4)\n",
    "print(random_int)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(33, 33), sigmaX= 33, sigmaY= 33)\n",
    "# target_mask_image_big = smooth_mask(target_mask_image,ksize=(11, 11), sigmaX= 33, sigmaY= 33)\n",
    "target_mask_image_big = smooth_mask(target_mask_image,ksize=(33 + random_int[0]*2, 33 + random_int[1]*2), sigmaX= 33 + random_int[2]*2, sigmaY= 43 + random_int[3]*2)\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlarged_box = mask_find_bbox(target_mask_image_big)\n",
    "random_point_nums = 50\n",
    "\n",
    "x_coords = np.random.randint(enlarged_box[0], enlarged_box[2], (random_point_nums, 1))\n",
    "y_coords = np.random.randint(enlarged_box[1], enlarged_box[3], (random_point_nums, 1))\n",
    "points = np.concatenate([x_coords, y_coords], axis= 1)\n",
    "\n",
    "pixel_values = target_mask_image_big[x_coords, y_coords]\n",
    "mask = np.concatenate([pixel_values == 0, pixel_values == 0], axis= 1)\n",
    "black_points = points[mask]\n",
    "black_points = np.reshape(black_points, (-1, 2))\n",
    "\n",
    "hull = cv2.convexHull(black_points)\n",
    "\n",
    "\n",
    "# blackbg = np.zeros((512, 512), dtype=np.uint8)\n",
    "# for hull_one in hull:\n",
    "#     blackbg = cv2.circle(blackbg, hull_one[0], radius=3, color=(255, 255, 255), thickness=-1)\n",
    "# plt.imshow(blackbg, cmap= 'bone')\n",
    "\n",
    "# # 将这些点作为轮廓生成一个面，面里面填充纯白色\n",
    "cv2.fillPoly(target_mask_image_big, [black_points], 255)\n",
    "\n",
    "# # 显示图像\n",
    "plt.imshow(target_mask_image_big, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_image_distance = target_mask_image_big - target_mask_image\n",
    "plt.imshow(target_mask_image_distance, cmap= 'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_masked = cv2.bitwise_and(target_image, target_image, mask = target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_background = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image_big) # get masked\n",
    "plt.imshow(target_image_background)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test id loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd7e70d5b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from utils.Face_Alignment.align_faces import warp_and_crop_face_tensor, get_reference_facial_points\n",
    "from utils.Face_Alignment.retinaface.detector import RetinafaceDetector\n",
    "\n",
    "image_a_1 = '/data1/wc_log/zxy/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000034.png'\n",
    "image_a_2 = '/data1/wc_log/zxy/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000025.png'\n",
    "\n",
    "image_b_1 = '/home/wenchi/zxy/HSD/compare_3.png'\n",
    "image_b_2 = '/home/wenchi/zxy/HSD/compare_4.png'\n",
    "\n",
    "detector = RetinafaceDetector(pretrained_path='/home/wenchi/zxy/HSD/ControlNet/utils/Face_Alignment/retinaface/weights/mobilenet0.25_Final.pth', \n",
    "                              type='cuda')\n",
    "\n",
    "\n",
    "def crop_align_image(img: torch.Tensor, detector: RetinafaceDetector, output_size = (112, 112)) -> torch.Tensor:\n",
    "    '''\n",
    "    Crop and align a face image.\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): The input image tensor, with shape (3, 512, 512) and values in the range of 0~255.\n",
    "        detector (RetinafaceDetector): The face detector used to detect the facial landmarks.\n",
    "        output_size (Tuple[int, int]): The output size of the aligned face image. Default is (112, 112).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The aligned face image tensor, with shape (1, 3, 112, 112) and values in the range of 0~255.\n",
    "    '''\n",
    "    \n",
    "    _, facial5points = detector.detect_faces(img)\n",
    "\n",
    "    # detect no face\n",
    "    if len(facial5points) == 0:\n",
    "        return torch.zeros((1, 3, 112, 112))\n",
    "\n",
    "    facial5points = np.reshape(facial5points[0], (2, 5))\n",
    "\n",
    "    default_square = True\n",
    "    inner_padding_factor = 0.25\n",
    "    outer_padding = (0, 0)\n",
    "\n",
    "    # get the reference 5 landmarks position in the crop settings\n",
    "    reference_5pts = get_reference_facial_points(\n",
    "        output_size, inner_padding_factor, outer_padding, default_square)\n",
    "\n",
    "    # dst_img = warp_and_crop_face(raw, facial5points, reference_5pts, crop_size)\n",
    "    dst_img = warp_and_crop_face_tensor(img, facial5points, reference_pts=reference_5pts, crop_size=output_size) # tensor, (1, 3, 112, 112), 0~1, RGB\n",
    "    return dst_img\n",
    "\n",
    "dst_img_a1 = np.asarray(Image.open(image_b_1).convert(\"RGB\"))\n",
    "dst_img_a2 = np.asarray(Image.open(image_b_2).convert(\"RGB\"))\n",
    "\n",
    "dst_img_a1 = torch.from_numpy(dst_img_a1.transpose(2, 0, 1).copy()).float().unsqueeze(0) # tensor (1, 3, 512, 512), 0~255, RGB\n",
    "dst_img_a2 = torch.from_numpy(dst_img_a2.transpose(2, 0, 1).copy()).float().unsqueeze(0) # tensor (1, 3, 512, 512), 0~255, RGB\n",
    "\n",
    "dst_img_a1 = crop_align_image(dst_img_a1, detector) # tensor (1, 3, 112, 112), 0~255, RGB\n",
    "dst_img_a2 = crop_align_image(dst_img_a2, detector) # tensor (1, 3, 112, 112), 0~255, RGB\n",
    "\n",
    "plt.imshow(dst_img_a2.squeeze(0).permute(1, 2, 0) / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.arcface.nets.arcface import Arcface as arcface\n",
    "\n",
    "model_path='/home/wenchi/zxy/HSD/ControlNet/utils/arcface/model_data/arcface_mobilenet_v1.pth'\n",
    "\n",
    "model = arcface(backbone='mobilenetv1', mode=\"predict\").eval()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8382], device='cuda:0', grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "dst_img_a1 = ((dst_img_a1 - 127.5) / 127.5).cuda() # (1, 3, 112, 112), -1~1\n",
    "output1 = model(dst_img_a1)\n",
    "\n",
    "dst_img_a2 = ((dst_img_a2 - 127.5) / 127.5).cuda() # (1, 3, 112, 112), -1~1\n",
    "output2 = model(dst_img_a2)\n",
    "\n",
    "# calculate cosine similarity\n",
    "import torch.nn.functional as F\n",
    "distance = 1.0 - F.cosine_similarity(output1, output2, dim=1)\n",
    "distance.backward()\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_a_1 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000034.png'\n",
    "image_a_2 = '/data0/wc_data/VFHQ/test/Clip+D4BdpI6h1As+P1+C0+F809-925/00000005.png'\n",
    "\n",
    "image_b_1 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000053.png'\n",
    "image_b_2 = '/data0/wc_data/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000023.png'\n",
    "# image_b_1 = image_a_2\n",
    "\n",
    "\n",
    "image_a_1 = np.asarray(Image.open(image_a_1).convert(\"RGB\"))\n",
    "image_a_2 = np.asarray(Image.open(image_a_2).convert(\"RGB\"))\n",
    "image_b_1 = np.asarray(Image.open(image_b_1).convert(\"RGB\"))\n",
    "image_b_2 = np.asarray(Image.open(image_b_2).convert(\"RGB\"))\n",
    "\n",
    "image_a_1 = torch.from_numpy((image_a_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_a_2 = torch.from_numpy((image_a_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_1 = torch.from_numpy((image_b_1.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "image_b_2 = torch.from_numpy((image_b_2.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1))\n",
    "\n",
    "image_batch_a = image_a_1.unsqueeze(0)\n",
    "image_batch_b = image_a_2.unsqueeze(0)\n",
    "\n",
    "image_batch_a = torch.concatenate([image_a_1.unsqueeze(0), image_a_2.unsqueeze(0)])\n",
    "image_batch_b = torch.concatenate([image_b_1.unsqueeze(0), image_b_2.unsqueeze(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_a = get_batch_id(image_batch_a, face_detector, arcface_model) # B, 1024\n",
    "feature_b = get_batch_id(image_batch_b, face_detector, arcface_model)\n",
    "\n",
    "loss = cosine_distance(feature_a, feature_b, dim=1)\n",
    "print(loss)\n",
    "print(str(np.around(loss[0].numpy(), decimals=3)))\n",
    "print(loss.mean())\n",
    "print(loss.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "torch.Size([4, 1024, 257, 1])\n",
      "torch.Size([4, 1024, 257, 1])\n",
      "torch.Size([4, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ldm.modules.attention import SpatialTransformer\n",
    "from einops import rearrange\n",
    "from modules.util import EqualLinear\n",
    "\n",
    "spatial_depth = 2\n",
    "id_residual_ST = SpatialTransformer(1024, 8, 128, depth=spatial_depth, \n",
    "                                                 context_dim=[1024]*spatial_depth, disable_self_attn=False, \n",
    "                                                 use_linear=True, use_checkpoint=False)\n",
    "\n",
    "\n",
    "img_embedding = torch.rand(4, 257, 1024)\n",
    "test_feature = rearrange(img_embedding, 'b t c -> b c t 1').contiguous()\n",
    "print(test_feature.shape)\n",
    "\n",
    "id_feature = torch.rand(4, 1, 1024).cuda()\n",
    "\n",
    "test_feature_1 = torch.rand(4, 1024, 1, 1).cuda()\n",
    "\n",
    "id_residual_ST = id_residual_ST.cuda()\n",
    "test_feature = test_feature.cuda()\n",
    "\n",
    "out = id_residual_ST(test_feature, id_feature)\n",
    "print(test_feature.shape)\n",
    "\n",
    "global_proj_out = EqualLinear(1024, 768, lr_mul=0.01, activation=\"fused_lrelu\")\n",
    "global_proj_out = nn.Linear(1024, 768)\n",
    "global_proj_out = global_proj_out\n",
    "out = global_proj_out(img_embedding)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13148735e92407d4e5779ae154c1cf483ec3f984d296a4cf4fc2e020ad66c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infernce hsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import argparse, os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchvision\n",
    "\n",
    "from cldm.model import create_model, load_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--outdir\",\n",
    "    type=str,\n",
    "    nargs=\"?\",\n",
    "    help=\"dir to write results to\",\n",
    "    default=\"inference_outputs\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ddim_steps\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"number of ddim sampling steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fixed_code\",\n",
    "    action='store_true',\n",
    "    help=\"if enabled, uses the same starting code across samples \",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--scale\",\n",
    "    type=float,\n",
    "    default=3.0,\n",
    "    help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--unconditional_guidance_id\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    help=\"unconditional guidance scale only id (True) or id+CLIP (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config\",\n",
    "    type=str,\n",
    "    default=\"/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.4.yaml\",\n",
    "    help=\"path to config which constructs model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/ckpt/v3.4-epoch=492-global_step=820619.0.ckpt\",\n",
    "    help=\"path to checkpoint of model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arcface_ckpt\",\n",
    "    type=str,\n",
    "    default=\"utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth\",\n",
    "    help=\"path to checkpoint of arcface model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    type=int,\n",
    "    default=42,\n",
    "    help=\"the seed (for reproducible sampling)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--precision\",\n",
    "    type=str,\n",
    "    help=\"evaluate at this precision\",\n",
    "    choices=[\"full\", \"autocast\"],\n",
    "    default=\"autocast\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source_image_path\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/VFHQ/train/Clip+_4QwQlIADHQ+P0+C0+F944-1155/00000000.png\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source_mask_path\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/VFHQ/train/Clip+_4QwQlIADHQ+P0+C0+F944-1155/mask_00000080.jpg\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target_image_path\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/VFHQ/train/Clip+_4QwQlIADHQ+P0+C0+F944-1155/00000000.png\"\n",
    "    # default=\"/data0/wc_data/VFHQ/test/Clip+ka64cyDltpI+P0+C2+F2883-3062/00000036.png\"|\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target_mask_path\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/VFHQ/train/Clip+_4QwQlIADHQ+P0+C0+F944-1155/mask_00000080.jpg\"\n",
    "    # default=\"/data0/wc_data/VFHQ/test/Clip+ka64cyDltpI+P0+C2+F2883-3062/mask_00000036.jpg\"\n",
    ")\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "seed_everything(opt.seed)\n",
    "config_path = opt.config\n",
    "ckpt_path = opt.ckpt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.4.yaml]\n",
      "Loaded state_dict from [/data1/wc_log/zxy/ckpt/v3.4-epoch=492-global_step=820619.0.ckpt]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(config_path).cpu()\n",
    "model.load_state_dict(load_state_dict(ckpt_path, location='cpu'))\n",
    "model = model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "# model.first_stage_cuda = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/deca_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:548: UserWarning: Mtl file does not exist: /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "# get face detector\n",
    "from face_alignment.detection.sfd.sfd_detector import SFDDetector\n",
    "face_detector = SFDDetector(device='cuda')\n",
    "\n",
    "# get ArcFace\n",
    "from utils.arcface_pytorch.models.resnet import resnet_face18\n",
    "arcface_model = resnet_face18(False)   # arcface get id information\n",
    "state_dict = torch.load(opt.arcface_ckpt)\n",
    "arcface_model.load_state_dict(state_dict)\n",
    "arcface_model.cuda()\n",
    "arcface_model.eval()\n",
    "\n",
    "# get condition_Branch\n",
    "from modules.ConditionBranch import Condition_Branch\n",
    "condition_branch = Condition_Branch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreprocess.getBBox import get_BBox\n",
    "from DataPreprocess.detect3DMM import detect_3dmm\n",
    "from DataPreprocess.getIdInformation import get_id_featurs\n",
    "\n",
    "from HSD_dataset import smooth_expand_mask, mask_find_bbox, get_align_image, get_tensor_clip\n",
    "\n",
    "source_image_path = opt.source_image_path\n",
    "target_image_path = opt.target_image_path\n",
    "source_mask_path = opt.source_mask_path\n",
    "target_mask_path = opt.target_mask_path\n",
    "\n",
    "imgs_numpy_list = [np.asarray(Image.open(imagepath).convert(\"RGB\")) for imagepath in [source_image_path, target_image_path]]\n",
    "imgs_numpy = np.asarray(imgs_numpy_list).transpose(0, 3, 1, 2) # (2, 3, H, W)\n",
    "imgs_tensor = torch.from_numpy(imgs_numpy)\n",
    "batch_size = 2\n",
    "\n",
    "# get bbox\n",
    "bboxlist = get_BBox(imgs_tensor, face_detector, batch_size=batch_size) # (2, 4)\n",
    "\n",
    "# get id information\n",
    "id_features = get_id_featurs(bboxlist, imgs_numpy, arcface_model, batch_size = batch_size) # (2, 512)\n",
    "\n",
    "# get 3DMM dict\n",
    "dict_3DMM = detect_3dmm(bboxlist, imgs_numpy, condition_branch.deca, batch_size = batch_size) # source, target\n",
    "\n",
    "# get combined 3DMM dict\n",
    "shape_code_new = dict_3DMM['shape'][0]\n",
    "tex_code_new = dict_3DMM['tex'][0]\n",
    "tforms_new = dict_3DMM['tforms'][1]\n",
    "exp_code_new = dict_3DMM['exp'][1]\n",
    "pose_code_new = dict_3DMM['pose'][1]\n",
    "cam_code_new = dict_3DMM['cam'][1]\n",
    "light_code_new = dict_3DMM['light'][1]\n",
    "\n",
    "new_code_dict = {\n",
    "    'tforms':tforms_new,\n",
    "    'shape':shape_code_new,\n",
    "    'tex':tex_code_new,\n",
    "    'exp':exp_code_new,\n",
    "    'pose':pose_code_new,\n",
    "    'cam':cam_code_new,\n",
    "    'light':light_code_new\n",
    "}\n",
    "\n",
    "\n",
    "# read images\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\"))\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\"))\n",
    "source_mask_image = np.asarray(Image.open(source_mask_path))\n",
    "target_mask_image = np.asarray(Image.open(target_mask_path))\n",
    "\n",
    "# smooth and enlarge masks\n",
    "source_mask_image = smooth_expand_mask(source_mask_image, ksize=(11, 11), sigmaX=11, sigmaY=11)\n",
    "target_mask_image = smooth_expand_mask(target_mask_image, ksize=(55, 55), sigmaX=33, sigmaY=33)\n",
    "\n",
    "# process source image\n",
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask_image) # get masked\n",
    "bbox = mask_find_bbox(source_mask_image)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image) # get align & resized source image, (224, 224, 3), numpy, 0~255\n",
    "source_tensor = get_tensor_clip()(source_image.copy())\n",
    "\n",
    "# get masked images (background)\n",
    "bg_image = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image)\n",
    "\n",
    "target_image = (target_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize target images to [-1, 1].\n",
    "source_image = (source_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize source images to [-1, 1].\n",
    "target_mask_image = np.expand_dims(target_mask_image.astype(np.float32) / 255.0, axis=0)\n",
    "\n",
    "bg_image = bg_image.astype(np.float32) / 255.0\n",
    "bg_image = torch.from_numpy(bg_image.transpose(2, 0, 1)) # (3, h, w)\n",
    "\n",
    "rendered_images = condition_branch(new_code_dict) # (1, 3, h, w)\n",
    "\n",
    "# all should be tensor and cuda\n",
    "target_image = torch.from_numpy(target_image).unsqueeze(0).cuda()\n",
    "target_mask_image = torch.from_numpy(target_mask_image).unsqueeze(0).cuda()\n",
    "bg_image = bg_image.unsqueeze(0).cuda()\n",
    "source_tensor = source_tensor.unsqueeze(0).cuda()\n",
    "id_feature = torch.from_numpy(id_features[0]).unsqueeze(0).cuda()\n",
    "source_image = torch.from_numpy(source_image).unsqueeze(0).cuda()\n",
    "rendered_images = rendered_images.cuda()\n",
    "\n",
    "batch = dict(target=target_image, \n",
    "             mask=target_mask_image, \n",
    "             background=bg_image, \n",
    "             source_global=source_tensor, \n",
    "             source_id=id_feature, \n",
    "             source_image=source_image, \n",
    "             hint=rendered_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test predict x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get input\n",
    "x_start, c = model.get_input(batch, model.first_stage_key, bs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simply predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "t = torch.randint(0, 1, (x_start.shape[0],), device=model.device).long()\n",
    "print(t)\n",
    "\n",
    "noise = torch.randn_like(x_start[:,:4,:,:])\n",
    "x_noisy = model.q_sample(x_start=x_start[:,:4,:,:], t=t, noise=noise)\n",
    "x_noisy = torch.cat((x_noisy, x_start[:,4:,:,:]),dim=1)\n",
    "\n",
    "model_out = model.apply_model(x_noisy, t, c)\n",
    "\n",
    "x_recon = model.predict_start_from_noise(x_noisy[:,:4,:,:], t, model_out) # predicted x0\n",
    "\n",
    "recon_x0 = model.decode_first_stage(x_recon)\n",
    "recon_x0 = recon_x0.detach().cpu().numpy()\n",
    "recon_x0 = ((recon_x0.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "\n",
    "plt.imshow(recon_x0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ddim predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
    "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
    "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
    "    steps_out = ddim_timesteps + 1\n",
    "    \n",
    "    return steps_out\n",
    "\n",
    "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
    "    # select alphas for computing the variance schedule\n",
    "    alphas = alphacums[ddim_timesteps]\n",
    "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
    "\n",
    "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
    "    \n",
    "    return sigmas, alphas, alphas_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.ddim_hsd import DDIMSampler\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "N = 1\n",
    "c_cat, c = c[\"c_concat\"][0][:N], c[\"c_crossattn\"][0][:N]\n",
    "\n",
    "samples, z_denoise_row = model.sample_log(cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]},\n",
    "                                                     batch_size=N, ddim=True,\n",
    "                                                     ddim_steps=50, eta=0., rest=x_start[:N,4:,:,:],log_every_t = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "for i in range(len(z_denoise_row['x_inter'])):\n",
    "    z_denoise = z_denoise_row['x_inter'][i]\n",
    "    z_denoise = model.decode_first_stage(z_denoise)\n",
    "    z_denoise = z_denoise.detach().cpu().numpy()\n",
    "    z_denoise = ((z_denoise.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "    plt.imshow(z_denoise)\n",
    "    plt.title(i)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "# ddim set\n",
    "\n",
    "## set input\n",
    "N = 1\n",
    "shape = (1, 4, 64, 64)\n",
    "img = torch.randn(shape, device=model.device)\n",
    "c_cat, c = c[\"c_concat\"][0][:N], c[\"c_crossattn\"][0][:N]\n",
    "cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "\n",
    "## set timestep\n",
    "timesteps = make_ddim_timesteps(num_ddim_timesteps=50, num_ddpm_timesteps=1000)\n",
    "time_range = np.flip(timesteps)\n",
    "total_steps = timesteps.shape[0]\n",
    "\n",
    "## set parameters\n",
    "to_torch = lambda x: x.clone().detach().to(torch.float32).to(model.device)\n",
    "alphas_cumprod = model.alphas_cumprod\n",
    "ddim_eta=0.\n",
    "ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(), ddim_timesteps=timesteps, eta=ddim_eta)\n",
    "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
    "\n",
    "\n",
    "b, *_, device = *x_noisy.shape, x_noisy.device\n",
    "\n",
    "from tqdm import tqdm\n",
    "iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
    "\n",
    "for i, step in enumerate(time_range):\n",
    "    index = total_steps - i - 1\n",
    "    ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "\n",
    "    ## cat background & mask\n",
    "    img = torch.cat((img, x_start[:,4:,:,:]), dim=1)\n",
    "\n",
    "    ## run model\n",
    "    e_t = model.apply_model(img, ts, cond)\n",
    "\n",
    "    ## get parameters\n",
    "    a_t = torch.full((b, 1, 1, 1), ddim_alphas[index], device=device)\n",
    "    sigma_t = torch.full((b, 1, 1, 1), ddim_sigmas[index], device=device)\n",
    "    a_prev = torch.full((b, 1, 1, 1), ddim_alphas_prev[index], device=device)\n",
    "    sqrt_one_minus_at = torch.full((b, 1, 1, 1), ddim_sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "    ## cal x_prev\n",
    "    pred_x0 = (img[:,:4,:,:] - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "    dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "    noise = sigma_t * noise_like(dir_xt.shape, device, repeat = False)\n",
    "    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "\n",
    "    img = x_prev \n",
    "    # img = pred_x0\n",
    "    \n",
    "\n",
    "    ## show image\n",
    "    recon = model.decode_first_stage(img)\n",
    "    recon = recon.detach().cpu().numpy()\n",
    "    recon = ((recon.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "\n",
    "    plt.imshow(recon)\n",
    "    plt.title(step)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:09<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    images = model.log_images(batch, ddim_steps = opt.ddim_steps, unconditional_guidance_scale = opt.scale, unconditional_guidance_id = opt.unconditional_guidance_id)\n",
    "    images['d_background'] = batch['background']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### track memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "params = list(model.control_model.parameters())\n",
    "\n",
    "params += list(model.cond_stage_model.parameters())\n",
    "\n",
    "opt = torch.optim.AdamW(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(1, 9, 64, 64)\n",
    "z.requires_grad = True\n",
    "z_cuda = z.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.first_stage_model.decode(z_cuda[:,:4,:,:])\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (a + 1).mean() - a.mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del a\n",
    "a.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process image & make grid & calculate id loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(u: torch.Tensor, v: torch.Tensor, dim) -> torch.Tensor:\n",
    "    # 计算两个张量之间的余弦距离\n",
    "    return 1.0 - F.cosine_similarity(u, v, dim=dim)\n",
    "\n",
    "def process_a_image(bbox, img, reshape_size = 128):\n",
    "    ##  to process a img for arcface\n",
    "    ##  img : numpy, uint8, 0~255, (3, h, w), RGB\n",
    "    img = img.transpose(1, 2, 0) # 3, h, w -> h, w, 3\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size), Image.LANCZOS).convert('L')\n",
    "    img = np.asarray(aligned_img)\n",
    "    img = np.dstack((img, np.fliplr(img)))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = img[:, np.newaxis, :, :]\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    img -= 127.5\n",
    "    img /= 127.5\n",
    "\n",
    "    # 2 * 1 * 128 * 128\n",
    "\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_id_feature(img_batch):\n",
    "    ## this method get id feature of a image batch\n",
    "    ## img_batch: tensor,  (B, 3, size, size), RGB, -1~1\n",
    "\n",
    "    img_batch = (img_batch + 1.0) * 127.5\n",
    "\n",
    "    output_batch = face_detector.detect_from_batch(img_batch)\n",
    "\n",
    "    preprocessed_batch = []\n",
    "    for i in range(len(output_batch)):\n",
    "        BBox = output_batch[i][0].astype('int32')\n",
    "        img = img_batch[i].numpy().astype('uint8')\n",
    "\n",
    "        img = process_a_image(BBox, img)\n",
    "        preprocessed_batch.append(img)\n",
    "    preprocessed_batch = np.concatenate(preprocessed_batch, axis=0) # preprocessed_batch : (B*2, 1, 128, 128)\n",
    "    preprocessed_batch = torch.from_numpy(preprocessed_batch)\n",
    "\n",
    "    output_batch = arcface_model(preprocessed_batch.cuda())\n",
    "    fe_1 = output_batch[::2]\n",
    "    fe_2 = output_batch[1::2]\n",
    "    feature = torch.cat((fe_1, fe_2), dim=1)\n",
    "\n",
    "    return feature\n",
    "\n",
    "@torch.no_grad()   \n",
    "def get_id_loss(source_img, output_img):\n",
    "\n",
    "    # 判断source_img的dim，如果 dim ==2 则是feature而不用处理\n",
    "    if len(source_img.shape) == 2:\n",
    "        source_id_feature = source_img\n",
    "    else:\n",
    "        source_id_feature = get_id_feature(source_img) # (B, 1024)\n",
    "\n",
    "    if len(output_img.shape) == 2:\n",
    "        ouput_id_feature = output_img\n",
    "    else:\n",
    "        ouput_id_feature = get_id_feature(output_img) # (B, 1024)\n",
    "    \n",
    "    loss = cosine_distance(source_id_feature, ouput_id_feature, dim=1)\n",
    "    return loss\n",
    "\n",
    "def draw_id_loss(loss, grid):\n",
    "    # grid: (H*B, W, 3), np.uint8, 0~255\n",
    "    _, W, _ = grid.shape\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    fontScale = 1\n",
    "    fix_up = 30\n",
    "\n",
    "    grid = grid.copy()\n",
    "    for i in range(loss.shape[0]):\n",
    "        text = str(np.around(loss[i].numpy(), decimals=4))\n",
    "        x = 0\n",
    "        y = W * i + fix_up\n",
    "        cv2.putText(grid, text, (x,y), font, fontScale=fontScale, color=color, thickness=thickness)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def log_local(images, img_size = 512, id_loss_samples=None, id_loss_cfg=None):\n",
    "    ## 获取当前时间秒数\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    root = opt.outdir\n",
    "    keys = sorted(images.keys())\n",
    "    grid_list = []\n",
    "    for k in keys:\n",
    "        B = images[k].shape[0]\n",
    "        grid = torchvision.utils.make_grid(images[k], nrow=1)\n",
    "        grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n",
    "        grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "        grid = grid.numpy()\n",
    "        grid = (grid * 255).astype(np.uint8) # H*B, W, 3\n",
    "        if images[k].shape[2] != img_size:\n",
    "            if B == 1:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size, img_size)))\n",
    "            else:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size + 4, (img_size * B + 2 * (B + 1)))))\n",
    "\n",
    "        # draw id loss\n",
    "        if k == 'samples' and id_loss_samples is not None:\n",
    "            grid = draw_id_loss(id_loss_samples, grid)\n",
    "\n",
    "        if k == 'samples_cfg_scale' and id_loss_cfg is not None:\n",
    "            grid = draw_id_loss(id_loss_cfg, grid)\n",
    "\n",
    "        grid_list.append(grid)\n",
    "\n",
    "    full_grid = np.concatenate(grid_list, axis=1)\n",
    "\n",
    "    filename = \"{}_scale[{}].png\".format(now, opt.scale)\n",
    "    path = os.path.join(root, filename)\n",
    "    os.makedirs(os.path.split(path)[0], exist_ok=True)\n",
    "    Image.fromarray(full_grid).save(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1223884/2654311574.py:30: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  aligned_img = aligned_img.resize((reshape_size, reshape_size), Image.LANCZOS).convert('L')\n"
     ]
    }
   ],
   "source": [
    "# jupyter inline show\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "for k in images:\n",
    "    images[k] = images[k].float()\n",
    "    if isinstance(images[k], torch.Tensor):\n",
    "        images[k] = images[k].detach().cpu()\n",
    "        images[k] = torch.clamp(images[k], -1., 1.)\n",
    "\n",
    "# 用 source_image 计算 id loss\n",
    "source_image = batch['source_image'].detach().cpu()\n",
    "\n",
    "# # 用 target_image 计算 id loss\n",
    "# source_image = batch['target'].detach().cpu()\n",
    "\n",
    "# # 用 保存的 pkl 计算 id loss\n",
    "# pkl_path = os.path.join(opt.target_image_path.split('/000')[0], 'id.pkl')\n",
    "# with open(pkl_path, 'rb') as f:\n",
    "#     source_image_list = pickle.load(f)\n",
    "# source_image = torch.from_numpy(source_image_list[0]).unsqueeze(0).cuda()\n",
    "\n",
    "id_loss_samples = get_id_loss(source_image.clone(), images['samples']).detach().cpu()\n",
    "id_loss_cfg = get_id_loss(source_image.clone(), images['samples_cfg_scale']).detach().cpu()\n",
    "\n",
    "log_local(images, id_loss_samples = id_loss_samples, id_loss_cfg = id_loss_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infernce hsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import argparse, os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchvision\n",
    "\n",
    "from cldm.model import create_model, load_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--outdir\",\n",
    "    type=str,\n",
    "    nargs=\"?\",\n",
    "    help=\"dir to write results to\",\n",
    "    default=\"inference_outputs\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ddim_steps\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"number of ddim sampling steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fixed_code\",\n",
    "    action='store_true',\n",
    "    help=\"if enabled, uses the same starting code across samples \",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--scale\",\n",
    "    type=float,\n",
    "    default=3.0,\n",
    "    help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--unconditional_guidance_id\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    help=\"unconditional guidance scale only id (True) or id+CLIP (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config\",\n",
    "    type=str,\n",
    "    default=\"/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.6.yaml\",\n",
    "    help=\"path to config which constructs model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/ckpt/v3.6.1-single-jjk_video1_Feature_2-epoch=88-global_step=13349.0.ckpt\",\n",
    "    help=\"path to checkpoint of model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arcface_ckpt\",\n",
    "    type=str,\n",
    "    default=\"utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth\",\n",
    "    help=\"path to checkpoint of arcface model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    type=int,\n",
    "    default=42,\n",
    "    help=\"the seed (for reproducible sampling)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--precision\",\n",
    "    type=str,\n",
    "    help=\"evaluate at this precision\",\n",
    "    choices=[\"full\", \"autocast\"],\n",
    "    default=\"autocast\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source_image_path\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/custom_dataset/jjk_video_1/train_1/00000027.png\"\n",
    "    # default=\"/data1/wc_log/zxy/VFHQ/train/Clip+YcztD7HIm9I+P0+C1+F1731-2627/00000007.png\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target_image_path\",\n",
    "    type=str,\n",
    "    # default=\"/data1/wc_log/zxy/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000002.png\"\n",
    "    default=\"/data1/wc_log/zxy/VFHQ/train/Clip+YcztD7HIm9I+P0+C1+F1731-2627/00000007.png\" # 蓝衣肌肉\n",
    "    # default=\"/data1/wc_log/zxy/VFHQ/test/Clip+okx7B5ggBvo+P0+C0+F3046-3157/00000002.png\"\n",
    "    # default=\"/data1/wc_log/zxy/VFHQ/test/Clip+1qf8dZpLED0+P2+C1+F5731-5855/00000002.png\"\n",
    "\n",
    ")\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "seed_everything(opt.seed)\n",
    "config_path = opt.config\n",
    "ckpt_path = opt.ckpt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'logit_scale', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.6.yaml]\n",
      "Loaded state_dict from [/data1/wc_log/zxy/ckpt/v3.6.1-single-jjk_video1_Feature_2-epoch=88-global_step=13349.0.ckpt]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(config_path).cpu()\n",
    "model.load_state_dict(load_state_dict(ckpt_path, location='cpu'))\n",
    "\n",
    "model.first_stage_cuda = 0\n",
    "model.cond_stage_cuda = 0\n",
    "\n",
    "model = model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/deca_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:548: UserWarning: Mtl file does not exist: /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "from modules.Face_feature import FaceFeatureExtractor\n",
    "from utils.face_parse.model import BiSeNet\n",
    "from face_alignment.detection.sfd.sfd_detector import SFDDetector\n",
    "\n",
    "# get face detector\n",
    "face_detector = SFDDetector(device='cuda')\n",
    "\n",
    "## face feature extractor\n",
    "face_feature_net = FaceFeatureExtractor('/home/wenchi/zxy/HSD/ControlNet/utils/Face_Alignment/retinaface/weights/mobilenet0.25_Final.pth', \n",
    "                                        '/home/wenchi/zxy/HSD/ControlNet/utils/arcface/model_data/arcface_mobilenet_v1.pth', \n",
    "                                        output_size=(112, 112), device = 'cuda')\n",
    "\n",
    "## face parse\n",
    "face_parse_net = BiSeNet(n_classes=19)\n",
    "save_pth = '/home/wenchi/zxy/HSD/ControlNet/utils/face_parse/res/cp/79999_iter.pth'\n",
    "face_parse_net.load_state_dict(torch.load(save_pth))\n",
    "face_parse_net.cuda()\n",
    "face_parse_net.eval()\n",
    "\n",
    "# get condition_Branch\n",
    "condition_branch = Condition_Branch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreprocess.getBBox import get_BBox\n",
    "from DataPreprocess.detect3DMM import detect_3dmm\n",
    "\n",
    "from dataset.HSD_dataset import smooth_expand_mask, mask_find_bbox, get_align_image, get_tensor_clip, render_add_gaze, square_mask\n",
    "\n",
    "def get_mask(face_parse_net, source_image, target_image):\n",
    "        # use face parse net to get mask\n",
    "        source_image_tensor = get_tensor_clip()(source_image.copy())\n",
    "        target_image_tensor = get_tensor_clip()(target_image.copy())\n",
    "\n",
    "        input_batch = torch.stack([source_image_tensor, target_image_tensor], dim=0) # (2, 3, 512, 512)\n",
    "        input_batch = input_batch.cuda()\n",
    "        out_batch = face_parse_net(input_batch)[0].argmax(1).cpu()\n",
    "\n",
    "        parsing = out_batch.numpy() # numpy but size is 512 , 2 * 512 * 512\n",
    "\n",
    "        source_parsing = out_batch[0]\n",
    "        target_parsing = out_batch[1]\n",
    "\n",
    "        source_mask_image = np.where( (source_parsing == 0)|(source_parsing == 14)|(source_parsing == 16), 0, 255).astype('uint8')  # 0 or 255\n",
    "        target_mask_image = np.where( (target_parsing == 0)|(target_parsing == 14)|(target_parsing == 16), 0, 255).astype('uint8')  # 0 or 255\n",
    "        target_gaze_mask = np.where( (target_parsing == 4)|(target_parsing == 5), 255, 0).astype('uint8')\n",
    "\n",
    "        return source_mask_image, target_mask_image, target_gaze_mask\n",
    "\n",
    "source_image_path = opt.source_image_path\n",
    "target_image_path = opt.target_image_path\n",
    "\n",
    "imgs_numpy_list = [np.asarray(Image.open(imagepath).convert(\"RGB\").resize((512, 512))) for imagepath in [source_image_path, target_image_path]]\n",
    "imgs_numpy = np.asarray(imgs_numpy_list).transpose(0, 3, 1, 2) # (2, 3, H, W)\n",
    "imgs_tensor = torch.from_numpy(imgs_numpy)\n",
    "batch_size = 2\n",
    "\n",
    "# get bbox\n",
    "bboxlist = get_BBox(imgs_tensor, face_detector, batch_size=batch_size) # (2, 4)\n",
    "\n",
    "# get 3DMM dict\n",
    "dict_3DMM = detect_3dmm(bboxlist, imgs_numpy, condition_branch.deca, batch_size = batch_size) # source, target\n",
    "\n",
    "# get combined 3DMM dict\n",
    "shape_code_new = dict_3DMM['shape'][0]\n",
    "tex_code_new = dict_3DMM['tex'][0]\n",
    "tforms_new = dict_3DMM['tforms'][1]\n",
    "exp_code_new = dict_3DMM['exp'][1]\n",
    "pose_code_new = dict_3DMM['pose'][1]\n",
    "cam_code_new = dict_3DMM['cam'][1]\n",
    "light_code_new = dict_3DMM['light'][1]\n",
    "\n",
    "new_code_dict = {\n",
    "    'tforms':tforms_new,\n",
    "    'shape':shape_code_new,\n",
    "    'tex':tex_code_new,\n",
    "    'exp':exp_code_new,\n",
    "    'pose':pose_code_new,\n",
    "    'cam':cam_code_new,\n",
    "    'light':light_code_new\n",
    "}\n",
    "\n",
    "\n",
    "# read images\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\").resize((512, 512)))\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\").resize((512, 512)))\n",
    "source_mask_image, target_mask_image, target_gaze_mask = get_mask(face_parse_net, source_image, target_image)\n",
    "\n",
    "# smooth and enlarge masks\n",
    "source_mask_image = smooth_expand_mask(source_mask_image, ksize=(11, 11), sigmaX=11, sigmaY=11)\n",
    "target_mask_image = smooth_expand_mask(target_mask_image, ksize=(55, 55), sigmaX=33, sigmaY=33)\n",
    "\n",
    "# get target square mask\n",
    "# target_mask_image = square_mask(target_mask_image)\n",
    "\n",
    "# process source image\n",
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask_image) # get masked\n",
    "bbox = mask_find_bbox(source_mask_image)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image) # get align & resized source image, (224, 224, 3), numpy, 0~255\n",
    "with torch.no_grad():\n",
    "    id_feature = face_feature_net(np.expand_dims(source_image, axis=0)).squeeze(0) # get id feature\n",
    "source_tensor = get_tensor_clip()(source_image.copy())\n",
    "\n",
    "# get masked images (background)\n",
    "bg_image = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image)\n",
    "\n",
    "target_image = (target_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize target images to [-1, 1].\n",
    "source_image = (source_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize source images to [-1, 1].\n",
    "target_mask_image = np.expand_dims(target_mask_image.astype(np.float32) / 255.0, axis=0)\n",
    "\n",
    "bg_image = bg_image.astype(np.float32) / 255.0\n",
    "bg_image = torch.from_numpy(bg_image.transpose(2, 0, 1)) # (3, h, w)\n",
    "\n",
    "rendered_images = condition_branch(new_code_dict) # (1, 3, h, w)\n",
    "rendered_image = render_add_gaze(rendered_images[0], target_gaze_mask, target_image)\n",
    "\n",
    "# all should be tensor and cuda\n",
    "target_image = torch.from_numpy(target_image).unsqueeze(0).cuda()\n",
    "target_mask_image = torch.from_numpy(target_mask_image).unsqueeze(0).cuda()\n",
    "bg_image = bg_image.unsqueeze(0).cuda()\n",
    "source_tensor = source_tensor.unsqueeze(0).cuda()\n",
    "id_feature = id_feature.unsqueeze(0).cuda()\n",
    "source_image = torch.from_numpy(source_image).unsqueeze(0).cuda()\n",
    "rendered_image = torch.from_numpy(rendered_image).unsqueeze(0).cuda()\n",
    "\n",
    "batch = dict(target=target_image, \n",
    "             mask=target_mask_image, \n",
    "             background=bg_image, \n",
    "             source_global=source_tensor, \n",
    "             source_id=id_feature, \n",
    "             source_image=source_image, \n",
    "             hint=rendered_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test predict x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get input\n",
    "x_start, c = model.get_input(batch, model.first_stage_key, bs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simply predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randint(0, model.num_timesteps, (x_start.shape[0],), device=model.device).long()\n",
    "t = torch.randint(0, 1, (x_start.shape[0],), device=model.device).long()\n",
    "print(t)\n",
    "\n",
    "noise = torch.randn_like(x_start[:,:4,:,:])\n",
    "x_noisy = model.q_sample(x_start=x_start[:,:4,:,:], t=t, noise=noise)\n",
    "x_noisy = torch.cat((x_noisy, x_start[:,4:,:,:]),dim=1)\n",
    "\n",
    "model_out = model.apply_model(x_noisy, t, c)\n",
    "\n",
    "x_recon = model.predict_start_from_noise(x_noisy[:,:4,:,:], t, model_out) # predicted x0\n",
    "\n",
    "recon_x0 = model.decode_first_stage(x_recon)\n",
    "recon_x0 = recon_x0.detach().cpu().numpy()\n",
    "recon_x0 = ((recon_x0.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "\n",
    "plt.imshow(recon_x0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ddim predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
    "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
    "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
    "    steps_out = ddim_timesteps + 1\n",
    "    \n",
    "    return steps_out\n",
    "\n",
    "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
    "    # select alphas for computing the variance schedule\n",
    "    alphas = alphacums[ddim_timesteps]\n",
    "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
    "\n",
    "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
    "    \n",
    "    return sigmas, alphas, alphas_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.ddim_hsd import DDIMSampler\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "N = 1\n",
    "c_cat, c = c[\"c_concat\"][0][:N], c[\"c_crossattn\"][0][:N]\n",
    "\n",
    "samples, z_denoise_row = model.sample_log(cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]},\n",
    "                                                     batch_size=N, ddim=True,\n",
    "                                                     ddim_steps=50, eta=0., rest=x_start[:N,4:,:,:],log_every_t = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "for i in range(len(z_denoise_row['x_inter'])):\n",
    "    z_denoise = z_denoise_row['x_inter'][i]\n",
    "    z_denoise = model.decode_first_stage(z_denoise)\n",
    "    z_denoise = z_denoise.detach().cpu().numpy()\n",
    "    z_denoise = ((z_denoise.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "    plt.imshow(z_denoise)\n",
    "    plt.title(i)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "# ddim set\n",
    "\n",
    "## set input\n",
    "N = 1\n",
    "shape = (1, 4, 64, 64)\n",
    "img = torch.randn(shape, device=model.device)\n",
    "c_cat, c = c[\"c_concat\"][0][:N], c[\"c_crossattn\"][0][:N]\n",
    "cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "\n",
    "## set timestep\n",
    "timesteps = make_ddim_timesteps(num_ddim_timesteps=50, num_ddpm_timesteps=1000)\n",
    "time_range = np.flip(timesteps)\n",
    "total_steps = timesteps.shape[0]\n",
    "\n",
    "## set parameters\n",
    "to_torch = lambda x: x.clone().detach().to(torch.float32).to(model.device)\n",
    "alphas_cumprod = model.alphas_cumprod\n",
    "ddim_eta=0.\n",
    "ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(), ddim_timesteps=timesteps, eta=ddim_eta)\n",
    "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
    "\n",
    "\n",
    "b, *_, device = *x_noisy.shape, x_noisy.device\n",
    "\n",
    "from tqdm import tqdm\n",
    "iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
    "\n",
    "for i, step in enumerate(time_range):\n",
    "    index = total_steps - i - 1\n",
    "    ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "\n",
    "    ## cat background & mask\n",
    "    img = torch.cat((img, x_start[:,4:,:,:]), dim=1)\n",
    "\n",
    "    ## run model\n",
    "    e_t = model.apply_model(img, ts, cond)\n",
    "\n",
    "    ## get parameters\n",
    "    a_t = torch.full((b, 1, 1, 1), ddim_alphas[index], device=device)\n",
    "    sigma_t = torch.full((b, 1, 1, 1), ddim_sigmas[index], device=device)\n",
    "    a_prev = torch.full((b, 1, 1, 1), ddim_alphas_prev[index], device=device)\n",
    "    sqrt_one_minus_at = torch.full((b, 1, 1, 1), ddim_sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "    ## cal x_prev\n",
    "    pred_x0 = (img[:,:4,:,:] - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "    dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "    noise = sigma_t * noise_like(dir_xt.shape, device, repeat = False)\n",
    "    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "\n",
    "    img = x_prev \n",
    "    # img = pred_x0\n",
    "    \n",
    "\n",
    "    ## show image\n",
    "    recon = model.decode_first_stage(img)\n",
    "    recon = recon.detach().cpu().numpy()\n",
    "    recon = ((recon.squeeze(0).transpose(1, 2, 0) + 1.0 ) * 127.5).astype(np.uint8)\n",
    "\n",
    "    plt.imshow(recon)\n",
    "    plt.title(step)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:05<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:08<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    images = model.log_images(batch, ddim_steps = opt.ddim_steps, unconditional_guidance_scale = opt.scale, unconditional_guidance_id = opt.unconditional_guidance_id)\n",
    "    images['d_background'] = batch['background']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### track memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "params = list(model.control_model.parameters())\n",
    "\n",
    "params += list(model.cond_stage_model.parameters())\n",
    "\n",
    "opt = torch.optim.AdamW(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(1, 9, 64, 64)\n",
    "z.requires_grad = True\n",
    "z_cuda = z.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.first_stage_model.decode(z_cuda[:,:4,:,:])\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (a + 1).mean() - a.mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del a\n",
    "a.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process image & make grid & calculate id loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_id_loss(loss, grid):\n",
    "    # grid: (H*B, W, 3), np.uint8, 0~255\n",
    "    _, W, _ = grid.shape\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    fontScale = 1\n",
    "    fix_up = 30\n",
    "\n",
    "    grid = grid.copy()\n",
    "    for i in range(loss.shape[0]):\n",
    "        text = str(np.around(loss[i].numpy(), decimals=4))\n",
    "        x = 0\n",
    "        y = W * i + fix_up\n",
    "        cv2.putText(grid, text, (x,y), font, fontScale=fontScale, color=color, thickness=thickness)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def log_local(images, img_size = 512, id_loss_samples=None, id_loss_cfg=None):\n",
    "    ## 获取当前时间秒数\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    root = opt.outdir\n",
    "    keys = sorted(images.keys())\n",
    "    grid_list = []\n",
    "    for k in keys:\n",
    "        B = images[k].shape[0]\n",
    "        grid = torchvision.utils.make_grid(images[k], nrow=1)\n",
    "        grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n",
    "        grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "        grid = grid.numpy()\n",
    "        grid = (grid * 255).astype(np.uint8) # H*B, W, 3\n",
    "        if images[k].shape[2] != img_size:\n",
    "            if B == 1:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size, img_size)))\n",
    "            else:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size + 4, (img_size * B + 2 * (B + 1)))))\n",
    "\n",
    "        # draw id loss\n",
    "        if k == 'samples' and id_loss_samples is not None:\n",
    "            grid = draw_id_loss(id_loss_samples, grid)\n",
    "\n",
    "        if k == 'samples_cfg_scale' and id_loss_cfg is not None:\n",
    "            grid = draw_id_loss(id_loss_cfg, grid)\n",
    "\n",
    "        grid_list.append(grid)\n",
    "\n",
    "    full_grid = np.concatenate(grid_list, axis=1)\n",
    "\n",
    "    filename = \"{}_scale[{}].png\".format(now, opt.scale)\n",
    "    path = os.path.join(root, filename)\n",
    "    os.makedirs(os.path.split(path)[0], exist_ok=True)\n",
    "    Image.fromarray(full_grid).save(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter inline show\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# id loss calculate & draw for cross_id\n",
    "source_image = batch['source_image']\n",
    "\n",
    "source_image_feature = face_feature_net((source_image.clone() + 1.0) * 127.5)\n",
    "sample_feature = face_feature_net((images['samples'] + 1.0) * 127.5)\n",
    "sample_cfg_feature = face_feature_net((images['samples_cfg_scale'] + 1.0) * 127.5)\n",
    "\n",
    "id_loss_samples = 1.0 - F.cosine_similarity(source_image_feature, sample_feature, dim=1).detach().cpu() # get id loss\n",
    "id_loss_cfg = 1.0 - F.cosine_similarity(source_image_feature, sample_cfg_feature, dim=1).detach().cpu() # get id loss\n",
    "# id_loss_samples = None\n",
    "# id_loss_cfg = None\n",
    "\n",
    "\n",
    "for k in images:\n",
    "    images[k] = images[k].float()\n",
    "    if isinstance(images[k], torch.Tensor):\n",
    "        images[k] = images[k].detach().cpu()\n",
    "        images[k] = torch.clamp(images[k], -1., 1.)\n",
    "\n",
    "log_local(images, id_loss_samples = id_loss_samples, id_loss_cfg = id_loss_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

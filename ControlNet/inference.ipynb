{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infernce hsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import argparse, os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchvision\n",
    "\n",
    "from cldm.model import create_model, load_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--outdir\",\n",
    "    type=str,\n",
    "    nargs=\"?\",\n",
    "    help=\"dir to write results to\",\n",
    "    default=\"inference_outputs\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ddim_steps\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"number of ddim sampling steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fixed_code\",\n",
    "    action='store_true',\n",
    "    help=\"if enabled, uses the same starting code across samples \",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--scale\",\n",
    "    type=float,\n",
    "    default=3.0,\n",
    "    help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--unconditional_guidance_id\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    help=\"unconditional guidance scale only id (True) or id+CLIP (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config\",\n",
    "    type=str,\n",
    "    default=\"/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.4.yaml\",\n",
    "    help=\"path to config which constructs model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt\",\n",
    "    type=str,\n",
    "    default=\"/data1/wc_log/zxy/ckpt/v3.2-epoch=805-global_step=781819.0.ckpt\",\n",
    "    help=\"path to checkpoint of model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arcface_ckpt\",\n",
    "    type=str,\n",
    "    default=\"utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth\",\n",
    "    help=\"path to checkpoint of arcface model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    type=int,\n",
    "    default=42,\n",
    "    help=\"the seed (for reproducible sampling)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--precision\",\n",
    "    type=str,\n",
    "    help=\"evaluate at this precision\",\n",
    "    choices=[\"full\", \"autocast\"],\n",
    "    default=\"autocast\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source_image_path\",\n",
    "    type=str,\n",
    "    default=\"/data0/wc_data/VFHQ/test/Clip+ka64cyDltpI+P0+C2+F2883-3062/00000066.png\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source_mask_path\",\n",
    "    type=str,\n",
    "    default=\"/data0/wc_data/VFHQ/test/Clip+ka64cyDltpI+P0+C2+F2883-3062/mask_00000066.jpg\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target_image_path\",\n",
    "    type=str,\n",
    "    # default=\"/data0/wc_data/VFHQ/test/Clip+g7Dd3DFXE9I+P0+C0+F8141-8266/00000009.png\"\n",
    "    default=\"/data0/wc_data/VFHQ/train/Clip+Zay8fFwgqYo+P0+C2+F3679-3955/00000001.png\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target_mask_path\",\n",
    "    type=str,\n",
    "    # default=\"/data0/wc_data/VFHQ/test/Clip+g7Dd3DFXE9I+P0+C0+F8141-8266/mask_00000009.jpg\"\n",
    "    default=\"/data0/wc_data/VFHQ/train/Clip+Zay8fFwgqYo+P0+C2+F3679-3955/mask_00000001.jpg\"\n",
    ")\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "seed_everything(opt.seed)\n",
    "config_path = opt.config\n",
    "ckpt_path = opt.ckpt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'visual_projection.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_projection.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v3.4.yaml]\n",
      "Loaded state_dict from [/data1/wc_log/zxy/ckpt/v3.2-epoch=805-global_step=781819.0.ckpt]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(config_path).cpu()\n",
    "model.load_state_dict(load_state_dict(ckpt_path, location='cpu'))\n",
    "model = model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/deca_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:548: UserWarning: Mtl file does not exist: /home/wenchi/zxy/HSD/ControlNet/utils/DECA/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "# get face detector\n",
    "from face_alignment.detection.sfd.sfd_detector import SFDDetector\n",
    "face_detector = SFDDetector(device='cuda')\n",
    "\n",
    "# get ArcFace\n",
    "from utils.arcface_pytorch.models.resnet import resnet_face18\n",
    "arcface_model = resnet_face18(False)   # arcface get id information\n",
    "state_dict = torch.load(opt.arcface_ckpt)\n",
    "arcface_model.load_state_dict(state_dict)\n",
    "arcface_model.cuda()\n",
    "arcface_model.eval()\n",
    "\n",
    "# get condition_Branch\n",
    "from modules.ConditionBranch import Condition_Branch\n",
    "condition_branch = Condition_Branch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreprocess.getBBox import get_BBox\n",
    "from DataPreprocess.detect3DMM import detect_3dmm\n",
    "from DataPreprocess.getIdInformation import get_id_featurs\n",
    "\n",
    "from dataset.HSD_dataset import smooth_expand_mask, mask_find_bbox, get_align_image, get_tensor_clip\n",
    "\n",
    "source_image_path = opt.source_image_path\n",
    "target_image_path = opt.target_image_path\n",
    "source_mask_path = opt.source_mask_path\n",
    "target_mask_path = opt.target_mask_path\n",
    "\n",
    "imgs_numpy_list = [np.asarray(Image.open(imagepath).convert(\"RGB\")) for imagepath in [source_image_path, target_image_path]]\n",
    "imgs_numpy = np.asarray(imgs_numpy_list).transpose(0, 3, 1, 2) # (2, 3, H, W)\n",
    "imgs_tensor = torch.from_numpy(imgs_numpy)\n",
    "batch_size = 2\n",
    "\n",
    "# get bbox\n",
    "bboxlist = get_BBox(imgs_tensor, face_detector, batch_size=batch_size) # (2, 4)\n",
    "\n",
    "# get id information\n",
    "id_features = get_id_featurs(bboxlist, imgs_numpy, arcface_model, batch_size = batch_size) # (2, 512)\n",
    "\n",
    "# get 3DMM dict\n",
    "dict_3DMM = detect_3dmm(bboxlist, imgs_numpy, condition_branch.deca, batch_size = batch_size) # source, target\n",
    "\n",
    "# get combined 3DMM dict\n",
    "shape_code_new = dict_3DMM['shape'][0]\n",
    "tex_code_new = dict_3DMM['tex'][0]\n",
    "tforms_new = dict_3DMM['tforms'][1]\n",
    "exp_code_new = dict_3DMM['exp'][1]\n",
    "pose_code_new = dict_3DMM['pose'][1]\n",
    "cam_code_new = dict_3DMM['cam'][1]\n",
    "light_code_new = dict_3DMM['light'][1]\n",
    "\n",
    "new_code_dict = {\n",
    "    'tforms':tforms_new,\n",
    "    'shape':shape_code_new,\n",
    "    'tex':tex_code_new,\n",
    "    'exp':exp_code_new,\n",
    "    'pose':pose_code_new,\n",
    "    'cam':cam_code_new,\n",
    "    'light':light_code_new\n",
    "}\n",
    "\n",
    "\n",
    "# read images\n",
    "source_image = np.asarray(Image.open(source_image_path).convert(\"RGB\"))\n",
    "target_image = np.asarray(Image.open(target_image_path).convert(\"RGB\"))\n",
    "source_mask_image = np.asarray(Image.open(source_mask_path))\n",
    "target_mask_image = np.asarray(Image.open(target_mask_path))\n",
    "\n",
    "# smooth and enlarge masks\n",
    "source_mask_image = smooth_expand_mask(source_mask_image, ksize=(11, 11), sigmaX=11, sigmaY=11)\n",
    "target_mask_image = smooth_expand_mask(target_mask_image, ksize=(55, 55), sigmaX=33, sigmaY=33)\n",
    "\n",
    "# process source image\n",
    "source_image = cv2.bitwise_and(source_image, source_image, mask = source_mask_image) # get masked\n",
    "bbox = mask_find_bbox(source_mask_image)\n",
    "source_image = get_align_image(bbox=bbox, img=source_image) # get align & resized source image, (224, 224, 3), numpy, 0~255\n",
    "source_tensor = get_tensor_clip()(source_image.copy())\n",
    "\n",
    "# get masked images (background)\n",
    "bg_image = cv2.bitwise_and(target_image, target_image, mask = 255 - target_mask_image)\n",
    "\n",
    "target_image = (target_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize target images to [-1, 1].\n",
    "source_image = (source_image.astype(np.float32) / 127.5 - 1.0).transpose(2, 0, 1)  # Normalize source images to [-1, 1].\n",
    "target_mask_image = np.expand_dims(target_mask_image.astype(np.float32) / 255.0, axis=0)\n",
    "\n",
    "bg_image = bg_image.astype(np.float32) / 255.0\n",
    "bg_image = torch.from_numpy(bg_image.transpose(2, 0, 1)) # (3, h, w)\n",
    "\n",
    "rendered_images = condition_branch(new_code_dict) # (1, 3, h, w)\n",
    "\n",
    "# all should be tensor and cuda\n",
    "target_image = torch.from_numpy(target_image).unsqueeze(0).cuda()\n",
    "target_mask_image = torch.from_numpy(target_mask_image).unsqueeze(0).cuda()\n",
    "bg_image = bg_image.unsqueeze(0).cuda()\n",
    "source_tensor = source_tensor.unsqueeze(0).cuda()\n",
    "id_feature = torch.from_numpy(id_features[0]).unsqueeze(0).cuda()\n",
    "source_image = torch.from_numpy(source_image).unsqueeze(0).cuda()\n",
    "rendered_images = rendered_images.cuda()\n",
    "\n",
    "batch = dict(target=target_image, \n",
    "             mask=target_mask_image, \n",
    "             background=bg_image, \n",
    "             source_global=source_tensor, \n",
    "             source_id=id_feature, \n",
    "             source_image=source_image, \n",
    "             hint=rendered_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:06<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    images = model.log_images(batch, ddim_steps = opt.ddim_steps, unconditional_guidance_scale = opt.scale, unconditional_guidance_id = opt.unconditional_guidance_id)\n",
    "    images['d_background'] = batch['background']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process image & make grid & calculate id loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(u: torch.Tensor, v: torch.Tensor, dim) -> torch.Tensor:\n",
    "    # 计算两个张量之间的余弦距离\n",
    "    return 1.0 - F.cosine_similarity(u, v, dim=dim)\n",
    "\n",
    "def process_a_image(bbox, img, reshape_size = 128):\n",
    "    ##  to process a img for arcface\n",
    "    ##  img : numpy, uint8, 0~255, (3, h, w), RGB\n",
    "    img = img.transpose(1, 2, 0) # 3, h, w -> h, w, 3\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "    center_point = [int((x1 + x2) / 2), int((y1 + y2) / 2)] ## recalculate the center point\n",
    "    expand_size = int((y2 - y1) * 0.5) # expand_size -- half of the total crop size\n",
    "    crop_size = expand_size * 2\n",
    "\n",
    "    new_x1 = center_point[0] - expand_size\n",
    "    new_x2 = center_point[0] + expand_size\n",
    "    new_y1 = center_point[1] - expand_size\n",
    "    new_y2 = center_point[1] + expand_size\n",
    "\n",
    "    (crop_left, origin_left) = (0, new_x1) if new_x1 >= 0 else (-new_x1, 0)\n",
    "    (crop_right, origin_right) = (crop_size, new_x2) if new_x2 <= w else (w-new_x1, w)\n",
    "    (crop_top, origin_top) = (0, new_y1) if new_y1 >= 0 else (-new_y1, 0)\n",
    "    (crop_bottom, origin_bottom) = (crop_size, new_y2) if new_y2 <= h else (h-new_y1, h)\n",
    "\n",
    "    aligned_img = np.zeros((crop_size, crop_size, 3), dtype=np.uint8)\n",
    "    aligned_img[crop_top:crop_bottom, crop_left:crop_right] = img[origin_top:origin_bottom, origin_left:origin_right]\n",
    "    aligned_img = Image.fromarray(aligned_img)\n",
    "    aligned_img = aligned_img.resize((reshape_size, reshape_size), Image.LANCZOS).convert('L')\n",
    "    img = np.asarray(aligned_img)\n",
    "    img = np.dstack((img, np.fliplr(img)))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = img[:, np.newaxis, :, :]\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    img -= 127.5\n",
    "    img /= 127.5\n",
    "\n",
    "    # 2 * 1 * 128 * 128\n",
    "\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_id_feature(img_batch):\n",
    "    ## this method get id feature of a image batch\n",
    "    ## img_batch: tensor,  (B, 3, size, size), RGB, -1~1\n",
    "\n",
    "    img_batch = (img_batch + 1.0) * 127.5\n",
    "\n",
    "    output_batch = face_detector.detect_from_batch(img_batch)\n",
    "\n",
    "    preprocessed_batch = []\n",
    "    for i in range(len(output_batch)):\n",
    "        BBox = output_batch[i][0].astype('int32')\n",
    "        img = img_batch[i].numpy().astype('uint8')\n",
    "\n",
    "        img = process_a_image(BBox, img)\n",
    "        preprocessed_batch.append(img)\n",
    "    preprocessed_batch = np.concatenate(preprocessed_batch, axis=0) # preprocessed_batch : (B*2, 1, 128, 128)\n",
    "    preprocessed_batch = torch.from_numpy(preprocessed_batch)\n",
    "\n",
    "    output_batch = arcface_model(preprocessed_batch.cuda())\n",
    "    fe_1 = output_batch[::2]\n",
    "    fe_2 = output_batch[1::2]\n",
    "    feature = torch.cat((fe_1, fe_2), dim=1)\n",
    "\n",
    "    return feature\n",
    "\n",
    "@torch.no_grad()   \n",
    "def get_id_loss(source_img, output_img):\n",
    "    source_id_feature = get_id_feature(source_img)\n",
    "    ouput_id_feature = get_id_feature(output_img) # (B, 1024)\n",
    "\n",
    "    loss = cosine_distance(source_id_feature, ouput_id_feature, dim=1)\n",
    "    return loss\n",
    "\n",
    "def draw_id_loss(loss, grid):\n",
    "    # grid: (H*B, W, 3), np.uint8, 0~255\n",
    "    _, W, _ = grid.shape\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    fontScale = 1\n",
    "    fix_up = 30\n",
    "\n",
    "    grid = grid.copy()\n",
    "    for i in range(loss.shape[0]):\n",
    "        text = str(np.around(loss[i].numpy(), decimals=4))\n",
    "        x = 0\n",
    "        y = W * i + fix_up\n",
    "        cv2.putText(grid, text, (x,y), font, fontScale=fontScale, color=color, thickness=thickness)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def log_local(images, img_size = 512, id_loss_samples=None, id_loss_cfg=None):\n",
    "    ## 获取当前时间秒数\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    root = opt.outdir\n",
    "    keys = sorted(images.keys())\n",
    "    grid_list = []\n",
    "    for k in keys:\n",
    "        B = images[k].shape[0]\n",
    "        grid = torchvision.utils.make_grid(images[k], nrow=1)\n",
    "        grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n",
    "        grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "        grid = grid.numpy()\n",
    "        grid = (grid * 255).astype(np.uint8) # H*B, W, 3\n",
    "        if images[k].shape[2] != img_size:\n",
    "            if B == 1:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size, img_size)))\n",
    "            else:\n",
    "                grid = np.asarray(Image.fromarray(grid).resize((img_size + 4, (img_size * B + 2 * (B + 1)))))\n",
    "\n",
    "        # draw id loss\n",
    "        if k == 'samples' and id_loss_samples is not None:\n",
    "            grid = draw_id_loss(id_loss_samples, grid)\n",
    "\n",
    "        if k == 'samples_cfg_scale' and id_loss_cfg is not None:\n",
    "            grid = draw_id_loss(id_loss_cfg, grid)\n",
    "\n",
    "        grid_list.append(grid)\n",
    "\n",
    "    full_grid = np.concatenate(grid_list, axis=1)\n",
    "\n",
    "    filename = \"{}_scale[{}].png\".format(now, opt.scale)\n",
    "    path = os.path.join(root, filename)\n",
    "    os.makedirs(os.path.split(path)[0], exist_ok=True)\n",
    "    Image.fromarray(full_grid).save(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2010690/944269119.py:30: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  aligned_img = aligned_img.resize((reshape_size, reshape_size), Image.LANCZOS).convert('L')\n"
     ]
    }
   ],
   "source": [
    "# jupyter inline show\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k in images:\n",
    "    images[k] = images[k].float()\n",
    "    if isinstance(images[k], torch.Tensor):\n",
    "        images[k] = images[k].detach().cpu()\n",
    "        images[k] = torch.clamp(images[k], -1., 1.)\n",
    "\n",
    "source_image = batch['source_image'].detach().cpu()\n",
    "\n",
    "id_loss_samples = get_id_loss(source_image.clone(), images['samples']).detach().cpu()\n",
    "id_loss_cfg = get_id_loss(source_image.clone(), images['samples_cfg_scale']).detach().cpu()\n",
    "\n",
    "log_local(images, id_loss_samples = id_loss_samples, id_loss_cfg = id_loss_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

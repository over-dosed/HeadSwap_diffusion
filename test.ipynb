{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Feature_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch(   \n",
    "                reshape_channel= reshape_channel, \n",
    "                 reshape_depth = reshape_depth, \n",
    "                 num_resblocks = num_resblocks, \n",
    "                 upsample_channels = upsample_channels, \n",
    "                 downsample_channels = downsample_channels, \n",
    "                 arcface_path = arcface_path, \n",
    "                 resNext_path = resNext_path,\n",
    "                 linear_channels = linear_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_branch.cuda()\n",
    "\n",
    "data_for_id = torch.randn(4, 1024).cuda()\n",
    "data_for_global = torch.randn(4, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data0/wc_data/VFHQ/test/Clip+_HebIzK_LP4+P2+C1+F16589-16715/3DMM_feature.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "verts = data['trans_verts'] # (N, 5023, 3)\n",
    "mesh_target = verts[:4].cuda() # (4, 5023, 3)\n",
    "mesh_source = verts[4:8].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_branch(data_for_id, data_for_global, mesh_source, mesh_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Condition_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load /home/wenchi/zxy/HSD/utils/DECA/data/deca_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:546: UserWarning: Mtl file does not exist: /home/wenchi/zxy/HSD/utils/DECA/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "\n",
    "test_branch = Condition_Branch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def get_code_dict(code_dict, batch_size = 4, pose_threshold = 0.02):\n",
    "    # this method get original a clip code_dict as input\n",
    "    # return the indexs selected randomly and the corresponding combined code_dict\n",
    "\n",
    "    tforms = code_dict['tforms']\n",
    "    shape_code = code_dict['shape']\n",
    "    tex_code = code_dict['tex']\n",
    "    exp_code = code_dict['exp']\n",
    "    pose_code = code_dict['pose']\n",
    "    cam_code = code_dict['cam']\n",
    "    light_code = code_dict['light']\n",
    "\n",
    "    tforms_new = torch.zeros(batch_size, tforms.shape[1], tforms.shape[2])\n",
    "    shape_code_new = torch.zeros(batch_size, shape_code.shape[1])\n",
    "    tex_code_new = torch.zeros(batch_size, tex_code.shape[1])\n",
    "    exp_code_new = torch.zeros(batch_size, exp_code.shape[1])\n",
    "    pose_code_new = torch.zeros(batch_size, pose_code.shape[1])\n",
    "    cam_code_new = torch.zeros(batch_size, cam_code.shape[1])\n",
    "    light_code_new = torch.zeros(batch_size, light_code.shape[1], light_code.shape[2])\n",
    "\n",
    "    total_num = pose_code.shape[0]\n",
    "    count = 0\n",
    "    index = []\n",
    "\n",
    "    while True:\n",
    "        a = random.randint(0, total_num-1)       # a for source\n",
    "        b = random.randint(0, total_num-1)       # b for target\n",
    "        if abs(torch.mean(pose_code[a] - pose_code[b])) >= pose_threshold:\n",
    "\n",
    "            # get combined code\n",
    "            tforms_new[count, :] = tforms[b]\n",
    "            shape_code_new[count, :] = shape_code[a]\n",
    "            tex_code_new[count, :] = tex_code[a]\n",
    "            exp_code_new[count, :] = exp_code[b]\n",
    "            pose_code_new[count, :] = pose_code[b]\n",
    "            cam_code_new[count, :] = cam_code[b]\n",
    "            light_code_new[count, :] = light_code[b]\n",
    "\n",
    "            # get index\n",
    "            index.append((a, b))\n",
    "\n",
    "            count +=1\n",
    "\n",
    "            if count == batch_size:\n",
    "                new_code_dict = {\n",
    "                    'tforms':tforms_new.cuda(),\n",
    "                    'shape':shape_code_new.cuda(),\n",
    "                    'tex':tex_code_new.cuda(),\n",
    "                    'exp':exp_code_new.cuda(),\n",
    "                    'pose':pose_code_new.cuda(),\n",
    "                    'cam':cam_code_new.cuda(),\n",
    "                    'light':light_code_new.cuda()\n",
    "                }\n",
    "                return new_code_dict, index\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "clip_path = '/data0/wc_data/VFHQ/train/Clip+xz26EN_LRa8+P0+C0+F4517-4639'\n",
    "\n",
    "with open(osp.join(clip_path, '3DMM_condition.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "codedict, index = get_code_dict(data)\n",
    "\n",
    "source_image_list = []\n",
    "target_image_list = []\n",
    "mask_image_list = []\n",
    "bg_image_list = []\n",
    "\n",
    "\n",
    "# get images\n",
    "for i in range(len(index)):\n",
    "    source_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][0]).zfill(8)))\n",
    "    target_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][1]).zfill(8)))\n",
    "    mask_image_path = osp.join(clip_path, 'mask_{}.jpg'.format(str(index[i][1]).zfill(8)))\n",
    "    source_image_list.append(np.asarray(Image.open(source_image_path).convert(\"RGB\")))\n",
    "    target_image_list.append(np.asarray(Image.open(target_image_path).convert(\"RGB\")))\n",
    "    mask_image_list.append(np.asarray(Image.open(mask_image_path)))\n",
    "\n",
    "# get masked images (background)\n",
    "for i in range(len(index)):\n",
    "    mask = mask_image_list[i]\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 11)\n",
    "    mask = np.where( (mask <= 0), 0, 255).astype('uint8')\n",
    "    bg_image_list.append(cv2.bitwise_and(target_image_list[i], target_image_list[i], mask = 255 - mask))\n",
    "\n",
    "source_images = np.asarray(source_image_list)\n",
    "target_images = np.asarray(target_image_list) # np.array, uint8, \n",
    "mask_images = np.asarray(mask_image_list)\n",
    "bg_images = np.asarray(bg_image_list)\n",
    "\n",
    "bg_images = torch.from_numpy((bg_images / 255.0).transpose(0, 3, 1, 2))\n",
    "bg_images = bg_images.cuda()\n",
    "\n",
    "# for key in codedict:\n",
    "#     print('key: {} has shape : {} '.format(key, str(codedict[key].shape)))\n",
    "\n",
    "out = test_branch(codedict, bg_images)\n",
    "\n",
    "Image_source = source_image_list[0]\n",
    "Image_target = target_image_list[0]\n",
    "Image_mask = np.tile(mask_image_list[0] , (3, 1, 1)).transpose(1, 2, 0)\n",
    "Image_bg = bg_image_list[0]\n",
    "Image_out = (out[0].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "Image_concat = np.concatenate((Image_source, Image_target, Image_mask, Image_bg, Image_out), axis= 1)\n",
    "a = Image.fromarray(Image_concat)\n",
    "a.save('/home/wenchi/zxy/HSD/test_condition.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test pose distance and test for pose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with open('/data0/wc_data/VFHQ/train/Clip+_aZphIp0KQE+P0+C1+F2675-2891/3DMM_condition.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "pose_code = data['pose']\n",
    "\n",
    "total_num = pose_code.shape[0]\n",
    " \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "row_list = []\n",
    "col_list = []\n",
    "# for i in range(1, 100):\n",
    "#     sum = 0\n",
    "#     for j in range(total_num - i):\n",
    "#         pose_1 = pose_code[j]\n",
    "#         pose_2 = pose_code[j+i]\n",
    "#         sum += torch.mean(pose_1 - pose_2)\n",
    "#     sum /= (total_num - i)\n",
    "#     print('间隔{}帧的图像pose 平均差值为{}'.format(i, sum))\n",
    "#     i_list.append(i)\n",
    "#     sum_list.append(sum)\n",
    "result = [0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(100000):\n",
    "    a = random.randint(0, total_num-1)\n",
    "    b = random.randint(0, total_num-1)\n",
    "\n",
    "    pose_1 = pose_code[a]\n",
    "    pose_2 = pose_code[b]\n",
    "    temp = torch.mean(pose_1 - pose_2)\n",
    "    index = min(abs(int(temp / 0.01)), 6)\n",
    "    result[index] +=1\n",
    "\n",
    "\n",
    "plt.scatter(['0~0.01', '0.01~0.02', '0.02~0.03', '0.03~0.04', '0.04~0.05', '0.05~0.06', '>=0.06'], result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3337, -1.2104, -0.6803, -1.8381,  0.2894,  2.4729])\n",
      "tensor(-0.2167)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.randn(6)\n",
    "print(a-b)\n",
    "print(torch.mean(a - b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test combine ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.system('export PYTHONPATH=/home/wenchi/zxy/HSD/ControlNet/')\n",
    "sys.path.append('/home/wenchi/zxy/HSD/ControlNet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_path = \"/data1/wc_log/zxy/paintbyexample_model.ckpt\"\n",
    "output_path = \"/data1/wc_log/zxy/control_pbe_ini.ckpt\"\n",
    "\n",
    "assert os.path.exists(input_path), 'Input model does not exist.'\n",
    "assert not os.path.exists(output_path), 'Output filename already exists.'\n",
    "assert os.path.exists(os.path.dirname(output_path)), 'Output path is not valid.'\n",
    "\n",
    "import torch\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "\n",
    "def get_node_name(name, parent_name):\n",
    "    if len(name) <= len(parent_name):\n",
    "        return False, ''\n",
    "    p = name[:len(parent_name)]\n",
    "    if p != parent_name:\n",
    "        return False, ''\n",
    "    return True, name[len(parent_name):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch(   \n",
    "                reshape_channel= reshape_channel, \n",
    "                 reshape_depth = reshape_depth, \n",
    "                 num_resblocks = num_resblocks, \n",
    "                 upsample_channels = upsample_channels, \n",
    "                 downsample_channels = downsample_channels, \n",
    "                 arcface_path = arcface_path, \n",
    "                 resNext_path = resNext_path,\n",
    "                 linear_channels = linear_channels)\n",
    "\n",
    "condition_weight = test_branch.state_dict()\n",
    "pretrained_weights = torch.load(input_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_keys = list(pretrained_weights.keys())\n",
    "for key in pretrained_weights_keys:\n",
    "    prefix = key.split('.', 1)[0]\n",
    "    if prefix == 'cond_stage_model':\n",
    "        del pretrained_weights[key]\n",
    "\n",
    "for key in condition_weight:\n",
    "    add_key = 'cond_stage_model.' + key\n",
    "    pretrained_weights[add_key] = condition_weight[key].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve.yaml')\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = {}\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13148735e92407d4e5779ae154c1cf483ec3f984d296a4cf4fc2e020ad66c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

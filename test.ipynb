{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Feature_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "reshape_channel = 32\n",
    "reshape_depth = 16\n",
    "num_resblocks = 6\n",
    "linear_channels = [3072, 2048, 1024, 768]\n",
    "upsample_channels = [3072, 2048, 1024, 512]\n",
    "downsample_channels = [512, 512, 512, 768]\n",
    "arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "test_branch = Feature_Branch(   \n",
    "                reshape_channel= reshape_channel, \n",
    "                 reshape_depth = reshape_depth, \n",
    "                 num_resblocks = num_resblocks, \n",
    "                 upsample_channels = upsample_channels, \n",
    "                 downsample_channels = downsample_channels, \n",
    "                 arcface_path = arcface_path, \n",
    "                 resNext_path = resNext_path,\n",
    "                 linear_channels = linear_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_branch.cuda()\n",
    "\n",
    "data_for_id = torch.randn(4, 1024).cuda()\n",
    "data_for_global = torch.randn(4, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data0/wc_data/VFHQ/test/Clip+_HebIzK_LP4+P2+C1+F16589-16715/3DMM_feature.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "verts = data['trans_verts'] # (N, 5023, 3)\n",
    "mesh_target = verts[:4].cuda() # (4, 5023, 3)\n",
    "mesh_source = verts[4:8].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_branch(data_for_id, data_for_global, mesh_source, mesh_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Condition_Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "trained model found. load /home/wenchi/zxy/HSD/utils/DECA/data/deca_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenchi/miniconda3/envs/diffusion/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:546: UserWarning: Mtl file does not exist: /home/wenchi/zxy/HSD/utils/DECA/data/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "from modules.ConditionBranch import Condition_Branch\n",
    "\n",
    "test_branch = Condition_Branch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def get_code_dict(code_dict, batch_size = 4, pose_threshold = 0.02):\n",
    "    # this method get original a clip code_dict as input\n",
    "    # return the indexs selected randomly and the corresponding combined code_dict\n",
    "\n",
    "    tforms = code_dict['tforms']\n",
    "    shape_code = code_dict['shape']\n",
    "    tex_code = code_dict['tex']\n",
    "    exp_code = code_dict['exp']\n",
    "    pose_code = code_dict['pose']\n",
    "    cam_code = code_dict['cam']\n",
    "    light_code = code_dict['light']\n",
    "\n",
    "    tforms_new = torch.zeros(batch_size, tforms.shape[1], tforms.shape[2])\n",
    "    shape_code_new = torch.zeros(batch_size, shape_code.shape[1])\n",
    "    tex_code_new = torch.zeros(batch_size, tex_code.shape[1])\n",
    "    exp_code_new = torch.zeros(batch_size, exp_code.shape[1])\n",
    "    pose_code_new = torch.zeros(batch_size, pose_code.shape[1])\n",
    "    cam_code_new = torch.zeros(batch_size, cam_code.shape[1])\n",
    "    light_code_new = torch.zeros(batch_size, light_code.shape[1], light_code.shape[2])\n",
    "\n",
    "    total_num = pose_code.shape[0]\n",
    "    count = 0\n",
    "    index = []\n",
    "\n",
    "    while True:\n",
    "        a = random.randint(0, total_num-1)       # a for source\n",
    "        b = random.randint(0, total_num-1)       # b for target\n",
    "        if abs(torch.mean(pose_code[a] - pose_code[b])) >= pose_threshold:\n",
    "\n",
    "            # get combined code\n",
    "            tforms_new[count, :] = tforms[b]\n",
    "            shape_code_new[count, :] = shape_code[a]\n",
    "            tex_code_new[count, :] = tex_code[a]\n",
    "            exp_code_new[count, :] = exp_code[b]\n",
    "            pose_code_new[count, :] = pose_code[b]\n",
    "            cam_code_new[count, :] = cam_code[b]\n",
    "            light_code_new[count, :] = light_code[b]\n",
    "\n",
    "            # get index\n",
    "            index.append((a, b))\n",
    "\n",
    "            count +=1\n",
    "\n",
    "            if count == batch_size:\n",
    "                new_code_dict = {\n",
    "                    'tforms':tforms_new.cuda(),\n",
    "                    'shape':shape_code_new.cuda(),\n",
    "                    'tex':tex_code_new.cuda(),\n",
    "                    'exp':exp_code_new.cuda(),\n",
    "                    'pose':pose_code_new.cuda(),\n",
    "                    'cam':cam_code_new.cuda(),\n",
    "                    'light':light_code_new.cuda()\n",
    "                }\n",
    "                return new_code_dict, index\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "clip_path = '/data0/wc_data/VFHQ/train/Clip+xz26EN_LRa8+P0+C0+F4517-4639'\n",
    "\n",
    "with open(osp.join(clip_path, '3DMM_condition.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "codedict, index = get_code_dict(data)\n",
    "\n",
    "source_image_list = []\n",
    "target_image_list = []\n",
    "mask_image_list = []\n",
    "bg_image_list = []\n",
    "\n",
    "\n",
    "# get images\n",
    "for i in range(len(index)):\n",
    "    source_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][0]).zfill(8)))\n",
    "    target_image_path = osp.join(clip_path, '{}.png'.format(str(index[i][1]).zfill(8)))\n",
    "    mask_image_path = osp.join(clip_path, 'mask_{}.jpg'.format(str(index[i][1]).zfill(8)))\n",
    "    source_image_list.append(np.asarray(Image.open(source_image_path).convert(\"RGB\")))\n",
    "    target_image_list.append(np.asarray(Image.open(target_image_path).convert(\"RGB\")))\n",
    "    mask_image_list.append(np.asarray(Image.open(mask_image_path)))\n",
    "\n",
    "# get masked images (background)\n",
    "for i in range(len(index)):\n",
    "    mask = mask_image_list[i]\n",
    "    mask = cv2.GaussianBlur(mask, (11, 11), 11)\n",
    "    mask = np.where( (mask <= 0), 0, 255).astype('uint8')\n",
    "    bg_image_list.append(cv2.bitwise_and(target_image_list[i], target_image_list[i], mask = 255 - mask))\n",
    "\n",
    "source_images = np.asarray(source_image_list)\n",
    "target_images = np.asarray(target_image_list) # np.array, uint8, \n",
    "mask_images = np.asarray(mask_image_list)\n",
    "bg_images = np.asarray(bg_image_list)\n",
    "\n",
    "bg_images = torch.from_numpy((bg_images / 255.0).transpose(0, 3, 1, 2))\n",
    "bg_images = bg_images.cuda()\n",
    "\n",
    "# for key in codedict:\n",
    "#     print('key: {} has shape : {} '.format(key, str(codedict[key].shape)))\n",
    "\n",
    "out = test_branch(codedict, bg_images)\n",
    "\n",
    "Image_source = source_image_list[0]\n",
    "Image_target = target_image_list[0]\n",
    "Image_mask = np.tile(mask_image_list[0] , (3, 1, 1)).transpose(1, 2, 0)\n",
    "Image_bg = bg_image_list[0]\n",
    "Image_out = (out[0].cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "\n",
    "Image_concat = np.concatenate((Image_source, Image_target, Image_mask, Image_bg, Image_out), axis= 1)\n",
    "a = Image.fromarray(Image_concat)\n",
    "a.save('/home/wenchi/zxy/HSD/test_condition.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test pose distance and test for pose threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with open('/data0/wc_data/VFHQ/train/Clip+_aZphIp0KQE+P0+C1+F2675-2891/3DMM_condition.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "pose_code = data['pose']\n",
    "\n",
    "total_num = pose_code.shape[0]\n",
    " \n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "row_list = []\n",
    "col_list = []\n",
    "# for i in range(1, 100):\n",
    "#     sum = 0\n",
    "#     for j in range(total_num - i):\n",
    "#         pose_1 = pose_code[j]\n",
    "#         pose_2 = pose_code[j+i]\n",
    "#         sum += torch.mean(pose_1 - pose_2)\n",
    "#     sum /= (total_num - i)\n",
    "#     print('间隔{}帧的图像pose 平均差值为{}'.format(i, sum))\n",
    "#     i_list.append(i)\n",
    "#     sum_list.append(sum)\n",
    "result = [0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(100000):\n",
    "    a = random.randint(0, total_num-1)\n",
    "    b = random.randint(0, total_num-1)\n",
    "\n",
    "    pose_1 = pose_code[a]\n",
    "    pose_2 = pose_code[b]\n",
    "    temp = torch.mean(pose_1 - pose_2)\n",
    "    index = min(abs(int(temp / 0.01)), 6)\n",
    "    result[index] +=1\n",
    "\n",
    "\n",
    "plt.scatter(['0~0.01', '0.01~0.02', '0.02~0.03', '0.03~0.04', '0.04~0.05', '0.05~0.06', '>=0.06'], result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3337, -1.2104, -0.6803, -1.8381,  0.2894,  2.4729])\n",
      "tensor(-0.2167)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.randn(6)\n",
    "print(a-b)\n",
    "print(torch.mean(a - b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test combine ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.system('export PYTHONPATH=/home/wenchi/zxy/HSD/ControlNet/')\n",
    "sys.path.append('/home/wenchi/zxy/HSD/ControlNet/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging improved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_path = \"/data0/wc_data/VFHQ/ckpt/sample-mnist-epoch=91-global_step=89239.0.ckpt\"\n",
    "output_path = \"/data1/wc_log/zxy/control_pbe_CLIP_ini_1.ckpt\"\n",
    "\n",
    "assert os.path.exists(input_path), 'Input model does not exist.'\n",
    "assert not os.path.exists(output_path), 'Output filename already exists.'\n",
    "assert os.path.exists(os.path.dirname(output_path)), 'Output path is not valid.'\n",
    "\n",
    "import torch\n",
    "from share import *\n",
    "from cldm.model import create_model\n",
    "\n",
    "\n",
    "def get_node_name(name, parent_name):\n",
    "    if len(name) <= len(parent_name):\n",
    "        return False, ''\n",
    "    p = name[:len(parent_name)]\n",
    "    if p != parent_name:\n",
    "        return False, ''\n",
    "    return True, name[len(parent_name):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 6.12 GiB already allocated; 3.81 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from modules.FeatureBranch import Feature_Branch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# reshape_channel = 32\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# condition_weight = test_branch.state_dict()\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m pretrained_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(input_path)[\u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1079\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1083\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1084\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.9/site-packages/torch/_utils.py:80\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\n\u001b[1;32m     81\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 6.12 GiB already allocated; 3.81 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# from modules.FeatureBranch import Feature_Branch\n",
    "\n",
    "# reshape_channel = 32\n",
    "# reshape_depth = 16\n",
    "# num_resblocks = 6\n",
    "# linear_channels = [3072, 2048, 1024, 768]\n",
    "# upsample_channels = [3072, 2048, 1024, 512]\n",
    "# downsample_channels = [512, 512, 512, 768]\n",
    "# arcface_path = '/home/wenchi/zxy/HSD/utils/arcface_pytorch/checkpoints/resnet18_110_onecard.pth' \n",
    "# resNext_path = '/home/wenchi/zxy/HSD/utils/ResNeXt/resnext_50_32x4d_modified.pth'\n",
    "\n",
    "# test_branch = Feature_Branch(   \n",
    "#                 reshape_channel= reshape_channel, \n",
    "#                  reshape_depth = reshape_depth, \n",
    "#                  num_resblocks = num_resblocks, \n",
    "#                  upsample_channels = upsample_channels, \n",
    "#                  downsample_channels = downsample_channels, \n",
    "#                  arcface_path = arcface_path, \n",
    "#                  resNext_path = resNext_path,\n",
    "#                  linear_channels = linear_channels)\n",
    "\n",
    "# condition_weight = test_branch.state_dict()\n",
    "pretrained_weights = torch.load(input_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable_vector\n",
      "betas\n",
      "alphas_cumprod\n",
      "alphas_cumprod_prev\n",
      "sqrt_alphas_cumprod\n",
      "sqrt_one_minus_alphas_cumprod\n",
      "log_one_minus_alphas_cumprod\n",
      "sqrt_recip_alphas_cumprod\n",
      "sqrt_recipm1_alphas_cumprod\n",
      "posterior_variance\n",
      "posterior_log_variance_clipped\n",
      "posterior_mean_coef1\n",
      "posterior_mean_coef2\n",
      "model.diffusion_model.time_embed.0.weight\n",
      "model.diffusion_model.time_embed.0.bias\n",
      "model.diffusion_model.time_embed.2.weight\n",
      "model.diffusion_model.time_embed.2.bias\n",
      "model.diffusion_model.input_blocks.0.0.weight\n",
      "model.diffusion_model.input_blocks.0.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.1.1.norm.weight\n",
      "model.diffusion_model.input_blocks.1.1.norm.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.2.1.norm.weight\n",
      "model.diffusion_model.input_blocks.2.1.norm.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.3.0.op.weight\n",
      "model.diffusion_model.input_blocks.3.0.op.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.4.1.norm.weight\n",
      "model.diffusion_model.input_blocks.4.1.norm.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.5.1.norm.weight\n",
      "model.diffusion_model.input_blocks.5.1.norm.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.6.0.op.weight\n",
      "model.diffusion_model.input_blocks.6.0.op.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.7.1.norm.weight\n",
      "model.diffusion_model.input_blocks.7.1.norm.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.8.1.norm.weight\n",
      "model.diffusion_model.input_blocks.8.1.norm.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.9.0.op.weight\n",
      "model.diffusion_model.input_blocks.9.0.op.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.1.norm.weight\n",
      "model.diffusion_model.middle_block.1.norm.bias\n",
      "model.diffusion_model.middle_block.1.proj_in.weight\n",
      "model.diffusion_model.middle_block.1.proj_in.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.middle_block.1.proj_out.weight\n",
      "model.diffusion_model.middle_block.1.proj_out.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.2.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.2.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.2.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.2.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.1.conv.weight\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.3.1.norm.weight\n",
      "model.diffusion_model.output_blocks.3.1.norm.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.4.1.norm.weight\n",
      "model.diffusion_model.output_blocks.4.1.norm.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.5.1.norm.weight\n",
      "model.diffusion_model.output_blocks.5.1.norm.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.2.conv.weight\n",
      "model.diffusion_model.output_blocks.5.2.conv.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.6.1.norm.weight\n",
      "model.diffusion_model.output_blocks.6.1.norm.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.7.1.norm.weight\n",
      "model.diffusion_model.output_blocks.7.1.norm.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.8.1.norm.weight\n",
      "model.diffusion_model.output_blocks.8.1.norm.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.2.conv.weight\n",
      "model.diffusion_model.output_blocks.8.2.conv.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.9.1.norm.weight\n",
      "model.diffusion_model.output_blocks.9.1.norm.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.10.1.norm.weight\n",
      "model.diffusion_model.output_blocks.10.1.norm.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.11.1.norm.weight\n",
      "model.diffusion_model.output_blocks.11.1.norm.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.bias\n",
      "model.diffusion_model.out.0.weight\n",
      "model.diffusion_model.out.0.bias\n",
      "model.diffusion_model.out.2.weight\n",
      "model.diffusion_model.out.2.bias\n",
      "proj_out.weight\n",
      "proj_out.bias\n",
      "first_stage_model.encoder.conv_in.weight\n",
      "first_stage_model.encoder.conv_in.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.0.downsample.conv.weight\n",
      "first_stage_model.encoder.down.0.downsample.conv.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.1.downsample.conv.weight\n",
      "first_stage_model.encoder.down.1.downsample.conv.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.2.downsample.conv.weight\n",
      "first_stage_model.encoder.down.2.downsample.conv.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv2.bias\n",
      "first_stage_model.encoder.mid.block_1.norm1.weight\n",
      "first_stage_model.encoder.mid.block_1.norm1.bias\n",
      "first_stage_model.encoder.mid.block_1.conv1.weight\n",
      "first_stage_model.encoder.mid.block_1.conv1.bias\n",
      "first_stage_model.encoder.mid.block_1.norm2.weight\n",
      "first_stage_model.encoder.mid.block_1.norm2.bias\n",
      "first_stage_model.encoder.mid.block_1.conv2.weight\n",
      "first_stage_model.encoder.mid.block_1.conv2.bias\n",
      "first_stage_model.encoder.mid.attn_1.norm.weight\n",
      "first_stage_model.encoder.mid.attn_1.norm.bias\n",
      "first_stage_model.encoder.mid.attn_1.q.weight\n",
      "first_stage_model.encoder.mid.attn_1.q.bias\n",
      "first_stage_model.encoder.mid.attn_1.k.weight\n",
      "first_stage_model.encoder.mid.attn_1.k.bias\n",
      "first_stage_model.encoder.mid.attn_1.v.weight\n",
      "first_stage_model.encoder.mid.attn_1.v.bias\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.encoder.mid.block_2.norm1.weight\n",
      "first_stage_model.encoder.mid.block_2.norm1.bias\n",
      "first_stage_model.encoder.mid.block_2.conv1.weight\n",
      "first_stage_model.encoder.mid.block_2.conv1.bias\n",
      "first_stage_model.encoder.mid.block_2.norm2.weight\n",
      "first_stage_model.encoder.mid.block_2.norm2.bias\n",
      "first_stage_model.encoder.mid.block_2.conv2.weight\n",
      "first_stage_model.encoder.mid.block_2.conv2.bias\n",
      "first_stage_model.encoder.norm_out.weight\n",
      "first_stage_model.encoder.norm_out.bias\n",
      "first_stage_model.encoder.conv_out.weight\n",
      "first_stage_model.encoder.conv_out.bias\n",
      "first_stage_model.decoder.conv_in.weight\n",
      "first_stage_model.decoder.conv_in.bias\n",
      "first_stage_model.decoder.mid.block_1.norm1.weight\n",
      "first_stage_model.decoder.mid.block_1.norm1.bias\n",
      "first_stage_model.decoder.mid.block_1.conv1.weight\n",
      "first_stage_model.decoder.mid.block_1.conv1.bias\n",
      "first_stage_model.decoder.mid.block_1.norm2.weight\n",
      "first_stage_model.decoder.mid.block_1.norm2.bias\n",
      "first_stage_model.decoder.mid.block_1.conv2.weight\n",
      "first_stage_model.decoder.mid.block_1.conv2.bias\n",
      "first_stage_model.decoder.mid.attn_1.norm.weight\n",
      "first_stage_model.decoder.mid.attn_1.norm.bias\n",
      "first_stage_model.decoder.mid.attn_1.q.weight\n",
      "first_stage_model.decoder.mid.attn_1.q.bias\n",
      "first_stage_model.decoder.mid.attn_1.k.weight\n",
      "first_stage_model.decoder.mid.attn_1.k.bias\n",
      "first_stage_model.decoder.mid.attn_1.v.weight\n",
      "first_stage_model.decoder.mid.attn_1.v.bias\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.decoder.mid.block_2.norm1.weight\n",
      "first_stage_model.decoder.mid.block_2.norm1.bias\n",
      "first_stage_model.decoder.mid.block_2.conv1.weight\n",
      "first_stage_model.decoder.mid.block_2.conv1.bias\n",
      "first_stage_model.decoder.mid.block_2.norm2.weight\n",
      "first_stage_model.decoder.mid.block_2.norm2.bias\n",
      "first_stage_model.decoder.mid.block_2.conv2.weight\n",
      "first_stage_model.decoder.mid.block_2.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.upsample.conv.weight\n",
      "first_stage_model.decoder.up.1.upsample.conv.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.2.upsample.conv.weight\n",
      "first_stage_model.decoder.up.2.upsample.conv.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.3.upsample.conv.weight\n",
      "first_stage_model.decoder.up.3.upsample.conv.bias\n",
      "first_stage_model.decoder.norm_out.weight\n",
      "first_stage_model.decoder.norm_out.bias\n",
      "first_stage_model.decoder.conv_out.weight\n",
      "first_stage_model.decoder.conv_out.bias\n",
      "first_stage_model.quant_conv.weight\n",
      "first_stage_model.quant_conv.bias\n",
      "first_stage_model.post_quant_conv.weight\n",
      "first_stage_model.post_quant_conv.bias\n",
      "cond_stage_model.transformer.vision_model.embeddings.class_embedding\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_ids\n",
      "cond_stage_model.transformer.vision_model.embeddings.patch_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.embeddings.position_embedding.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.weight\n",
      "cond_stage_model.transformer.vision_model.pre_layrnorm.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.weight\n",
      "cond_stage_model.transformer.vision_model.post_layernorm.bias\n",
      "cond_stage_model.final_ln.weight\n",
      "cond_stage_model.final_ln.bias\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.0.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.0.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.0.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.0.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.0.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.0.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.1.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.1.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.1.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.1.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.1.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.1.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.2.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.2.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.2.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.2.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.2.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.2.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.3.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.3.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.3.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.3.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.3.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.3.ln_2.bias\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_qkv.weight\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_qkv.bias\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.4.attn.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.4.ln_1.weight\n",
      "cond_stage_model.mapper.resblocks.4.ln_1.bias\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_fc.weight\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_fc.bias\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_proj.weight\n",
      "cond_stage_model.mapper.resblocks.4.mlp.c_proj.bias\n",
      "cond_stage_model.mapper.resblocks.4.ln_2.weight\n",
      "cond_stage_model.mapper.resblocks.4.ln_2.bias\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights_keys = list(pretrained_weights.keys())\n",
    "for key in pretrained_weights_keys:\n",
    "    print(key)\n",
    "    # prefix = key.split('.', 1)[0]\n",
    "    # if prefix == 'cond_stage_model':\n",
    "    #     del pretrained_weights[key]\n",
    "\n",
    "# for key in condition_weight:\n",
    "#     add_key = 'cond_stage_model.' + key\n",
    "#     pretrained_weights[add_key] = condition_weight[key].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM_HSD: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.54 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v2.yaml]\n",
      "odict_keys(['betas', 'alphas_cumprod', 'alphas_cumprod_prev', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'log_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'posterior_variance', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'logvar', 'model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.2.weight', 'model.diffusion_model.middle_block.2.in_layers.2.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.3.weight', 'model.diffusion_model.middle_block.2.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias', 'proj_out.weight', 'proj_out.bias', 'first_stage_model.encoder.conv_in.weight', 'first_stage_model.encoder.conv_in.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.weight', 'first_stage_model.encoder.down.0.block.0.conv1.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.weight', 'first_stage_model.encoder.down.0.block.0.conv2.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.weight', 'first_stage_model.encoder.down.0.block.1.conv1.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.weight', 'first_stage_model.encoder.down.0.block.1.conv2.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.weight', 'first_stage_model.encoder.down.1.block.0.conv1.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.weight', 'first_stage_model.encoder.down.1.block.0.conv2.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.weight', 'first_stage_model.encoder.down.1.block.1.conv1.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.weight', 'first_stage_model.encoder.down.1.block.1.conv2.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.weight', 'first_stage_model.encoder.down.2.block.0.conv1.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.weight', 'first_stage_model.encoder.down.2.block.0.conv2.bias', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.weight', 'first_stage_model.encoder.down.2.block.1.conv1.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.weight', 'first_stage_model.encoder.down.2.block.1.conv2.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.weight', 'first_stage_model.encoder.down.3.block.0.conv1.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.weight', 'first_stage_model.encoder.down.3.block.0.conv2.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.weight', 'first_stage_model.encoder.down.3.block.1.conv1.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.weight', 'first_stage_model.encoder.down.3.block.1.conv2.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.weight', 'first_stage_model.encoder.mid.block_1.conv1.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.weight', 'first_stage_model.encoder.mid.block_1.conv2.bias', 'first_stage_model.encoder.mid.attn_1.norm.weight', 'first_stage_model.encoder.mid.attn_1.norm.bias', 'first_stage_model.encoder.mid.attn_1.q.weight', 'first_stage_model.encoder.mid.attn_1.q.bias', 'first_stage_model.encoder.mid.attn_1.k.weight', 'first_stage_model.encoder.mid.attn_1.k.bias', 'first_stage_model.encoder.mid.attn_1.v.weight', 'first_stage_model.encoder.mid.attn_1.v.bias', 'first_stage_model.encoder.mid.attn_1.proj_out.weight', 'first_stage_model.encoder.mid.attn_1.proj_out.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.weight', 'first_stage_model.encoder.mid.block_2.conv1.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.weight', 'first_stage_model.encoder.mid.block_2.conv2.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.weight', 'first_stage_model.encoder.conv_out.bias', 'first_stage_model.decoder.conv_in.weight', 'first_stage_model.decoder.conv_in.bias', 'first_stage_model.decoder.mid.block_1.norm1.weight', 'first_stage_model.decoder.mid.block_1.norm1.bias', 'first_stage_model.decoder.mid.block_1.conv1.weight', 'first_stage_model.decoder.mid.block_1.conv1.bias', 'first_stage_model.decoder.mid.block_1.norm2.weight', 'first_stage_model.decoder.mid.block_1.norm2.bias', 'first_stage_model.decoder.mid.block_1.conv2.weight', 'first_stage_model.decoder.mid.block_1.conv2.bias', 'first_stage_model.decoder.mid.attn_1.norm.weight', 'first_stage_model.decoder.mid.attn_1.norm.bias', 'first_stage_model.decoder.mid.attn_1.q.weight', 'first_stage_model.decoder.mid.attn_1.q.bias', 'first_stage_model.decoder.mid.attn_1.k.weight', 'first_stage_model.decoder.mid.attn_1.k.bias', 'first_stage_model.decoder.mid.attn_1.v.weight', 'first_stage_model.decoder.mid.attn_1.v.bias', 'first_stage_model.decoder.mid.attn_1.proj_out.weight', 'first_stage_model.decoder.mid.attn_1.proj_out.bias', 'first_stage_model.decoder.mid.block_2.norm1.weight', 'first_stage_model.decoder.mid.block_2.norm1.bias', 'first_stage_model.decoder.mid.block_2.conv1.weight', 'first_stage_model.decoder.mid.block_2.conv1.bias', 'first_stage_model.decoder.mid.block_2.norm2.weight', 'first_stage_model.decoder.mid.block_2.norm2.bias', 'first_stage_model.decoder.mid.block_2.conv2.weight', 'first_stage_model.decoder.mid.block_2.conv2.bias', 'first_stage_model.decoder.up.0.block.0.norm1.weight', 'first_stage_model.decoder.up.0.block.0.norm1.bias', 'first_stage_model.decoder.up.0.block.0.conv1.weight', 'first_stage_model.decoder.up.0.block.0.conv1.bias', 'first_stage_model.decoder.up.0.block.0.norm2.weight', 'first_stage_model.decoder.up.0.block.0.norm2.bias', 'first_stage_model.decoder.up.0.block.0.conv2.weight', 'first_stage_model.decoder.up.0.block.0.conv2.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.weight', 'first_stage_model.decoder.up.0.block.1.norm1.bias', 'first_stage_model.decoder.up.0.block.1.conv1.weight', 'first_stage_model.decoder.up.0.block.1.conv1.bias', 'first_stage_model.decoder.up.0.block.1.norm2.weight', 'first_stage_model.decoder.up.0.block.1.norm2.bias', 'first_stage_model.decoder.up.0.block.1.conv2.weight', 'first_stage_model.decoder.up.0.block.1.conv2.bias', 'first_stage_model.decoder.up.0.block.2.norm1.weight', 'first_stage_model.decoder.up.0.block.2.norm1.bias', 'first_stage_model.decoder.up.0.block.2.conv1.weight', 'first_stage_model.decoder.up.0.block.2.conv1.bias', 'first_stage_model.decoder.up.0.block.2.norm2.weight', 'first_stage_model.decoder.up.0.block.2.norm2.bias', 'first_stage_model.decoder.up.0.block.2.conv2.weight', 'first_stage_model.decoder.up.0.block.2.conv2.bias', 'first_stage_model.decoder.up.1.block.0.norm1.weight', 'first_stage_model.decoder.up.1.block.0.norm1.bias', 'first_stage_model.decoder.up.1.block.0.conv1.weight', 'first_stage_model.decoder.up.1.block.0.conv1.bias', 'first_stage_model.decoder.up.1.block.0.norm2.weight', 'first_stage_model.decoder.up.1.block.0.norm2.bias', 'first_stage_model.decoder.up.1.block.0.conv2.weight', 'first_stage_model.decoder.up.1.block.0.conv2.bias', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.1.block.1.norm1.weight', 'first_stage_model.decoder.up.1.block.1.norm1.bias', 'first_stage_model.decoder.up.1.block.1.conv1.weight', 'first_stage_model.decoder.up.1.block.1.conv1.bias', 'first_stage_model.decoder.up.1.block.1.norm2.weight', 'first_stage_model.decoder.up.1.block.1.norm2.bias', 'first_stage_model.decoder.up.1.block.1.conv2.weight', 'first_stage_model.decoder.up.1.block.1.conv2.bias', 'first_stage_model.decoder.up.1.block.2.norm1.weight', 'first_stage_model.decoder.up.1.block.2.norm1.bias', 'first_stage_model.decoder.up.1.block.2.conv1.weight', 'first_stage_model.decoder.up.1.block.2.conv1.bias', 'first_stage_model.decoder.up.1.block.2.norm2.weight', 'first_stage_model.decoder.up.1.block.2.norm2.bias', 'first_stage_model.decoder.up.1.block.2.conv2.weight', 'first_stage_model.decoder.up.1.block.2.conv2.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.weight', 'first_stage_model.decoder.up.2.block.0.norm1.bias', 'first_stage_model.decoder.up.2.block.0.conv1.weight', 'first_stage_model.decoder.up.2.block.0.conv1.bias', 'first_stage_model.decoder.up.2.block.0.norm2.weight', 'first_stage_model.decoder.up.2.block.0.norm2.bias', 'first_stage_model.decoder.up.2.block.0.conv2.weight', 'first_stage_model.decoder.up.2.block.0.conv2.bias', 'first_stage_model.decoder.up.2.block.1.norm1.weight', 'first_stage_model.decoder.up.2.block.1.norm1.bias', 'first_stage_model.decoder.up.2.block.1.conv1.weight', 'first_stage_model.decoder.up.2.block.1.conv1.bias', 'first_stage_model.decoder.up.2.block.1.norm2.weight', 'first_stage_model.decoder.up.2.block.1.norm2.bias', 'first_stage_model.decoder.up.2.block.1.conv2.weight', 'first_stage_model.decoder.up.2.block.1.conv2.bias', 'first_stage_model.decoder.up.2.block.2.norm1.weight', 'first_stage_model.decoder.up.2.block.2.norm1.bias', 'first_stage_model.decoder.up.2.block.2.conv1.weight', 'first_stage_model.decoder.up.2.block.2.conv1.bias', 'first_stage_model.decoder.up.2.block.2.norm2.weight', 'first_stage_model.decoder.up.2.block.2.norm2.bias', 'first_stage_model.decoder.up.2.block.2.conv2.weight', 'first_stage_model.decoder.up.2.block.2.conv2.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.weight', 'first_stage_model.decoder.up.3.block.0.norm1.bias', 'first_stage_model.decoder.up.3.block.0.conv1.weight', 'first_stage_model.decoder.up.3.block.0.conv1.bias', 'first_stage_model.decoder.up.3.block.0.norm2.weight', 'first_stage_model.decoder.up.3.block.0.norm2.bias', 'first_stage_model.decoder.up.3.block.0.conv2.weight', 'first_stage_model.decoder.up.3.block.0.conv2.bias', 'first_stage_model.decoder.up.3.block.1.norm1.weight', 'first_stage_model.decoder.up.3.block.1.norm1.bias', 'first_stage_model.decoder.up.3.block.1.conv1.weight', 'first_stage_model.decoder.up.3.block.1.conv1.bias', 'first_stage_model.decoder.up.3.block.1.norm2.weight', 'first_stage_model.decoder.up.3.block.1.norm2.bias', 'first_stage_model.decoder.up.3.block.1.conv2.weight', 'first_stage_model.decoder.up.3.block.1.conv2.bias', 'first_stage_model.decoder.up.3.block.2.norm1.weight', 'first_stage_model.decoder.up.3.block.2.norm1.bias', 'first_stage_model.decoder.up.3.block.2.conv1.weight', 'first_stage_model.decoder.up.3.block.2.conv1.bias', 'first_stage_model.decoder.up.3.block.2.norm2.weight', 'first_stage_model.decoder.up.3.block.2.norm2.bias', 'first_stage_model.decoder.up.3.block.2.conv2.weight', 'first_stage_model.decoder.up.3.block.2.conv2.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.weight', 'first_stage_model.decoder.norm_out.bias', 'first_stage_model.decoder.conv_out.weight', 'first_stage_model.decoder.conv_out.bias', 'first_stage_model.quant_conv.weight', 'first_stage_model.quant_conv.bias', 'first_stage_model.post_quant_conv.weight', 'first_stage_model.post_quant_conv.bias', 'cond_stage_model.transformer.vision_model.embeddings.class_embedding', 'cond_stage_model.transformer.vision_model.embeddings.position_ids', 'cond_stage_model.transformer.vision_model.embeddings.patch_embedding.weight', 'cond_stage_model.transformer.vision_model.embeddings.position_embedding.weight', 'cond_stage_model.transformer.vision_model.pre_layrnorm.weight', 'cond_stage_model.transformer.vision_model.pre_layrnorm.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.12.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.13.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.14.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.15.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.16.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.17.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.18.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.19.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.20.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.21.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.22.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc1.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.mlp.fc2.bias', 'cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.weight', 'cond_stage_model.transformer.vision_model.encoder.layers.23.layer_norm2.bias', 'cond_stage_model.transformer.vision_model.post_layernorm.weight', 'cond_stage_model.transformer.vision_model.post_layernorm.bias', 'cond_stage_model.final_ln.weight', 'cond_stage_model.final_ln.bias', 'cond_stage_model.mapper.resblocks.0.attn.c_qkv.weight', 'cond_stage_model.mapper.resblocks.0.attn.c_qkv.bias', 'cond_stage_model.mapper.resblocks.0.attn.c_proj.weight', 'cond_stage_model.mapper.resblocks.0.attn.c_proj.bias', 'cond_stage_model.mapper.resblocks.0.ln_1.weight', 'cond_stage_model.mapper.resblocks.0.ln_1.bias', 'cond_stage_model.mapper.resblocks.0.mlp.c_fc.weight', 'cond_stage_model.mapper.resblocks.0.mlp.c_fc.bias', 'cond_stage_model.mapper.resblocks.0.mlp.c_proj.weight', 'cond_stage_model.mapper.resblocks.0.mlp.c_proj.bias', 'cond_stage_model.mapper.resblocks.0.ln_2.weight', 'cond_stage_model.mapper.resblocks.0.ln_2.bias', 'cond_stage_model.mapper.resblocks.1.attn.c_qkv.weight', 'cond_stage_model.mapper.resblocks.1.attn.c_qkv.bias', 'cond_stage_model.mapper.resblocks.1.attn.c_proj.weight', 'cond_stage_model.mapper.resblocks.1.attn.c_proj.bias', 'cond_stage_model.mapper.resblocks.1.ln_1.weight', 'cond_stage_model.mapper.resblocks.1.ln_1.bias', 'cond_stage_model.mapper.resblocks.1.mlp.c_fc.weight', 'cond_stage_model.mapper.resblocks.1.mlp.c_fc.bias', 'cond_stage_model.mapper.resblocks.1.mlp.c_proj.weight', 'cond_stage_model.mapper.resblocks.1.mlp.c_proj.bias', 'cond_stage_model.mapper.resblocks.1.ln_2.weight', 'cond_stage_model.mapper.resblocks.1.ln_2.bias', 'cond_stage_model.mapper.resblocks.2.attn.c_qkv.weight', 'cond_stage_model.mapper.resblocks.2.attn.c_qkv.bias', 'cond_stage_model.mapper.resblocks.2.attn.c_proj.weight', 'cond_stage_model.mapper.resblocks.2.attn.c_proj.bias', 'cond_stage_model.mapper.resblocks.2.ln_1.weight', 'cond_stage_model.mapper.resblocks.2.ln_1.bias', 'cond_stage_model.mapper.resblocks.2.mlp.c_fc.weight', 'cond_stage_model.mapper.resblocks.2.mlp.c_fc.bias', 'cond_stage_model.mapper.resblocks.2.mlp.c_proj.weight', 'cond_stage_model.mapper.resblocks.2.mlp.c_proj.bias', 'cond_stage_model.mapper.resblocks.2.ln_2.weight', 'cond_stage_model.mapper.resblocks.2.ln_2.bias', 'cond_stage_model.mapper.resblocks.3.attn.c_qkv.weight', 'cond_stage_model.mapper.resblocks.3.attn.c_qkv.bias', 'cond_stage_model.mapper.resblocks.3.attn.c_proj.weight', 'cond_stage_model.mapper.resblocks.3.attn.c_proj.bias', 'cond_stage_model.mapper.resblocks.3.ln_1.weight', 'cond_stage_model.mapper.resblocks.3.ln_1.bias', 'cond_stage_model.mapper.resblocks.3.mlp.c_fc.weight', 'cond_stage_model.mapper.resblocks.3.mlp.c_fc.bias', 'cond_stage_model.mapper.resblocks.3.mlp.c_proj.weight', 'cond_stage_model.mapper.resblocks.3.mlp.c_proj.bias', 'cond_stage_model.mapper.resblocks.3.ln_2.weight', 'cond_stage_model.mapper.resblocks.3.ln_2.bias', 'cond_stage_model.mapper.resblocks.4.attn.c_qkv.weight', 'cond_stage_model.mapper.resblocks.4.attn.c_qkv.bias', 'cond_stage_model.mapper.resblocks.4.attn.c_proj.weight', 'cond_stage_model.mapper.resblocks.4.attn.c_proj.bias', 'cond_stage_model.mapper.resblocks.4.ln_1.weight', 'cond_stage_model.mapper.resblocks.4.ln_1.bias', 'cond_stage_model.mapper.resblocks.4.mlp.c_fc.weight', 'cond_stage_model.mapper.resblocks.4.mlp.c_fc.bias', 'cond_stage_model.mapper.resblocks.4.mlp.c_proj.weight', 'cond_stage_model.mapper.resblocks.4.mlp.c_proj.bias', 'cond_stage_model.mapper.resblocks.4.ln_2.weight', 'cond_stage_model.mapper.resblocks.4.ln_2.bias', 'control_model.time_embed.0.weight', 'control_model.time_embed.0.bias', 'control_model.time_embed.2.weight', 'control_model.time_embed.2.bias', 'control_model.input_blocks.0.0.weight', 'control_model.input_blocks.0.0.bias', 'control_model.input_blocks.1.0.in_layers.0.weight', 'control_model.input_blocks.1.0.in_layers.0.bias', 'control_model.input_blocks.1.0.in_layers.2.weight', 'control_model.input_blocks.1.0.in_layers.2.bias', 'control_model.input_blocks.1.0.emb_layers.1.weight', 'control_model.input_blocks.1.0.emb_layers.1.bias', 'control_model.input_blocks.1.0.out_layers.0.weight', 'control_model.input_blocks.1.0.out_layers.0.bias', 'control_model.input_blocks.1.0.out_layers.3.weight', 'control_model.input_blocks.1.0.out_layers.3.bias', 'control_model.input_blocks.1.1.norm.weight', 'control_model.input_blocks.1.1.norm.bias', 'control_model.input_blocks.1.1.proj_in.weight', 'control_model.input_blocks.1.1.proj_in.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.1.1.proj_out.weight', 'control_model.input_blocks.1.1.proj_out.bias', 'control_model.input_blocks.2.0.in_layers.0.weight', 'control_model.input_blocks.2.0.in_layers.0.bias', 'control_model.input_blocks.2.0.in_layers.2.weight', 'control_model.input_blocks.2.0.in_layers.2.bias', 'control_model.input_blocks.2.0.emb_layers.1.weight', 'control_model.input_blocks.2.0.emb_layers.1.bias', 'control_model.input_blocks.2.0.out_layers.0.weight', 'control_model.input_blocks.2.0.out_layers.0.bias', 'control_model.input_blocks.2.0.out_layers.3.weight', 'control_model.input_blocks.2.0.out_layers.3.bias', 'control_model.input_blocks.2.1.norm.weight', 'control_model.input_blocks.2.1.norm.bias', 'control_model.input_blocks.2.1.proj_in.weight', 'control_model.input_blocks.2.1.proj_in.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.2.1.proj_out.weight', 'control_model.input_blocks.2.1.proj_out.bias', 'control_model.input_blocks.3.0.op.weight', 'control_model.input_blocks.3.0.op.bias', 'control_model.input_blocks.4.0.in_layers.0.weight', 'control_model.input_blocks.4.0.in_layers.0.bias', 'control_model.input_blocks.4.0.in_layers.2.weight', 'control_model.input_blocks.4.0.in_layers.2.bias', 'control_model.input_blocks.4.0.emb_layers.1.weight', 'control_model.input_blocks.4.0.emb_layers.1.bias', 'control_model.input_blocks.4.0.out_layers.0.weight', 'control_model.input_blocks.4.0.out_layers.0.bias', 'control_model.input_blocks.4.0.out_layers.3.weight', 'control_model.input_blocks.4.0.out_layers.3.bias', 'control_model.input_blocks.4.0.skip_connection.weight', 'control_model.input_blocks.4.0.skip_connection.bias', 'control_model.input_blocks.4.1.norm.weight', 'control_model.input_blocks.4.1.norm.bias', 'control_model.input_blocks.4.1.proj_in.weight', 'control_model.input_blocks.4.1.proj_in.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.4.1.proj_out.weight', 'control_model.input_blocks.4.1.proj_out.bias', 'control_model.input_blocks.5.0.in_layers.0.weight', 'control_model.input_blocks.5.0.in_layers.0.bias', 'control_model.input_blocks.5.0.in_layers.2.weight', 'control_model.input_blocks.5.0.in_layers.2.bias', 'control_model.input_blocks.5.0.emb_layers.1.weight', 'control_model.input_blocks.5.0.emb_layers.1.bias', 'control_model.input_blocks.5.0.out_layers.0.weight', 'control_model.input_blocks.5.0.out_layers.0.bias', 'control_model.input_blocks.5.0.out_layers.3.weight', 'control_model.input_blocks.5.0.out_layers.3.bias', 'control_model.input_blocks.5.1.norm.weight', 'control_model.input_blocks.5.1.norm.bias', 'control_model.input_blocks.5.1.proj_in.weight', 'control_model.input_blocks.5.1.proj_in.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.5.1.proj_out.weight', 'control_model.input_blocks.5.1.proj_out.bias', 'control_model.input_blocks.6.0.op.weight', 'control_model.input_blocks.6.0.op.bias', 'control_model.input_blocks.7.0.in_layers.0.weight', 'control_model.input_blocks.7.0.in_layers.0.bias', 'control_model.input_blocks.7.0.in_layers.2.weight', 'control_model.input_blocks.7.0.in_layers.2.bias', 'control_model.input_blocks.7.0.emb_layers.1.weight', 'control_model.input_blocks.7.0.emb_layers.1.bias', 'control_model.input_blocks.7.0.out_layers.0.weight', 'control_model.input_blocks.7.0.out_layers.0.bias', 'control_model.input_blocks.7.0.out_layers.3.weight', 'control_model.input_blocks.7.0.out_layers.3.bias', 'control_model.input_blocks.7.0.skip_connection.weight', 'control_model.input_blocks.7.0.skip_connection.bias', 'control_model.input_blocks.7.1.norm.weight', 'control_model.input_blocks.7.1.norm.bias', 'control_model.input_blocks.7.1.proj_in.weight', 'control_model.input_blocks.7.1.proj_in.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.7.1.proj_out.weight', 'control_model.input_blocks.7.1.proj_out.bias', 'control_model.input_blocks.8.0.in_layers.0.weight', 'control_model.input_blocks.8.0.in_layers.0.bias', 'control_model.input_blocks.8.0.in_layers.2.weight', 'control_model.input_blocks.8.0.in_layers.2.bias', 'control_model.input_blocks.8.0.emb_layers.1.weight', 'control_model.input_blocks.8.0.emb_layers.1.bias', 'control_model.input_blocks.8.0.out_layers.0.weight', 'control_model.input_blocks.8.0.out_layers.0.bias', 'control_model.input_blocks.8.0.out_layers.3.weight', 'control_model.input_blocks.8.0.out_layers.3.bias', 'control_model.input_blocks.8.1.norm.weight', 'control_model.input_blocks.8.1.norm.bias', 'control_model.input_blocks.8.1.proj_in.weight', 'control_model.input_blocks.8.1.proj_in.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'control_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'control_model.input_blocks.8.1.proj_out.weight', 'control_model.input_blocks.8.1.proj_out.bias', 'control_model.input_blocks.9.0.op.weight', 'control_model.input_blocks.9.0.op.bias', 'control_model.input_blocks.10.0.in_layers.0.weight', 'control_model.input_blocks.10.0.in_layers.0.bias', 'control_model.input_blocks.10.0.in_layers.2.weight', 'control_model.input_blocks.10.0.in_layers.2.bias', 'control_model.input_blocks.10.0.emb_layers.1.weight', 'control_model.input_blocks.10.0.emb_layers.1.bias', 'control_model.input_blocks.10.0.out_layers.0.weight', 'control_model.input_blocks.10.0.out_layers.0.bias', 'control_model.input_blocks.10.0.out_layers.3.weight', 'control_model.input_blocks.10.0.out_layers.3.bias', 'control_model.input_blocks.11.0.in_layers.0.weight', 'control_model.input_blocks.11.0.in_layers.0.bias', 'control_model.input_blocks.11.0.in_layers.2.weight', 'control_model.input_blocks.11.0.in_layers.2.bias', 'control_model.input_blocks.11.0.emb_layers.1.weight', 'control_model.input_blocks.11.0.emb_layers.1.bias', 'control_model.input_blocks.11.0.out_layers.0.weight', 'control_model.input_blocks.11.0.out_layers.0.bias', 'control_model.input_blocks.11.0.out_layers.3.weight', 'control_model.input_blocks.11.0.out_layers.3.bias', 'control_model.zero_convs.0.0.weight', 'control_model.zero_convs.0.0.bias', 'control_model.zero_convs.1.0.weight', 'control_model.zero_convs.1.0.bias', 'control_model.zero_convs.2.0.weight', 'control_model.zero_convs.2.0.bias', 'control_model.zero_convs.3.0.weight', 'control_model.zero_convs.3.0.bias', 'control_model.zero_convs.4.0.weight', 'control_model.zero_convs.4.0.bias', 'control_model.zero_convs.5.0.weight', 'control_model.zero_convs.5.0.bias', 'control_model.zero_convs.6.0.weight', 'control_model.zero_convs.6.0.bias', 'control_model.zero_convs.7.0.weight', 'control_model.zero_convs.7.0.bias', 'control_model.zero_convs.8.0.weight', 'control_model.zero_convs.8.0.bias', 'control_model.zero_convs.9.0.weight', 'control_model.zero_convs.9.0.bias', 'control_model.zero_convs.10.0.weight', 'control_model.zero_convs.10.0.bias', 'control_model.zero_convs.11.0.weight', 'control_model.zero_convs.11.0.bias', 'control_model.input_hint_block.0.weight', 'control_model.input_hint_block.0.bias', 'control_model.input_hint_block.2.weight', 'control_model.input_hint_block.2.bias', 'control_model.input_hint_block.4.weight', 'control_model.input_hint_block.4.bias', 'control_model.input_hint_block.6.weight', 'control_model.input_hint_block.6.bias', 'control_model.input_hint_block.8.weight', 'control_model.input_hint_block.8.bias', 'control_model.input_hint_block.10.weight', 'control_model.input_hint_block.10.bias', 'control_model.input_hint_block.12.weight', 'control_model.input_hint_block.12.bias', 'control_model.input_hint_block.14.weight', 'control_model.input_hint_block.14.bias', 'control_model.middle_block.0.in_layers.0.weight', 'control_model.middle_block.0.in_layers.0.bias', 'control_model.middle_block.0.in_layers.2.weight', 'control_model.middle_block.0.in_layers.2.bias', 'control_model.middle_block.0.emb_layers.1.weight', 'control_model.middle_block.0.emb_layers.1.bias', 'control_model.middle_block.0.out_layers.0.weight', 'control_model.middle_block.0.out_layers.0.bias', 'control_model.middle_block.0.out_layers.3.weight', 'control_model.middle_block.0.out_layers.3.bias', 'control_model.middle_block.1.norm.weight', 'control_model.middle_block.1.norm.bias', 'control_model.middle_block.1.proj_in.weight', 'control_model.middle_block.1.proj_in.bias', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'control_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'control_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'control_model.middle_block.1.transformer_blocks.0.norm1.weight', 'control_model.middle_block.1.transformer_blocks.0.norm1.bias', 'control_model.middle_block.1.transformer_blocks.0.norm2.weight', 'control_model.middle_block.1.transformer_blocks.0.norm2.bias', 'control_model.middle_block.1.transformer_blocks.0.norm3.weight', 'control_model.middle_block.1.transformer_blocks.0.norm3.bias', 'control_model.middle_block.1.proj_out.weight', 'control_model.middle_block.1.proj_out.bias', 'control_model.middle_block.2.in_layers.0.weight', 'control_model.middle_block.2.in_layers.0.bias', 'control_model.middle_block.2.in_layers.2.weight', 'control_model.middle_block.2.in_layers.2.bias', 'control_model.middle_block.2.emb_layers.1.weight', 'control_model.middle_block.2.emb_layers.1.bias', 'control_model.middle_block.2.out_layers.0.weight', 'control_model.middle_block.2.out_layers.0.bias', 'control_model.middle_block.2.out_layers.3.weight', 'control_model.middle_block.2.out_layers.3.bias', 'control_model.middle_block_out.0.weight', 'control_model.middle_block_out.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = create_model(config_path='/home/wenchi/zxy/HSD/ControlNet/models/cldm_pve_v2.yaml')\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These weights are newly added: logvar\n",
      "These weights are newly added: control_model.zero_convs.0.0.weight\n",
      "These weights are newly added: control_model.zero_convs.0.0.bias\n",
      "These weights are newly added: control_model.zero_convs.1.0.weight\n",
      "These weights are newly added: control_model.zero_convs.1.0.bias\n",
      "These weights are newly added: control_model.zero_convs.2.0.weight\n",
      "These weights are newly added: control_model.zero_convs.2.0.bias\n",
      "These weights are newly added: control_model.zero_convs.3.0.weight\n",
      "These weights are newly added: control_model.zero_convs.3.0.bias\n",
      "These weights are newly added: control_model.zero_convs.4.0.weight\n",
      "These weights are newly added: control_model.zero_convs.4.0.bias\n",
      "These weights are newly added: control_model.zero_convs.5.0.weight\n",
      "These weights are newly added: control_model.zero_convs.5.0.bias\n",
      "These weights are newly added: control_model.zero_convs.6.0.weight\n",
      "These weights are newly added: control_model.zero_convs.6.0.bias\n",
      "These weights are newly added: control_model.zero_convs.7.0.weight\n",
      "These weights are newly added: control_model.zero_convs.7.0.bias\n",
      "These weights are newly added: control_model.zero_convs.8.0.weight\n",
      "These weights are newly added: control_model.zero_convs.8.0.bias\n",
      "These weights are newly added: control_model.zero_convs.9.0.weight\n",
      "These weights are newly added: control_model.zero_convs.9.0.bias\n",
      "These weights are newly added: control_model.zero_convs.10.0.weight\n",
      "These weights are newly added: control_model.zero_convs.10.0.bias\n",
      "These weights are newly added: control_model.zero_convs.11.0.weight\n",
      "These weights are newly added: control_model.zero_convs.11.0.bias\n",
      "These weights are newly added: control_model.input_hint_block.0.weight\n",
      "These weights are newly added: control_model.input_hint_block.0.bias\n",
      "These weights are newly added: control_model.input_hint_block.2.weight\n",
      "These weights are newly added: control_model.input_hint_block.2.bias\n",
      "These weights are newly added: control_model.input_hint_block.4.weight\n",
      "These weights are newly added: control_model.input_hint_block.4.bias\n",
      "These weights are newly added: control_model.input_hint_block.6.weight\n",
      "These weights are newly added: control_model.input_hint_block.6.bias\n",
      "These weights are newly added: control_model.input_hint_block.8.weight\n",
      "These weights are newly added: control_model.input_hint_block.8.bias\n",
      "These weights are newly added: control_model.input_hint_block.10.weight\n",
      "These weights are newly added: control_model.input_hint_block.10.bias\n",
      "These weights are newly added: control_model.input_hint_block.12.weight\n",
      "These weights are newly added: control_model.input_hint_block.12.bias\n",
      "These weights are newly added: control_model.input_hint_block.14.weight\n",
      "These weights are newly added: control_model.input_hint_block.14.bias\n",
      "These weights are newly added: control_model.middle_block_out.0.weight\n",
      "These weights are newly added: control_model.middle_block_out.0.bias\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "scratch_dict = model.state_dict()\n",
    "\n",
    "target_dict = {}\n",
    "for k in scratch_dict.keys():\n",
    "    is_control, name = get_node_name(k, 'control_')\n",
    "    if is_control:\n",
    "        copy_k = 'model.diffusion_' + name\n",
    "    else:\n",
    "        copy_k = k\n",
    "    if copy_k in pretrained_weights:\n",
    "        target_dict[k] = pretrained_weights[copy_k].clone()\n",
    "    else:\n",
    "        target_dict[k] = scratch_dict[k].clone()\n",
    "        print(f'These weights are newly added: {k}')\n",
    "\n",
    "model.load_state_dict(target_dict, strict=True)\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'visual_projection.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import  CLIPVisionModel\n",
    "from ControlNet.ldm.modules.encoders.xf import LayerNorm, Transformer\n",
    "\n",
    "version=\"openai/clip-vit-large-patch14\"\n",
    "transformer = CLIPVisionModel.from_pretrained(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "image = torch.rand(2, 3, 224, 224)\n",
    "\n",
    "mapper = Transformer(\n",
    "                n_ctx = 257,\n",
    "                width = 1024,\n",
    "                layers = 5,\n",
    "                heads = 8,\n",
    "            )\n",
    "final_ln = LayerNorm(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state :  torch.Size([2, 257, 1024])\n",
      "pooler_output :  torch.Size([2, 1024])\n",
      "z :  torch.Size([2, 257, 1024])\n",
      "z mapper:  torch.Size([2, 257, 1024])\n",
      "z final:  torch.Size([2, 257, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = transformer(pixel_values=image)\n",
    "    print('last_hidden_state : ', outputs.last_hidden_state.shape)\n",
    "    print('pooler_output : ', outputs.pooler_output.shape)\n",
    "\n",
    "    z = outputs.last_hidden_state\n",
    "    print('z : ', z.shape)\n",
    "    z = mapper(z)\n",
    "    print('z mapper: ', z.shape)\n",
    "    z = final_ln(z)\n",
    "    print('z final: ', z.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13148735e92407d4e5779ae154c1cf483ec3f984d296a4cf4fc2e020ad66c24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
